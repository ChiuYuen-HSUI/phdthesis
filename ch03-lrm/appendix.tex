
\appendix

\section{Bias analysis LRM}
\begin{align}
   Y(k) &= \true{Y}(k) + N_Y(k) \\
   U(k) &= \true{U}(k) + N_U(k) \\
   \true{Y}(k) &= \true{G}(k)\cdot \true{U}(k)
\end{align}
Assumptions on the excitation and noise: everything is white in the window.
\begin{align}
  \sigma^2_{N_Y}(k) &= \sigma_Y^2 \\
  \sigma^2_{N_U}(k) &= \sigma_U^2 \\
  \abs{U(k)} &= \abs{U}
\end{align}
Local Model (first order, complex coefficients):
\begin{align}
  \left( a_0 + a_1 r \right) Y(r) & = \left( b_0 + b_1 r \right)  U(r) \\
  \left( 1 + a_1 r \right) Y(r) &= \left( b_0 + b_1 r \right) U(r)
\end{align}
,i.e. fix $a_0=1$.
Define
\begin{equation}
  L = \mat{
            -N_W &        & 0   \\ 
                 & \ddots &     \\   
            0    &        & N_W  
            } 
\end{equation}
and 
\begin{equation}
  \true{G} = \mat{
                   G(-N_W) &        & 0 \\
                           & \ddots &   \\
                  0        &        & G(N_W)
              }
\end{equation}
and the spectra in the local window:
\begin{equation}
  U = \mat{ U(k-N_W) \\
              \vdots   \\
              U(k+N_W) 
            }
  \text{ and }
  Y = \mat{ Y(k-N_W) \\
              \vdots   \\
              Y(k+N_W) 
            }
\end{equation}
such that $\true{Y} = \true{G} \true{U}$ and 
then the local observation matrix becomes
\begin{equation}
  K = \mat{
        U & L U & \cdots & L^{N_B} U &
              L Y & \cdots & L^{N_A} Y
      }
\end{equation}
in the general case or for $N_A=N_B=1$ the normal equations $\hat{\theta} = \KTK{K} Y$ then consist of
\begin{align}
  K^{\HT}K &= \mat{
                     U^{\HT} \\
                     U^{\HT} L \\
                     Y^{\HT} L
             }
               \mat{
                     U & L U & L Y
               } \\
           &= \mat{
              U^{\HT} U     & U^{\HT} L U   & U^{\HT} L Y \\   
              U^{\HT} L U & U^{\HT} L^2 U & U^{\HT} L^2 Y \\ 
              Y^{\HT} L U & Y^{\HT} L^2 U & Y^{\HT} L^2 Y \\ 
           } \\
  K^{\HT}Y &= \mat{ U^{\HT}    Y\\
                      U^{\HT} L Y\\
                      Y^{\HT} L Y
                    }           
\end{align}
Using the structure of $L$ is diagonal, $\trace{L} = 0$ and for a constant input amplitude spectrum:
\begin{equation}
U \approx   \mat{\abs{U} \e^{j \phi_{-N_W} }\\ \vdots \\ \abs{U} \e^{j \phi_{N_W} }}
    = \abs{U} \mat{\e^{j \phi_{-N_W} }\\ \vdots \\ \e^{j \phi_{N_W} }}
    = \abs{U} E_W
\end{equation}
this can be simplified to
\begin{align}
  K^{\HT}K 
           &= \mat{
              \kappa_0 \abs{U}^2 & 0                     & U^{\HT} L Y \\   
              0                  & \kappa_2 \abs{U}^2    & U^{\HT} L^2 Y \\ 
              Y^{\HT} L U  & Y^{\HT} L^2 U   & Y^{\HT} L^2 Y \\ 
           } \\
  K^{\HT}Y &= \mat{ U^{\HT}    Y\\
                     U^{\HT} L Y\\
                     Y^{\HT} L Y
                    }
\end{align}
where
\begin{align}
  \kappa_0 &= E_W^{\HT} E_W       = 2 N_W + 1\\
  \kappa_2 &= E_W^{\HT} L^2 E_W = \trace{L^2}\\
  \gamma_0 &= 
  \text{.}
\end{align}
The different elements (or their hermitian transpose) have an expected value as below:
\begin{align}
  \E{U^{\HT}L^n Y} 
            &=
               \E{ 
                 \true{U}^{\HT}L^n \true{Y} 
               + \true{U}^{\HT}L^n N_Y
               +      N_U^{\HT}L^n\true{Y}
               +      N_U^{\HT}L^n N_Y
                 }\\
            &= 
               \true{U}^{\HT}L^n\true{Y} \qquad \forall n \in \NaturalNumbers\\
 \E{Y^{\HT}L^n Y}
            &=
               \E{
                 \true{Y}^{\HT}L^n \true{Y} 
               + \true{Y}^{\HT}L^n N_Y
               +      N_Y^{\HT}L^n\true{Y}
               +      N_Y^{\HT}L^n N_Y
                 }\\
            &= \true{Y}^{\HT} L^n \true{Y} + N_Y^{\HT} L^n N_Y \qquad \forall n \in \NaturalNumbers \\
            &= \true{U}^{\HT} \true{G}^{\HT} L^n \true{G} \true{U} + N_Y^{\HT} L^n N_Y \qquad \forall n \in \NaturalNumbers \\
            &= \abs{\true{U}}^2 \trace{\true{G}^{\HT} L^n \true{G}} + N_Y^{\HT} L^n N_Y \qquad \forall n \in \NaturalNumbers
\end{align}
Therefore, the complete matrices have an expected value
\begin{align}
  \E{K^{\HT}K} 
            &=
            \mat{
              \kappa_0 \left( \abs{\true{U}}^2 + \sigma_U^2 \right)  & 0                    & \true{U}^{\HT} L \true{Y} \\   
              0                  & \kappa_2 \left( \abs{\true{U}}^2 + \sigma_U^2 \right)    & \true{U}^{\HT} L^2 \true{Y} \\ 
              \true{Y}^{\HT} L \true{U}  & \true{Y}^{\HT} L^2 \true{U}   & \true{Y}^{\HT} L^2 \true{Y} + \kappa_2 \sigma_Y^2\\ 
           } \\
  \frac{\E{K^{\HT}K}}{\abs{\true{U}}^2}
            &=
            \mat{
              \kappa_0 \left( 1 + \frac{\sigma_U^2}
                                 {\abs{\true{U}}^2} \right) & 0                                     & 
                                 \trace{L \true{G}} \\                        
              0                                             & \kappa_2 \left( 1 + \frac{\sigma_U^2}
                                                                         {\abs{\true{U}}^2} \right) & \trace{L^2 \true{G}} \\                      
              \trace{\true{G}^{\HT} L}                     & \trace{\true{G}^{\HT} L^2}          & \true{Y}^{\HT} L^2 \true{Y} + \kappa_2 \sigma_Y^2\\ 
           } \\           
  \E{K^{\HT}Y }
            &= \mat{ 
                     \true{U}^{\HT} \true{Y} \\
                     \true{U}^{\HT} L \true{Y}\\
                     \true{Y}^{\HT} L \true{Y}
                    }
             = \true{K}^{\HT} \true{Y}
\end{align}
\begin{align}
  \E{\hat{\theta}} &= \E{\KTK{K}{Y}} \\
                   &\approx 
                      \left(\E{K^{\HT}K}\right)^{-1} \true{K}^{\HT} \true{Y} \\
                   &\propto
                   \bigO{\frac{\sigma_U^2}{\abs{U}^2}} + \bigO{\frac{\sigma_Y^2}{\abs{Y}^2}}
\end{align}
And since $\E{K^{\HT}K} \geqByElement \true{K}^{\HT}\true{K}$ it follow that $\hat{\theta} \leqByElement \true{\theta}$.
Since the pole is $-a_1$, it shifts to the left (more damped/stable) in the complex plane.

\section{Bias calculations}
According to \cite[Appendix A]{Guillaume1995}.

\begin{equation}
  Y(k) = \frac{B(\omega_k,\theta)}{A(\omega_k,\theta)} \true{U} +  \frac{C(\omega_k,\theta)}{A(\omega_k,\theta)}+ N_Y(k)
\end{equation}
\begin{align}
  Y &= \true{Y} + N_Y \\
  U &= \true{U} + N_U \\
\end{align}
With $F= 2N_W + 1$, we have the LS cost function:
\begin{align}
  \LSCost{\theta, Z} &\isdef \inv{F}\sum_k \abs{AY - BU - C}^{2} \\
                     &= \inv{F}\sum_k \abs{A\true{Y} - B\true{U} -C}^2 \\
          &\phantom{=}+ \inv{F}\sum_k \abs{AN_Y - BN_U}^2  \\
          &\phantom{=}+2\inv{F}\sum_k \cconj{A\true{Y} - B\true{U} - C} \left( AN_Y - BN_U \right)\\
                     &\isdef \true{V}\left( \theta, \true{Z} \right)  + V_N\left( \theta, N_{Z} \right) + V_X\left( \theta, {Z} \right)
\end{align}
Its expected value (with respect to the measurements $Z$) is
\begin{align}
  \ELSCost{\theta} &\isdef \E[Z]{\LSCost{\theta,Z}} \\
                   &= \E{\true{V}} + \E{V_N} + \E{V_X} \\
      \true{E} = \E{\true{V}} &= \true{V} \\
      E_N = \E{V_N}      &= \inv{F} \sum_k \left( \abs{A}^2 \sigma_Y^2
                                          +  \abs{B}^2 \sigma_U^2
                                   - 2\cconj{A}B \sigma_{YU}
                                    \right)\\
      E_X = \E{V_X}      &= 0
\end{align}
Expand the expected cost in Taylor series around the true parameters $\true{\theta}$:
\begin{align}
  \ELSCost{\theta} &= 
                     \ELSCost{\true{\theta}} \\
          &\phantom{=}+ \pdiff{\ELSCost{\true\theta}}{\true\theta} \left( \theta - \true\theta \right) \\
          &\phantom{=}+ \frac{1}{2} \left( \theta - \true\theta \right)^{\HT}
                      \pdiff[2]{\ELSCost{\true\theta}}{\true\theta}
                     \left( \theta - \true\theta \right)
                     \text{.}
\end{align}
This expansion is exact since the cost function is quadratic in $\theta$.
Let us define ${\theta_{\star}} \isdef \arg\min \LSCost{\theta,Z}$ and asserting that $\pdiff{\ELSCost{\theta_{\star}}}{\theta_{\star}} = 0$ yields:
\begin{align}
  \theta_{\star} - \true\theta &= - \inv{\left[ \pdiff[2]{\ELSCost{\true\theta}}
                                                         {\true\theta} \right]}
                                    \left( \pdiff{\ELSCost{\true\theta}}{\true\theta} \right)^{\HT} \\
                               &= - \inv{\left[ \pdiff[2]{V_{N}\left( {\true\theta} \right)}
                                                         {\true\theta} \right]}
                                    \left( \pdiff{V_{N}\left( {\true\theta} \right) }{\true\theta} \right)^{\HT}
\end{align} 
\TODO{the parameters must be split in real/imag parts, as with the cost contributions!!!}
Splitting the parameter $\theta = \mat{\theta_A^{\T} & \theta_B^{\T} & \theta_C^{\T}}$ and defining
$E_N = \inv{F}\sum_k e_N(k)$ such that $\pdiff[n]{E_N}{\theta_j} = \inv{F}\sum_k \pdiff[n]{e_N}{\theta_j(k)}$ allows to simply evaluate the different contributions:
\begin{align}
  \pdiff{e_N(k)}{a_j} 
     &= 2 \sigma_Y^2 \real \cconj{\Omega_k^j} A(k,\theta) - 2 \sigma_{YU} \cconj{\Omega_k^j} B(k,\theta) \\
  \pdiff{e_N(k)}{b_j}
     &= 2 \sigma_U^2 \real \cconj{\Omega_k^j} B(k,\theta) - 2 \sigma_{YU} \Omega_k^j \cconj{A(k,\theta)}\\
  \pdiff{e_N(k)}{c_j} &= \pdiffr[2]{e_N(k)}{c_j \pd a_i} = \pdiffr[2]{e_N(k)}{c_j \pd b_i} = 0\\
  \pdiffr[2]{e_N}{a_j \pd a_i} &= ? \\
  \pdiffr[2]{e_N}{a_j \pd b_i} &= -2 \sigma_{YU} \cconj{\Omega_k^j} \Omega_k^i \\
  \pdiffr[2]{e_N}{b_j \pd b_i} &= ? \\
\end{align}
