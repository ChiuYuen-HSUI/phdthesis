Estimating a non-parametric \gls{FRF} is often a key step in the analysis and/or design of physical systems.
However, obtaining a good-quality non-parametric \gls{FRF} from a measured input-output data set can prove difficult due to the presence of noise and measurement time limitations to observe lightly-damped dynamics well.

Recently, there has been a renewed interest in the development of non-parametric estimators to estimate the \gls{FRF} of a system.
Many of these approaches have focused on separating the periodic response (i.e. the \gls{FRF}), the measurement noise and leakage contributions of the spectrum by exploiting the structure that those different contributions exhibit.

% \paragraph*{Contributions}
% In this paper, the existing \gls{LRM} is extended to the \gls{LRIC} that does not suffer from  the bias that is present in the \gls{LRM}.
% An extensive study is carried out to compare the performance of the \gls{LPM}, \gls{LRM} and \gls{LRIC} to describe resonant systems under varying conditions of the signal-to-noise ratio.
% Also, a pragmatic model-order-selection tool for the \gls{LPM} and \gls{LRM} is introduced.

\paragraph*{Outline}
\secref{sec:theory} introduces theory of estimating \glspl{FRF} and the relevant local modeling techniques.
In \secref{sec:biascalc}, the bias of the \gls{LRM} is derived.
In \secref{sec:simulations} the performance of the different modeling methods are compared by means of simulations.
Finally, conclusions are presented in \secref{sec:conclusion}.
%TODO: choice s vs z
%TODO: influence of resonant noise filter

\section{Local modeling approaches}
\label{sec:theory}

Consider a discrete-time generalized output-error \gls{LTI} set-up, excited by the input signal $u[n]$.
For an infinite data length, such a system can be described in the time domain by
\begin{equation}
  y[n] = \ezbrace{\true{G}(z) u[n]}{\true{y}[n]} + \ezbrace{\true{H}(z) \true{e}[n]}{v[n]} \text{ with } n \in \IntegerNumbers
  \label{eq:output-error-TD-infinite}
\end{equation}
where $v[n]$ is filtered white noise, $v[n] = \true{H}(z) \true{e}[n]$ where $z^{-1}$ is the backward shift operator, i.e. $z^ {-1}x[n] = x[n-1]$.
The transfer functions $\true{G}$ and $\true{H}$ are rational functions that are stable and causal.

During typical measurements, however, $y[n]$ and $u[n]$ are only measured over a limited time span, i.e. $n \in \set{0,1,\ldots,N-1}$.
This introduces transient terms $t_{\bullet}$ in this relationship~\citep{Pintelon1997ARB}:
\begin{equation}
y[n] = \true{G}(z) u[n] + \true{H}(z) \true{e}[n] + \ezbrace{t_G[n] + t_H[n]}{t[n]}
\label{eq:output-error-TD-finite}
\end{equation}
where $t_G[n]$ and $t_H[n]$ are due to the different conditions of respectively $\true{G}$ and $\true{H}$ at the beginning and end of the measurement record~\citep{Pintelon1997ARB}.
Both terms can be lumped together as $t[n]$, which is often~\citep{Pintelon2010LPM1} determined predominantly by $t_G[n]$.

\begin{definition}\label{def:DFT}
The $N$-points \glsfirst{DFT} of a signal $x(n\Ts) = x[n]$ with $n \in \set{0,\ldots,N-1}$ is
\begin{equation}
  X \left[ k \right] =
  X \left( \omega_k \right)
  \isdef
  \frac{1}{\sqrt{N}}\sum_{n=0}^{N-1} x \left( n \Ts \right)  \exponent{- j \omega_k n \Ts }
  \label{eq:DFT}
\end{equation}
where $\omega_k \isdef \tfrac{2 \pi k }{N \Ts}$, $k \in \set{0,\ldots,N-1}$ and $\Ts$ is the sampling time.
\end{definition}

By applying the \gls{DFT}~\eqref{eq:DFT} to both sides of \eqref{eq:output-error-TD-finite}, one obtains its frequency-domain counterpart:
\begin{align}
    Y \left( \omega_k \right) 
    & = \true{G} \left( \omega_k \right) U \left( \omega_k \right) 
      + \true{H} \left( \omega_k \right) E \left( \omega_k \right)
      + T \left( \omega_k \right)\\
      &= \true{G} \left( \omega_k \right) U \left( \omega_k \right)  + V(\omega_k) + T(\omega_k)
      \text{.}
  \label{eq:output-error-FD}
\end{align}

In this paper, we focus on using local modeling to separate the different terms, i.e.
\begin{itemize}
  \item the transfer function $\true{G}$,
  \item the leakage term $T$, and
  \item the noise term $V$.
\end{itemize}

\subsection{Local modeling}
Local modeling methods  exploit the `smoothness' (or conversely `roughness') of the different terms of \eqref{eq:output-error-FD} over the frequency $\omega$.
In particular, we will assume that $U(\omega)$ is `rough' over the frequency~\citep{Schoukens2009LPM}, e.g. as is the case for noise excitations or random phase multisines (see further).
On the other hand, it is well-known that the transfer functions $\true{G}$, $\true{H}$ and hence also the corresponding transient contributions $T = T_G + T_H$ are smooth over the frequency.
As such, these smooth contributions can be approximated well around each frequency $\omega_k$ using a local rational model:
\begin{align}
  \true{G}(\omega_{k}+d) 
  &\approx
  \frac{\Sum_{i=0}^{\order{B}} b_i d^i}
            {1 + \Sum_{i=1}^{\order{A}} a_i d^i}
    &\isdef
    \frac{\LocalModel[k]{B}(d)}%
           {\LocalModel[k]{A}(d)} 
           = \LocalModel[k]{G}(d)
  \text{,}
  \label{eq:LRM:model:G}
  \\
  \true{T}(\omega_k + d) &\approx
  \frac{\Sum_{i=0}^{\order{C}} c_i d^i}
            {1 + \Sum_{i=1}^{\order{A}} a_i d^i}
    &\isdef 
      \frac{\LocalModel[k]{C}(d)}%
           {\LocalModel[k]{A}(d)}
      = \LocalModel[k]{T}(d)
  \text{.}
  \label{eq:LRM:model:T}
\end{align}

In these expressions, we denote $\LocalModel[k]{G}$ and $\LocalModel[k]{T}$ for the local model for respectively transfer function and the transient.
Note that such local quantities (which depend on the frequency bin $k$) are denoted in bold.
To alleviate the notations, the subscript $k$ is omitted unless 

The local parameters $\theta$  
\begin{equation}
\theta \isdef 
\begin{bmatrix}
\theta_A\\ 
\theta_B\\
\theta_C
\end{bmatrix}
\qquad \text{ with }
\theta_A \isdef
\begin{bmatrix}
a_1\\ \vdots\\ a_{\order{A}}
\end{bmatrix}
\text{, }
\theta_B \isdef
\begin{bmatrix}
b_0\\ \vdots\\ b_{\order{B}}
\end{bmatrix}
\text{, and }
\theta_C \isdef
\begin{bmatrix}
c_0\\ \vdots\\ c_{\order{C}}
\end{bmatrix}
\end{equation}
in \eqref{eq:LRM:model:G} and \eqref{eq:LRM:model:T}, we consider a local window
\begin{equation}
  \LocalWindow[k] 
  \isdef
  \Set{
    \omega_{k+r} 
    | 
    r \in \LocalShifts{k}
  }
\end{equation}
with $\LocalShifts{k} \isdef \Set{-\numel{W}, -\numel{W}+1, \ldots, 0, \ldots, \numel{W}}$
such that the local window around $\omega_k$ consists of $\numel{\LocalWindow} = 2 \numel{W} + 1$ bins. 
If one denotes the input/output spectra in such a local frequency window $\LocalWindow[k]$ as
\begin{equation}
  \LocalVector{U}_k \isdef 
  \mat{
    U(\omega_{k-\numel{W}})\\
    \vdots\\
    U(\omega_{k})\\
    \vdots\\
    U(\omega_{k+\numel{W}})\\
  }
  \text{ and }
  \LocalVector{Y}_k \isdef 
  \mat{
    Y(\omega_{k-\numel{W}})\\
    \vdots\\
    Y(\omega_{k})\\
    \vdots\\
    Y(\omega_{k+\numel{W}})\\
  }
  \text{,}
\end{equation}
and similarly for the other quantities, equation \eqref{eq:output-error-FD} limited to $\LocalWindow[k]$ can be written as
\begin{equation}
  \LocalVector{Y}_k  = {\true{\LocalVector{G}}}_k \hadamard \LocalVector{U}_k + \LocalVector{T}_k + \LocalVector{V}_k
\text{,}
\end{equation} 
where $\hadamard$ denotes the element-wise product (also, Hadamard product).
Substituting $\true{\LocalVector{G}}$ and $\LocalVector{T}$ with the local models $\LocalModel{G}$ and $\LocalModel{T}$ and neglecting the influence of $\LocalVector{V}$ yields
\begin{equation}
  \LocalVector{Y} 
  \approx 
  \hat{\LocalVector{Y}} 
    =
      \LocalModel{G} \hadamard \LocalVector{U} + \LocalModel{T}
      \text{.}
      \label{eq:local-system-equations}
\end{equation}
Note that this encompasses $\nWind$ complex equations in the $\nth = \order{A} + \order{B} + \order{C} + 2$  unknown complex model parameters ($a_i$, $b_i$ and $c_i$ in \eqref{eq:LRM:model:G} and \eqref{eq:LRM:model:T}).
Consequently, a necessary condition to compute the model parameters is that
\begin{equation}
  \DOF = \nWind - \nth
       = 2 \numel{W} - \order{A} - \order{B} - \order{C} - 1
\end{equation}
is positive.
To effectively compute the local model parameters, the equation error in \eqref{eq:local-system-equations} is minimized by formulating a quadratic cost function:
\begin{equation}
  \LocalVector{\CostFunc{\LRIC}} 
    \isdef 
      \left( \LocalVector{Y}  -  \LocalModel{G} \hadamard \LocalVector{U} - \LocalModel{T} \right)^{\HT} 
      \left( \LocalVector{Y}  -  \LocalModel{G} \hadamard \LocalVector{U} - \LocalModel{T} \right)
   \label{eq:costfunc:LRIC}
   \text{.}
\end{equation}
Due to the presence of the denominator of $\LocalModel{G} = \frac{\LocalModel{B}}{\LocalModel{A}}$, the equation error is not linear in the parameters $a_i$ and hence \eqref{eq:costfunc:LRIC} requires starting values and time-consuming iterative optimization schemes to obtain a parameter estimate.
We denote such a method as $\lric{\numel{W},\order{B},\order{A},\order{C}}$.

\subsection{The Local Rational Method}
\label{sec:nparam:LRM}
The \gls{LRM} as first introduced by \citep{McKelvey2012LRM}, overcomes the computational burden of an iterative procedure by weighting the equation error by the denominator polynomial $\LocalModel{A}$  akin to the procedure in \citep{Levy1959}.
I.e. the \gls{LRM} procedure tries to minimize the equation error in
\begin{equation}
  \LocalModel{A} \hadamard \LocalVector{Y} = \LocalModel{B} \hadamard \LocalVector{U}  + \LocalVector{C} + \LocalVector{V}
  \text{,}
\end{equation}
for which the corresponding cost function is
\begin{equation}
  \LocalVector{\CostFunc{\LRM}}
  \isdef 
  \left( \LocalModel{A} \hadamard \LocalVector{Y}  -  \LocalModel{B} \hadamard \LocalVector{U} - \LocalModel{C} \right)^{\HT} 
      \left( \LocalModel{A} \hadamard \LocalVector{Y}  -  \LocalModel{B} \hadamard \LocalVector{U} - \LocalModel{C} \right)
      \text{.}
      \label{eq:nparam:LRM:costFunc}
\end{equation}
where $\LocalVector{V}$ is vector consisting of \gls{iid} complex normally distributed variables~\citep{Gallager2008} that each have a variance $\sigma_V^2$, i.e. the disturbing noise is assumed white over the local frequency window.
Equivalently, the last equation can be rewritten as a linear regression problem
\begin{equation}
  \LocalVector{Y} = \LocalMatrix{K} \LocalVector{\theta} + \LocalVector{V}
\end{equation}
where $\LocalMatrix{K}$ is the so-called design matrix (or observation matrix):
\begin{align}
  \label{eq:nparam:designMatrix}
  \LocalMatrix{K} 
    & \isdef 
  \mat{
     \LocalMatrix{K}_A &
     \LocalMatrix{K}_B & 
     \LocalMatrix{K}_C
  }\\
  \LocalMatrix{K}_A 
    &\isdef
    \mat{
      \LocalVector{Y} \hadamard \LocalVector{d}^1 &
      \cdots &
      \LocalVector{Y} \hadamard \LocalVector{d}^{\order{A}}
    }\\
  \LocalMatrix{K}_B 
    &\isdef
    \mat{
      \LocalVector{U} \hadamard \LocalVector{d}^0 &
      \cdots &
      \LocalVector{U} \hadamard \LocalVector{d}^{\order{B}}
    }\\
  \LocalMatrix{K}_C
    &\isdef
    \mat{
      \LocalVector{d}^0 &
      \cdots &
      \LocalVector{d}^{\order{C}}
    }
    \text{.}
\end{align}
In this formulation, we have used $\LocalVector{d}^{n}$ to denote the $n^{\text{th}}$ Hadamard power of $\LocalVector{d}$ (this corresponds to \mcode{d.^n} in \MATLAB) such that
\begin{equation}
    \LocalVector{d}^{n} 
    \isdef
    \mat{
      (-\numel{W})^{n} &
      \cdots &
      (\numel{W})^{n}
    }^{\T}
    \text{ with $n \in \mathbb{Z}$}
\end{equation}
where every element corresponds to a value of $d^{n}$ in accordance with $\LocalShifts{k}$.

This formulation facilitates to solve the problem in a one-step approach:
\begin{equation}
  \LocalVector{\theta}_{\LRM} 
    \isdef \pinv{\LocalMatrix{K}} \LocalVector{Y}
    = \KTK{\LocalMatrix{K}} \LocalVector{Y}
    \label{eq:nparam:LRM:normalEquations}
\end{equation}
where $\pinv{\LocalMatrix{K}}$ denotes the Moore-Penroose pseudo-inverse of $\LocalMatrix{K}$.

\TODO{hier voortschrijven: ruisvariantie berekenen}

\begin{remark}
This system can only be solved reliably if the columns in \eqref{eq:nparam:designMatrix} are linearly independent.
In practice, this corresponds to the requirements on the input spectrum as stated in \citet{Schoukens2009LPM}: the input spectrum should be sufficiently `rough' to separate the transient contribution.
This can be obtained e.g. using random phase multisines or random input signals.
\end{remark}

\begin{remark}
Since it is well-known that the Vandemonde structure in the design matrix \eqref{eq:nparam:designMatrix} leads to numerical ill-conditioning for higher model complexities, additional measures are taken to improve numerical conditioning.
To improve numerical conditioning~\citep{Pintelon2005} of the estimation problem, we substitute $d = \dw{r}{k}$ in equations \eqref{eq:LRM:model:G} and \eqref{eq:LRM:model:T}
\begin{equation}
\dw{r}{k} \isdef \frac{\omega_{k+r} - \omega_k}{\Delta \omega_k}
\label{eq:freqScaling}
\end{equation}
where
\begin{equation}
  \Delta \omega_k \isdef
  \max
  \Set{
    \abs{ \omega_k - \omega_j} |  \vphantom{\frac{a}{b}}  \omega_j \in \LocalWindow[k]
  }
\end{equation}
such that $\abs{\dw{r}{k}} \leq 1$ when $r \in \LocalShifts{k}$.
\end{remark}

We denote a method that solves \eqref{eq:nparam:LRM:costFunc} for given bandwidth and local orders as $\lrm{\numel{W}, \order{B},\order{A},\order{C}}$.

\subsection{The Local Polynomial Method}
The \gls{LPM} is method that predates the \gls{LRM} and has been devised by~\citet{Schoukens2006LPM}.
The \gls{LPM} approximates both the \gls{FRF} and transient contribution by means of complex-valued local polynomials in the frequency domain.

The \gls{LPM} can be understood as a specific case of the \gls{LRM}: for  $\order{A} = 0$, the \gls{LRM} reduces to the \gls{LPM}.
As such, we can use the notation $\lpm{\numel{W}, \order{B}, \order{C}}$ as a synonym for $\lrm{\nWind, 0, \order{C}}$.

\begin{remark}
The trivial setting for local `modeling' where a window with a total width of $1\unit{bin}$ and no transient estimation ($\order{C} = -1$), actually corresponds to the simple \gls{ETFE}~\citep{Broersen1995,Stenman2000ASETFE,Stenman2001ASFRF} when no further smoothing or windowing is applied:
\begin{equation}
  \estimated{G}_{\ETFE}(\omega_k) \isdef \frac{Y(\omega_k)}{U(\omega_k)}
  \label{eq:nparam:ETFE}
\end{equation}
As such, $\ETFE \equiv \lrm{0,0,0,-1} \equiv \lpm{0,0,-1}$.
\end{remark}

\begin{remark}
The methods proposed by \citet{Stenman2001ASFRF,Stenman2000ASETFE} also refer to `local polynomial regression' to smoothen the \gls{FRF}.
It should be noted, however, that such methods differ significantly from the \gls{LPM}.
Specifically, those methods operate directly on the raw \gls{ETFE}, in contrast to the \gls{LPM} which uses the input-output spectra and can hence estimate both an \gls{FRF} and a transient contribution.
Moreover, the methods proposed by \citet{Stenman2001ASFRF} build upon `local regression` approaches such as \gls{LOWESS}.
See e.g. \citet{Loader1999} for more information regarding these `local regression' approaches.
\end{remark}

\section{Theory: bias}
\label{sec:biascalc}
Since the local design matrix \eqref{eq:nparam:designMatrix} of the \gls{LRM} contains the noisy output spectrum $\LocalVector{Y}$ when $\order{A} > 0$, the \gls{LRM} is expected to be a biased estimator.
In this section, we will derive expressions for the bias such that it can be quantified how important this bias is in different situations.

To determine the bias, an approach similar to \citep[Appendix A]{Guillaume1995} is followed.
Let us denote the expected value of the cost function over all measurements $\LocalVector{Z} = \mat{\LocalMatrix{Y}, \LocalMatrix{U}}$ as
\begin{align}
  \ELSCost{\theta}
              & \isdef 
                   \E[\LocalVector{Z}]%
                         {    
                                \LSCost{\LocalVector{\theta}, \LocalVector{Z}}} \\
              & = \ELSCost[0]{\theta} + \ELSCost[n]{\theta} \\
    \ELSCost[0]{\theta} & \isdef 
            \frac{1}{\nWind} 
                \sum_{\dW} 
                     \left| 
                             \LocalModel{A}(\dW, \LocalVector{\theta}) \true{\LocalVector{Y}}
                          - \LocalModel{B}(\dW, \LocalVector{\theta}) \true{\LocalVector{U}}
                          - \LocalModel{C}(\dW, \LocalVector{\theta}) 
                    \right|^2 \\
    \ELSCost[n]{\theta} & \isdef 
              \frac{1}{\nWind} 
                     \sum_{\dW} 
                              \left| \LocalModel{A}(\dW,\LocalVector{\theta}) \right|^2 
                              \sigma_V^2
\end{align}
where $\true{\LocalVector{U}}$ and $\true{\LocalVector{Y}}$ denote the `true' (noiseless) input-output spectra in the local window.
Note that this function is a quadratic function of $\LocalVector{\theta}$ and hence it can be represented exactly by its second order Taylor series around $\true\theta$:
\begin{multline}
  \ELSCost{\theta} = \ELSCost{\true\theta} 
  + \PartialDerivative{\ELSCost{\true\theta}}{\true\theta}  \left( \theta - \true\theta \right) %\\
  + \frac{1}{2} \left( \theta - \true\theta \right)^{\T} 
  \PartialDerivative[2]{\ELSCost{\true\theta}}{\true\theta} 
   \left( \theta - \true\theta \right)
   \text{.}
\end{multline}
The minimizer $\estimated\theta$ of this function can be obtained by equating the derivative
\begin{equation}
  \PartialDerivative{\ELSCost{\estimated\theta}}
                                          {\estimated\theta} 
  = 
  \left( \PartialDerivative{\ELSCost{\true\theta}}{\true\theta} \right)^{\T} 
  + \PartialDerivative[2]{\ELSCost{\true\theta}}{\true\theta} \left( \estimated\theta - \true\theta \right)
\end{equation}
to zero.
This is equivalent to 
\begin{equation}
  \left( \estimated\theta - \true\theta \right) 
  = 
  - \left(   \PartialDerivative[2]{\ELSCost{\true\theta}}{\true\theta} \right)^{-1}  
     \left( \PartialDerivative{\ELSCost{\true\theta}}{\true\theta} \right)^{\T}
  \text{.}
\end{equation}
which expresses the bias $\Delta\theta = \left( \estimated\theta - \true\theta \right)$ of the estimated parameters.
Remark that 
\begin{equation}
  \PartialDerivative{\ELSCost{\true\theta}}{\true\theta}  
  = 
  \PartialDerivative{\ELSCost[0]{\theta} + \ELSCost[n]{\theta} }{\true\theta} 
  = 
  \PartialDerivative{\ELSCost[n]{\theta} }{\true\theta} 
\end{equation}
since $\ELSCost[0]{\theta}$ is minimized in $\true\theta$ by definition.
As such, the bias on the parameters can be simplified to
\begin{equation}
  \Delta\theta = 
  - \left(   \PartialDerivative[2]{\ELSCost{\true\theta}}{\true\theta^2} \right)^{-1} 
     \left( \PartialDerivative{\ELSCost[n]{\true\theta}}{\true\theta} \right)^{\T}
  \text{.}
  \label{eq:nparam:bias-basic}
\end{equation}

Do note that the estimated parameters $\theta \in \ComplexMatrix{\nth \times 1}$ but that $\ELSCost{\theta}$ is a real-valued function.
In contrast to a complex-valued function, this requires additional attention when the derivatives are calculated~\citep{Messerschmitt2006} and~\citep[Section 15.9]{Pintelon2012} since $\ELSCost{\theta}$ is not an analytic function.
Particularly, the function $\ELSCost{\theta}$ is regarded as a two-dimensional function $\ELSCost{\theta,\conj{\theta}}$ where $\conj{\theta}$ and $\theta$ are considered as independent variables~\citep{Hjorungnes2007,Hjorungnes2011}.
The derivatives are then computed with respect to these variables.
% Let us introduce the notations
% \begin{equation}
%   \ReIm{X} 
%   \isdef 
%   \begin{bmatrix}
%     \real{X}\\
%     \imag{X}
%   \end{bmatrix}
%   \quad  \text{,} \quad
%   \ReImImRe{X} 
%   \isdef 
%   \begin{bmatrix}
%     \real{X} & -\imag{X}\\
%     \imag{X} & \hphantom{-} \real{X}
%   \end{bmatrix}
% \end{equation}
% and the shorthand $\ReIm{\theta} = \ReImTheta$.
% E.g. in \citet[Section 15.9]{Pintelon2012} and \citet[Section 5.1]{Seber2007}, it is stated that $\ReImImRe{X}$ is isomorphic with $X$ such that operations on a complex matrix $X$ can be written as equivalent operations on the real matrix $\ReImImRe{X}$.
% As a result,
% \begin{align}
%   \Delta\theta &=
%   \left( \mat{1 & j} \kron \Identity{\nth[]}  \right) 
%   \ReIm{\Delta\theta}
%   \\
%   \ReIm{\Delta\theta} &= 
%   - \ReImImRe{ \left(   \PartialDerivative[2]{\ELSCost{\true\theta}}{\true\theta^2} \right)^{-1} }
%     \ReIm{ \left( \PartialDerivative{\ELSCost[n]{\true\theta}}{\true\theta} \right)^{\T}}
% \end{align}
% where $\kron$ denotes the Kronecker product~\citep{Brewer1978}.
To facilitate the derivations, we rewrite the expected value of the cost function in terms of $\theta$ and $\conj{\theta}$:
\begin{align}
   \ELSCost[0]{\theta,\conj{\theta}} & =
      \frac{1}{\nWind}
           \sum_{\dW}
           \LocalModel{Q}(      \dW,        \LocalVector{\theta},  \true{      \LocalVector{Y}},  \true{      \LocalVector{U} })
           \LocalModel{Q}(\conj{\dW}, \conj{\LocalVector{\theta}}, \true{\conj{\LocalVector{Y}}}, \true{\conj{\LocalVector{U}}})
   \\
   \LocalModel{Q}(\dW, \LocalVector{\theta},  \true{\LocalVector{Y}},  \true{ \LocalVector{U} }) & \isdef
                 \LocalModel{A}(\dW, \LocalVector{\theta}) \true{\LocalVector{Y}}(\dW)
               - \LocalModel{B}(\dW, \LocalVector{\theta}) \true{\LocalVector{U}}(\dW)
               - \LocalModel{C}(\dW, \LocalVector{\theta})
    \\             
    \ELSCost[n]{\theta,\conj{\theta}} & =
              \frac{\sigma_V^2}{\nWind} 
                     \sum_{r} 
                               \LocalModel{A}(      \dW,        \LocalVector{\theta} ) 
                               \LocalModel{A}(\conj{\dW}, \conj{\LocalVector{\theta}}) 
\end{align}
since $\conj{ \LocalModel{A}(\dW,\theta)} =  \LocalModel{A}(\conj{\dW},\conj{\theta}) $, and similar for $\LocalModel{B}$ and $\LocalModel{C}$ and $\LocalModel{Q}$.

\paragraph{Contributions to the first order derivative}
The last term in \eqref{eq:nparam:bias-basic}, i.e. the first order derivative,  is given by
\begin{equation}
  \PartialDerivative{\ELSCost[0]{\true\theta, \conj{\true\theta}}}{\theta}
  = 
  \begin{bmatrix}
  \PartialDerivative{\ELSCost[0]{\true\theta, \conj{\true\theta}}}{\theta_A} &
  \PartialDerivative{\ELSCost[0]{\true\theta, \conj{\true\theta}}}{\theta_B} &
  \PartialDerivative{\ELSCost[0]{\true\theta, \conj{\true\theta}}}{\theta_C}
  \end{bmatrix}
\end{equation}
and split into the parts pertaining to respectively $\LocalModel{A}$, $\LocalModel{B}$ and, $\LocalModel{C}$.
The respective components are given by:
\begin{align}
  \PartialDerivative{\ELSCost[n]{\theta,\conj{\theta}}}{a_i} 
     &= 
     \frac{\sigma_V^2}{\nWind}
     \sum_{\dW} \dW^i \LocalModel{A}(\conj{\dW},\conj{\LocalVector{\theta}}) 
     && \forall i \in \set{1,\ldots,\order{A}}
     \\
   \PartialDerivative{\ELSCost[n]{\theta,\conj{\theta}}}{b_i} 
   &= 0
   && \forall i \in \set{0,\ldots,\order{B}}
   \\
   \PartialDerivative{\ELSCost[n]{\theta,\conj{\theta}}}{c_i} 
   &= 0
   &&\forall i \in \set{0,\ldots,\order{C}}
   \text{.}
\end{align}
It can be seen that only the factors pertaining to $\theta_{A}$ are present.
As this term occurs linearly in expression \eqref{eq:nparam:bias-basic}, only $\theta_A$ can incur a bias.
Consequently, only the poles can shifted by the presence of noise.
As such, the bias term $\Delta \theta$ can be written in the following sparse form:
\begin{equation}
  \Delta \theta = 
  \begin{bmatrix}
    \Delta \theta_A\\
    \Delta \theta_B\\
    \Delta \theta_C
  \end{bmatrix}
  =
  \begin{bmatrix}
    \Delta \theta_A\\
    \deemph{0}\\
    \deemph{0}
  \end{bmatrix}
  \text{.}
  \label{eq:nparam:lrm:bias:sparse}
\end{equation}

\paragraph{Contributions to the Hessian}
Computing the Hessian in \eqref{eq:nparam:bias-basic} also boils down to recognizing the block structure.
This results in the following block matrix:
\begin{equation}
  \PartialDerivative[2]{\ELSCost{\true\theta}}{\true\theta^2} =
  \begin{bmatrix}
    \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{\theta}_A \pd \theta_A } &
    \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{\theta}_B \pd \theta_A } & 
    \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{\theta}_C \pd \theta_A } \\
    \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{\theta}_A \pd \theta_B } &
    \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{\theta}_B \pd \theta_B } & 
    \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{\theta}_C \pd \theta_B } \\
    \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{\theta}_A \pd \theta_C } &
    \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{\theta}_B \pd \theta_C } & 
    \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{\theta}_C \pd \theta_C }
  \end{bmatrix}
  \label{eq:nparam:lrm:bias:hessian}
\end{equation}
where every sub-block consists of the different second-order derivatives and exhibit a similar pattern.

The different contributions are given below:
\begin{align}
  \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{a_{i_1}} \pd a_{i_2}} & = 
  \frac{1}{\nWind} \sum_{\dW} \dW^{i_1+i_2} \abs{\true{\LocalVector{Y}}(\dW)}^2\\
  \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{b_{i_1}} \pd b_{i_2}} & = 
  \frac{1}{\nWind} \sum_{\dW} \dW^{i_1+i_2} \abs{\true{\LocalVector{U}}(\dW)}^2\\
  \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{c_{i_1}} \pd c_{i_2}} & = 
  \frac{1}{\nWind} \sum_{\dW} \dW^{i_1+i_2}\\
  \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{b_{i_1}} \pd a_{i_2}}  =
  \conj{ \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{a_{i_2}} \pd b_{i_1}} }& = 
  \frac{1}{\nWind} \sum_{\dW} \dW^{i_1+i_2} \conj{\true{\LocalVector{U}}(\dW)} \true{\LocalVector{Y}}(\dW)\\
  \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{c_{i_1}} \pd a_{i_2}}  =
  \conj{ \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{a_{i_2}} \pd c_{i_1}} }& = 
  \frac{1}{\nWind} \sum_{\dW} \dW^{i_1+i_2} \true{\LocalVector{Y}}(\dW)\\
  \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{c_{i_1}} \pd b_{i_2}}  =
  \conj{ \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{c_{i_2}} \pd b_{i_1}} }& = 
  \frac{1}{\nWind} \sum_{\dW} \dW^{i_1+i_2} \true{\LocalVector{U}}(\dW)
  \text{.}
\end{align}

\begin{remark}
Computing the inverse of this Hessian cannot, to the knowledge of the author, be reduced to a form that yields more insight.
However, if the non-diagonal blocks can be neglected, this would lead to an inverse that is also block-diagonal:
\begin{equation}
   \begin{bmatrix}
    \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{\theta}_A \pd \theta_A } &
    \deemph{0} & 
    \deemph{0} \\
    \deemph{0} &
    \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{\theta}_B \pd \theta_B } & 
    \deemph{0} \\
    \deemph{0} &
    \deemph{0} & 
    \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{\theta}_C \pd \theta_C }
  \end{bmatrix}^{-1}
  \!\!\!=
  \begin{bmatrix}
    \left( \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{\theta}_A \pd \theta_A }\right)^{-1} &
    \deemph{0} & 
    \deemph{0} \\
    \deemph{0} &
    \left( \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{\theta}_B \pd \theta_B }\right)^{-1} & 
    \deemph{0} \\
    \deemph{0} &
    \deemph{0} & 
    \left( \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{\theta}_C \pd \theta_C }\right)^{-1}
  \end{bmatrix}
  % \text{.}
\end{equation}
\end{remark}
\paragraph{Bias on the \glsentrydesc{FRF}}
As a first order approximation, the bias on the \gls{FRF} can be written as
\begin{equation}
  \Delta \LocalModel{G}(\omega_k) 
  =
  \Delta \LocalModel{G}(\dw{0}{k}) 
     \approx 
        \PartialDerivative{\LocalModel{G}(\theta, \dw{0}{k})}
                                          {\theta} 
      \Delta \theta
      \text{.}
\end{equation}
The derivative in that expression can be evaluated easily
\begin{align}
    \PartialDerivative{\LocalModel{G}(\theta, \dW)}{\theta} 
     &=
     \begin{bmatrix}
       \PartialDerivative{\LocalModel{G}(\theta,\dW)}{\theta_A} &
       \PartialDerivative{\LocalModel{G}(\theta,\dW)}{\theta_B} &
       \PartialDerivative{\LocalModel{G}(\theta,\dW)}{\theta_C} 
     \end{bmatrix}
     \\
     \PartialDerivative{\LocalModel{G}(\theta, \dW)}{\theta_A} 
     &=
     - \frac{\LocalModel{B}(\theta, r)}{\LocalModel{A}^2(\theta, r)}
     \begin{bmatrix}
         \dW^1 & \dW^2 & \deemph{\cdots} & \dW^{\order{A}}
     \end{bmatrix}
     \\
     \PartialDerivative{\LocalModel{G}(\theta, \dW)}{\theta_B} 
     &=
     \frac{1}{\LocalModel{A}(\theta, \dW)}
     \begin{bmatrix}
         1 & \dW^1 & \dW^2 & \deemph{\cdots} & \dW^{\order{B}}
     \end{bmatrix}
     \\
     \PartialDerivative{\LocalModel{G}(\theta, \dW)}{\theta_C} 
     &=
     \Zero{1 \times \order{C}+1}
     \text{.}
\end{align}

However, the sparsity of $\Delta\theta$, see \eqref{eq:nparam:lrm:bias:sparse}, can be exploited such that the bias on the \gls{FRF} can be expressed as:
\begin{equation}
  \Delta \LocalModel{G}(\theta,\dW)
  \approx
  \frac{\LocalModel{G}(\theta, \dW)}
           {\LocalModel{A}(\theta, \dW)}
  \begin{bmatrix}
    \dW^1 & \deemph{\cdots} & \dW^{\order{A}}
  \end{bmatrix}
  \Delta \theta_A
  \text{.}
\end{equation}
Equivalently, the relative bias of the \gls{FRF} can be written as
\begin{equation}
\frac{\Delta \LocalModel{G}(\theta,\dW)}{\LocalModel{G}(\theta,\dW)}
\approx
\LocalModel{A}(\theta,\dW)^{-1}
\begin{bmatrix}
    \dW^1 & \deemph{\cdots} & \dW^{\order{A}}
  \end{bmatrix}
  \left(  \PartialDerivative[2]{\ELSCost{\theta}}{ \theta^2 } \right)^{-1}
  \begin{bmatrix}
  \Delta\theta_A\\
  0\\
  0\\
  \end{bmatrix}
  \text{.}
\end{equation}


\section{Simulations}
\label{sec:simulations}

In the simulations, we consider a discrete-time second order system with transfer function
\begin{equation}
\true{G}(z) = \frac{0.64587 z + 0.64143}
                                      {47.9426 z^2 - 51.2955 z + 46.9933}
\end{equation}


\begin{figure}
 \centering
  \input{\thisDir/figs/generalized-output-error.tikz}
  \caption{Block schematic used in the simulations}
\end{figure}

\TODO{H0}    

The \gls{SNR} in the system bandwidth $\BW$ is defined as
\begin{equation}
  \SNR_{\BW} \isdef
  \frac{\int_{\BW} \abs{\true{Y}(\omega)}^2 \dd{\omega}}
            {\int_{\BW} \abs{V(\omega)}^2 \dd{\omega}}
\end{equation}
where $\BW$ is the $3 \unit{dB}$ bandwidth of the system.

\begin{figure}
  \centering
  \setlength{\figurewidth}{0.85\columnwidth}
  \setlength{\figureheight}{0.62\figurewidth}
  \TODOfig[Example spectra]
  \label{fig:nparam:spectra}
\end{figure}

We then compute basic statistics of the estimated \glspl{FRF} $\model[\atSimulation{i}]{\bullet}$ over all  Monte Carlo runs.
Specifically, the following values
\begin{align}
  \sampleMean{\bullet}(\omega_k) & 
  \isdef
    \frac{1}{\nMC}
    \sum_{i=1}^{\nMC}
    \model[\atSimulation{i}]{\bullet}(\omega_k)
  \\
  \sampleBias{\bullet}(\omega_k) &
    \isdef
    \frac{1}{\nMC}
    \sum_{i=1}^{\nMC}
    \model[\atSimulation{i}]{\bullet}(\omega_k) - \true{G}(\omega_k)
    = 
    \sampleMean{\bullet}(\omega_k) - \true{G}(\omega_k)
    \\
    \sampleVariance{\bullet}(\omega_k) &
    \isdef
    \frac{1}{\nMC - 1}
    \sum_{i=1}^{\nMC}
                   \left({\model[\atSimulation{i}]{\bullet}}(\omega_k) - \true{G}(\omega_k) \right)
    \conj { \left({\model[\atSimulation{i}]{\bullet}}(\omega_k) - \true{G}(\omega_k) \right) }
\end{align}
are computed and are the sample estimates of respectively the expected value $\E{\model{\bullet}}$, bias $\bias{\model{\bullet}}$ and variance $\Var{\model{\bullet}}$ of the model.
Remark, that the standard deviation on the $\sampleBias{\bullet} \approx \sqrt{\sampleVariance{\bullet} / \nMC}$ which limits the precision with which we can hence observe $\sampleBias{\bullet}$.

  % \subsection{Implications of the model structure}
  \subsection{Implications of the noise level}
  % \subsection{Implications of the noise coloring}

\begin{figure}
  \centering
  \setlength{\figurewidth}{0.85\columnwidth}
  \setlength{\figureheight}{0.62\figurewidth}
  \TODOfig[Y(Y,E,T) vs freq of single simulation]
\end{figure}

\begin{figure}
  \centering
  \setlength{\figurewidth}{0.85\columnwidth}
  \setlength{\figureheight}{0.62\figurewidth}
  \TODOfig[RMSE, var, bias for (SNR, NBW) over freq]
\end{figure}

\TODO{describe aggregation over $\omega$}

\begin{table}
\centering
\caption{Local modeling methods used in the Monte Carlo analysis.}
\label{tbl:nparam:methods}
\begin{tabular}{lccccr} 
\toprule
\textsc{Method} & \textsc{Color} & $\nWind$ & $\nth$ & \DOF & $\Delta\SNR \axisunit{dB}$ \\
\midrule
 $\ETFE$ &\color{etfe} --- & 1  & 1  & 0 & 0 \\
 $\lrm{7,1,1,1}$ & \color{lrm7111}\bfseries --- & 15  & 5 & 10  & 4.77 \\
 $\lric{7,2,2,2}$ &\color{lric7222} \bfseries --- &   15 & 8 & 7  & 2.73 \\
 $\lrm{7,2,2,2}$ &\color{lrm7222} \bfseries --- &   15 & 8 & 7  & 2.73 \\
 $\lrm{6,2,2,2}$ &\color{lrm6222} \bfseries --- &   13 & 8 & 5  & 2.11 \\
 $\lpm{7,4,4}$ &\color{lpm744} \bfseries --- &   15 & 10 & 5  & 1.76 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Noiseless case}
In the noiseless case, one can observe the approximation error that is made by using the \gls{LRM}, \gls{LPM}, \gls{LRIC} and \gls{ETFE} as shown in \figref{fig:nparam:comparison:noiseless}.
For the \gls{ETFE}, it can be seen that the error is very high.
This can be expected since the \gls{ETFE} does not separate transient/leakage contributions.
Consequently, the leakage contributions (which change over the different Monte Carlo runs), lead to a large variance error for the \gls{ETFE}.
The approximation error of the \gls{LPM} for short datasets ($\nBW \leq 8$) is dominated by the bias error.
The is to be expected since the resonance in the band of interest is modeled using only polynomials instead of the full rational form.
For longer datasets, 
For the \gls{LRM} and \gls{LRIC}, the \gls{RMS} error is already very low for small datasets ($-64 \unit{dB}$ for the first-order method, $-114\unit{dB}$ for second-order methods).
For short datasets ($\nBW \leq 16$), the $\lrm{7,1,1,1}$ error has a bias contribution that is slightly larger ($1$ to $4\unit{dB}$) than the variance contribution.
For larger datasets ($\nBW \geq 16$), this bias contribution starts to diminish significantly as $\bigO{\nBW^{-2}}$.
For second-order rational methods, the error is dominated by the variance and is about $50\unit{dB}$ smaller than for the first-order methods.
Practically, this approximation error is very likely to be negligible in measurements as such an error level corresponds to the quantization noise level of a $19\unit{bit}$ \gls{ADC}.

\begin{figure}
  \centering
  \setlength{\figurewidth}{0.85\columnwidth}
  \setlength{\figureheight}{0.62\figurewidth}
  \input{\thisDir/figs/MC-SNR-Inf.tikz}
  \caption[Comparison of local models for $\SNR = \infty$]{Local modeling of a discrete-time system results in an approximation error that is low enough for many practical uses. See \tabref{tbl:nparam:methods} for the color legend.}
  \label{fig:nparam:comparison:noiseless}
\end{figure}

\paragraph{Good signal-to-noise ratios}
The next situation we consider is when $\SNR=60\unit{dB}$ as shown in \figref{fig:nparam:comparison:hiSNR}.
Here, the \gls{LPM} remains severly biased for small datasets ($\nBW < 12$).
However, for sufficiently long experiments ($\nBW > 32$), the bias becomes negligible.
For the \gls{LRM} and \gls{LRIC}, the bias is always much smaller than the variance.
In contrast to \gls{LPM}, the \gls{LPM} already reaches a constant variance level when $\nBW > 10$.
It should also be noted that for the \gls{LRIC}, the variance is almost $10\unit{dB}$ more than the corresponding \gls{LPM} method.
It is hence not advantageous to use the \gls{LRIC} as-is.
For all methods (except the \gls{ETFE} and \gls{LRIC}), the simulations show that the \gls{RMSE} reaches
\begin{equation}
  \lim_{\nBW \to \infty} \RMSE \approx \frac{\SNR}{\Delta \SNR}
\end{equation}
where
\begin{equation}
  \Delta\SNR = \sqrt{\frac{\nth}{\nWind}}
\end{equation}
accounts for the smoothing action of local modeling (see \tabref{tbl:nparam:methods} for numerical values of $\Delta \SNR$).

\begin{figure}[p]
  \centering
  \setlength{\figurewidth}{0.85\columnwidth}
  \setlength{\figureheight}{0.62\figurewidth}
  \input{\thisDir/figs/MC-SNR-60.tikz}
  \caption[Comparison of local models for $\SNR = 60 \unit{dB}$]{Comparison of bias and variance of local modeling for $\SNR=60\unit{dB}$. See \tabref{tbl:nparam:methods} for the color legend.}
  \label{fig:nparam:comparison:hiSNR}
\end{figure}

\begin{figure}[p]
  \centering
  \setlength{\figurewidth}{0.85\columnwidth}
  \setlength{\figureheight}{0.62\figurewidth}
  \input{\thisDir/figs/MC-SNR-40.tikz}
  \caption[Comparison of local models for $\SNR = 40 \unit{dB}$]{Comparison of bias and variance of local modeling for $\SNR=40\unit{dB}$. See \tabref{tbl:nparam:methods} for the color legend.}
  \label{fig:nparam:comparison:hiSNR}
\end{figure}

\paragraph{Poor signal-to-noise ratios}
In \figref{fig:nparam:comparison:lowSNR} and \figref{fig:nparam:comparison:terribleSNR}, the results are shown for $\SNR=20\unit{dB}$ and $\SNR=10\unit{dB}$ respectively.
Again, it can be seen that \gls{LRIC} is plagued by high variance and is hence not directly usable.


\begin{figure}[p]
  \centering
  \setlength{\figurewidth}{0.85\columnwidth}
  \setlength{\figureheight}{0.62\figurewidth}
  \input{\thisDir/figs/MC-SNR-20.tikz}
  \caption[Comparison of local models for $\SNR = 20 \unit{dB}$]{Comparison of bias and variance of local modeling for $\SNR=20\unit{dB}$. See \tabref{tbl:nparam:methods} for the color legend.}
  \label{fig:nparam:comparison:lowSNR}
\end{figure}

\begin{figure}[p]
  \centering
  \setlength{\figurewidth}{0.85\columnwidth}
  \setlength{\figureheight}{0.62\figurewidth}
  \input{\thisDir/figs/MC-SNR-10.tikz}
 \caption[Comparison of local models for $\SNR = 10 \unit{dB}$]{Comparison of bias and variance of local modeling for $\SNR=10\unit{dB}$. See \tabref{tbl:nparam:methods} for the color legend.}
  \label{fig:nparam:comparison:terribleSNR}
\end{figure}

\begin{guideline}[Use the LRM for resonant systems when $\SNR > 10 \unit{dB}$]
For resonant systems, the \gls{LRM} offers better performance than \gls{LPM} when a good \gls{SNR} is obtained in the frequency domain.
When the \gls{SNR} is poor ($\leq 10\unit{dB}$), the \gls{LPM} may perform a bit better than \gls{LRM}.
\end{guideline}

\begin{guideline}[For long measurements, LPM and LRM are similar]
For very long measurements, the difference between \gls{LPM} and \gls{LRM} becomes negligible if the different resonances are well-separated.
\end{guideline}

\begin{remark}
In all the simulations, it can be seen that for $\nWind \to \infty$, the error tapers off.
\TODO{andere pieken gaan in de praktijk alles om zeep helpen en dus bovengrens stellen}
\end{remark}

  % \section{Measurements}
  % \label{sec:measurements}
  %  \subsection{AVIS}
   % \subsection{F16}

