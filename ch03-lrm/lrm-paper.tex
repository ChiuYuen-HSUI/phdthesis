Estimating a non-parametric \gls{FRF} is often a key step in the analysis and/or design of physical systems.
However, obtaining a good-quality non-parametric \gls{FRF} from a measured input-output data set can prove difficult due to the presence of noise and measurement time limitations to observe lightly-damped dynamics well.

Recently, there has been a renewed interest in the development of non-parametric estimators to estimate the \gls{FRF} of a system.
Many of these approaches have focused on separating the periodic response (i.e. the \gls{FRF}), the measurement noise and leakage contributions of the spectrum by exploiting the structure that those different contributions exhibit.


\citep{Pintelon2010LPM1},\citep{Pintelon2010LPM2},\citep{McKelvey2012LRM}


% \paragraph*{Contributions}
% In this paper, the existing \gls{LRM} is extended to the \gls{LRIC} that does not suffer from  the bias that is present in the \gls{LRM}.
% An extensive study is carried out to compare the performance of the \gls{LPM}, \gls{LRM} and \gls{LRIC} to describe resonant systems under varying conditions of the signal-to-noise ratio.
% Also, a pragmatic model-order-selection tool for the \gls{LPM} and \gls{LRM} is introduced.

\paragraph*{Outline}
\secref{sec:theory} introduces theory of estimating \glspl{FRF} and the relevant local modeling techniques.
In \secref{sec:simulations} the performance of the different modeling methods is investigated by means of simulations.
The methods are then illustrated on measurements of mechanical systems in \secref{sec:measurements}.
Finally, conclusions are presented in \secref{sec:conclusion}.


\begin{itemize}
    \item \TODO{choice $s$ or $z$}
    \item \TODO{influence of noise filter: mismatch with model}
\end{itemize}

\section{Local modeling approaches}
\label{sec:theory}

Consider a discrete-time generalized output-error \gls{LTI} set-up, excited by the input signal $u[n]$.
For an infinite data length, such a system can be described in the time domain by
\begin{equation}
  y[n] = \ezbrace{\true{G}(z) u[n]}{\true{y}[n]} + \ezbrace{\true{H}(z) \true{e}[n]}{v[n]} \text{ with } n \in \IntegerNumbers
  \label{eq:output-error-TD-infinite}
\end{equation}
where $v[n]$ is filtered white noise, $v[n] = \true{H}(z) \true{e}[n]$ where $z^{-1}$ is the backward shift operator, i.e. $z^ {-1}x[n] = x[n-1]$.
The transfer functions $\true{G}$ and $\true{H}$ are rational functions that are stable and causal.

During typical measurements, however, $y[n]$ and $u[n]$ are only measured over a limited time span, i.e. $n \in \set{0,1,\ldots,N-1}$.
This introduces transient terms $t_{\bullet}$ in this relationship~\citep{Pintelon1997ARB}:
\begin{equation}
y[n] = \true{G}(z) u[n] + \true{H}(z) \true{e}[n] + \ezbrace{t_G[n] + t_H[n]}{t[n]}
\label{eq:output-error-TD-finite}
\end{equation}
where $t_G[n]$ and $t_H[n]$ are due to the different conditions of respectively $\true{G}$ and $\true{H}$ at the beginning and end of the measurement record~\citep{Pintelon1997ARB}.
Both terms can be lumped together as $t[n]$, which is often~\citep{Pintelon2010LPM1} determined predominantly by $t_G[n]$.

\begin{definition}\label{def:DFT}
The $N$-points \glsfirst{DFT} of a signal $x(n\Ts) = x[n]$ with $n \in \set{0,\ldots,N-1}$ is
\begin{equation}
  X \left[ k \right] =
  X \left( \omega_k \right)
  \isdef
  \frac{1}{\sqrt{N}}\sum_{n=0}^{N-1} x \left( n \Ts \right)  \exponent{- j \omega_k n \Ts }
  \label{eq:DFT}
\end{equation}
where $\omega_k \isdef \tfrac{2 \pi k }{N \Ts}$, $k \in \set{0,\ldots,N-1}$ and $\Ts$ is the sampling time.
\end{definition}

By applying the \gls{DFT}~\eqref{eq:DFT} to both sides of \eqref{eq:output-error-TD-finite}, one obtains its frequency-domain counterpart:
\begin{align}
    Y \left( \omega_k \right) 
    & = \true{G} \left( \omega_k \right) U \left( \omega_k \right) 
      + \true{H} \left( \omega_k \right) E \left( \omega_k \right)
      + T \left( \omega_k \right)\\
      &= \true{G} \left( \omega_k \right) U \left( \omega_k \right)  + V(\omega_k) + T(\omega_k)
      \text{.}
  \label{eq:output-error-FD}
\end{align}

In this paper, we focus on using local modeling to separate the different terms, i.e.
\begin{itemize}
  \item the transfer function $\true{G}$,
  \item the leakage term $T$, and
  \item the noise term $V$.
\end{itemize}

\subsection{Local modeling}
Local modeling methods  exploit the `smoothness' (or conversely `roughness') of the different terms of \eqref{eq:output-error-FD} over the frequency $\omega$.
In particular, we will assume that $U(\omega)$ is `rough' over the frequency~\citep{Schoukens2009LPM}, e.g. as is the case for noise excitations or random phase multisines (see further).
On the other hand, it is well-known that the transfer functions $\true{G}$, $\true{H}$ and hence also the corresponding transient contributions $T = T_G + T_H$ are smooth over the frequency.
As such, these smooth contributions can be approximated well around each frequency $\omega_k$ using a local rational model:
\begin{align}
  \true{G}(\omega_{k}+d) 
  &\approx
  \frac{\Sum_{i=0}^{\order{B}} b_i d^i}
            {1 + \Sum_{i=1}^{\order{A}} a_i d^i}
    &\isdef
    \frac{\LocalModel[k]{B}(d)}%
           {\LocalModel[k]{A}(d)} 
           = \LocalModel[k]{G}(d)
  \text{,}
  \label{eq:LRM:model:G}
  \\
  \true{T}(\omega_k + d) &\approx
  \frac{\Sum_{i=0}^{\order{C}} c_i d^i}
            {1 + \Sum_{i=1}^{\order{A}} a_i d^i}
    &\isdef 
      \frac{\LocalModel[k]{C}(d)}%
           {\LocalModel[k]{A}(d)}
      = \LocalModel[k]{T}(d)
  \text{.}
  \label{eq:LRM:model:T}
\end{align}

In these expressions, we denote $\LocalModel[k]{G}$ and $\LocalModel[k]{T}$ for the local model for respectively transfer function and the transient.
Note that such local quantities (which depend on the frequency bin $k$) are denoted in bold.
To alleviate the notations, the subscript $k$ is omitted unless 

The local parameters $\theta$  
\begin{equation}
\theta \isdef 
\begin{bmatrix}
\theta_A\\ 
\theta_B\\
\theta_C
\end{bmatrix}
\qquad \text{ with }
\theta_A \isdef
\begin{bmatrix}
a_1\\ \vdots\\ a_{\order{A}}
\end{bmatrix}
\text{, }
\theta_B \isdef
\begin{bmatrix}
b_0\\ \vdots\\ b_{\order{B}}
\end{bmatrix}
\text{, and }
\theta_C \isdef
\begin{bmatrix}
c_0\\ \vdots\\ c_{\order{C}}
\end{bmatrix}
\end{equation}
in \eqref{eq:LRM:model:G} and \eqref{eq:LRM:model:T}, we consider a local window
\begin{equation}
  \LocalWindow[k] 
  \isdef
  \Set{
    \omega_{k+r} 
    | 
    r \in \LocalShifts{k}
  }
\end{equation}
with $\LocalShifts{k} \isdef \Set{-\numel{W}, -\numel{W}+1, \ldots, 0, \ldots, \numel{W}}$
such that the local window around $\omega_k$ consists of $\numel{\LocalWindow} = 2 \numel{W} + 1$ bins. 
If one denotes the input/output spectra in such a local frequency window $\LocalWindow[k]$ as
\begin{equation}
  \LocalVector{U}_k \isdef 
  \mat{
    U(\omega_{k-N_W})\\
    \vdots\\
    U(\omega_{k})\\
    \vdots\\
    U(\omega_{k+N_W})\\
  }
  \text{ and }
  \LocalVector{Y}_k \isdef 
  \mat{
    Y(\omega_{k-N_W})\\
    \vdots\\
    Y(\omega_{k})\\
    \vdots\\
    Y(\omega_{k+N_W})\\
  }
  \text{,}
\end{equation}
and similarly for the other quantities, equation \eqref{eq:output-error-FD} limited to $\LocalWindow[k]$ can be written as
\begin{equation}
  \LocalVector{Y}_k  = {\true{\LocalVector{G}}}_k \hadamard \LocalVector{U}_k + \LocalVector{T}_k + \LocalVector{V}_k
\text{,}
\end{equation} 
where $\hadamard$ denotes the element-wise product (also, Hadamard product).
Substituting $\true{\LocalVector{G}}$ and $\LocalVector{T}$ with the local models $\LocalModel{G}$ and $\LocalModel{T}$ and neglecting the influence of $\LocalVector{V}$ yields
\begin{equation}
  \LocalVector{Y} 
  \approx 
  \hat{\LocalVector{Y}} 
    =
      \LocalModel{G} \hadamard \LocalVector{U} + \LocalModel{T}
      \text{.}
      \label{eq:local-system-equations}
\end{equation}
Note that this encompasses $\nWind$ complex equations in the $\nth = \numel{A} + \numel{B} + \numel{C} + 2$  unknown complex model parameters ($a_i$, $b_i$ and $c_i$ in \eqref{eq:LRM:model:G} and \eqref{eq:LRM:model:T}).
Consequently, a necessary condition to compute the model parameters is that
\begin{equation}
  \DOF = \nWind - \nth
       = 2 \numel{W} - \numel{A} - \numel{B} - \numel{C} - 1
\end{equation}
is positive.
To effectively compute the local model parameters, the equation error in \eqref{eq:local-system-equations} is minimized by formulating a quadratic cost function:
\begin{equation}
  \LocalVector{\CostFunc{\LRIC}} 
    \isdef 
      \left( \LocalVector{Y}  -  \LocalModel{G} \hadamard \LocalVector{U} - \LocalModel{T} \right)^{\HT} 
      \left( \LocalVector{Y}  -  \LocalModel{G} \hadamard \LocalVector{U} - \LocalModel{T} \right)
   \label{eq:costfunc:LRIC}
   \text{.}
\end{equation}
Due to the presence of the denominator of $\LocalModel{G} = \frac{\LocalModel{B}}{\LocalModel{A}}$, the equation error is not linear in the parameters $a_i$ and hence \eqref{eq:costfunc:LRIC} requires time-consuming iterative optimization schemes to obtain a parameter estimate.

\subsection{The Local Rational Method}
\label{sec:nparam:LRM}
The \gls{LRM} as first introduced by \citep{McKelvey2012LRM}, overcomes the computational burden of an iterative procedure by weighting the equation error by the denominator polynomial $\LocalModel{A}$  akin to the procedure in \citep{Levy1959}.
I.e. the \gls{LRM} procedure tries to minimize the equation error in
\begin{equation}
  \LocalModel{A} \hadamard \LocalVector{Y} = \LocalModel{B} \hadamard \LocalVector{U}  + \LocalVector{C} + \LocalVector{V}
  \text{,}
\end{equation}
for which the corresponding cost function is
\begin{equation}
  \LocalVector{\CostFunc{\LRM}}
  \isdef 
  \left( \LocalModel{A} \hadamard \LocalVector{Y}  -  \LocalModel{B} \hadamard \LocalVector{U} - \LocalModel{C} \right)^{\HT} 
      \left( \LocalModel{A} \hadamard \LocalVector{Y}  -  \LocalModel{B} \hadamard \LocalVector{U} - \LocalModel{C} \right)
      \text{.}
\end{equation}
where $\LocalVector{V}$ is vector consisting of \gls{iid} complex normally distributed variables~\citep{Gallager2008} that each have a variance $\sigma_V^2$, i.e. the disturbing noise is assumed white over the local frequency window.
Equivalently, the last equation can be rewritten as a linear regression problem
\begin{equation}
  \LocalVector{Y} = \LocalMatrix{K} \LocalVector{\theta} + \LocalVector{V}
\end{equation}
where $\LocalMatrix{K}$ is the so-called design matrix (or observation matrix):
\begin{align}
  \label{eq:nparam:designMatrix}
  \LocalMatrix{K} 
    & \isdef 
  \mat{
     \LocalMatrix{K}_A &
     \LocalMatrix{K}_B & 
     \LocalMatrix{K}_C
  }\\
  \LocalMatrix{K}_A 
    &\isdef
    \mat{
      \LocalVector{Y} \hadamard \LocalVector{d}^1 &
      \cdots &
      \LocalVector{Y} \hadamard \LocalVector{d}^{\numel{A}}
    }\\
  \LocalMatrix{K}_B 
    &\isdef
    \mat{
      \LocalVector{U} \hadamard \LocalVector{d}^0 &
      \cdots &
      \LocalVector{U} \hadamard \LocalVector{d}^{\numel{B}}
    }\\
  \LocalMatrix{K}_C
    &\isdef
    \mat{
      \LocalVector{d}^0 &
      \cdots &
      \LocalVector{d}^{\numel{C}}
    }\\
    \LocalVector{d}^{n} & \isdef
    \mat{
      (-\numel{W})^{n} &
      \cdots &
      (\numel{W})^{n}
    }^{\T}
    \text{ with $n \in \mathbb{Z}$.}
\end{align}
This formulation facilitates to solve the problem in a one-step approach:
\begin{equation}
  \LocalVector{\theta}_{\LRM} 
    \isdef \pinv{\LocalMatrix{K}} \LocalVector{Y}
    = \KTK{\LocalMatrix{K}} \LocalVector{Y}
\end{equation}
where $\pinv{\LocalMatrix{K}}$ denotes the Moore-Penroose pseudo-inverse of $\LocalMatrix{K}$.

\TODO{hier voortschrijven}


Since it is well-known that the Vandemonde structure in the design matrix \eqref{eq:nparam:designMatrix} leads to numerical ill-conditioning for higher model complexities, additional measures are taken to improve numerical conditioning.
To improve numerical conditioning~\citep{Pintelon2005} of the estimation problem, we substitute $d = \dw{r}{k}$ in equations \eqref{eq:LRM:model:G} and \eqref{eq:LRM:model:T}
\begin{equation}
\dw{r}{k} \isdef \frac{\omega_{k+r} - \omega_k}{\Delta \omega_k}
\label{eq:freqScaling}
\end{equation}
where
\begin{equation}
  \Delta \omega_k \isdef
  \max
  \Set{
    \abs{ \omega_k - \omega_j} |  \vphantom{\frac{a}{b}}  \omega_j \in \LocalWindow[k]
  }
\end{equation}
such that $\abs{\dw{r}{k}} \leq 1$ when $r \in \LocalShifts{k}$.

\TODO{define notations for LPM etc.}

\section{Theory: bias}
\label{sec:biascalc}
Since the local design matrix \eqref{eq:nparam:designMatrix} contains the noisy output spectrum $\LocalVector{Y}$ when $\numel{A} > 0$, the \gls{LRM} is expected to be a biased estimator.
In this section, we will derive expressions for the bias such that it can be quantified how important this bias is in different situations.

To determine the bias, an approach similar to \citep[Appendix A]{Guillaume1995} is followed.
Let us denote the expected value of the cost function over all measurements $\LocalVector{Z} = \mat{\LocalMatrix{Y}, \LocalMatrix{U}}$ as
\begin{align}
  \ELSCost{\theta}
              & \isdef 
                   \E[\LocalVector{Z}]%
                         {    
                                \LSCost{\LocalVector{\theta}, \LocalVector{Z}}} \\
              & = \ELSCost[0]{\theta} + \ELSCost[n]{\theta} \\
    \ELSCost[0]{\theta} & \isdef 
            \frac{1}{\nWind} 
                \sum_{r} 
                     \left| 
                             \LocalModel{A}(r, \LocalVector{\theta}) \true{\LocalVector{Y}}
                          - \LocalModel{B}(r, \LocalVector{\theta}) \true{\LocalVector{U}}
                          - \LocalModel{C}(r, \LocalVector{\theta}) 
                    \right|^2 \\
    \ELSCost[n]{\theta} & \isdef 
              \frac{1}{\nWind} 
                     \sum_{r} 
                              \left| \LocalModel{A}(r,\LocalVector{\theta}) \right|^2 
                              \sigma_V^2
\end{align}
where $\true{\LocalVector{U}}$ and $\true{\LocalVector{Y}}$ denote the `true' (noiseless) input-output spectra in the local window.
Note that this function is a quadratic function of $\LocalVector{\theta}$ and hence it can be represented exactly by its second order Taylor series around $\true\theta$:
\begin{multline}
  \ELSCost{\theta} = \ELSCost{\true\theta} 
  + \PartialDerivative{\ELSCost{\true\theta}}{\true\theta}  \left( \theta - \true\theta \right) %\\
  + \frac{1}{2} \left( \theta - \true\theta \right)^{\T} 
  \PartialDerivative[2]{\ELSCost{\true\theta}}{\true\theta} 
   \left( \theta - \true\theta \right)
   \text{.}
\end{multline}
The minimizer $\estimated\theta$ of this function can be obtained by equating the derivative
\begin{equation}
  \PartialDerivative{\ELSCost{\estimated\theta}}
                                          {\estimated\theta} 
  = 
  \left( \PartialDerivative{\ELSCost{\true\theta}}{\true\theta} \right)^{\T} 
  + \PartialDerivative[2]{\ELSCost{\true\theta}}{\true\theta} \left( \estimated\theta - \true\theta \right)
\end{equation}
to zero.
This is equivalent to 
\begin{equation}
  \left( \estimated\theta - \true\theta \right) 
  = 
  - \left(   \PartialDerivative[2]{\ELSCost{\true\theta}}{\true\theta} \right)^{-1}  
     \left( \PartialDerivative{\ELSCost{\true\theta}}{\true\theta} \right)^{\T}
  \text{.}
\end{equation}
which expresses the bias $\Delta\theta = \left( \estimated\theta - \true\theta \right)$ of the estimated parameters.
Remark that 
\begin{equation}
  \PartialDerivative{\ELSCost{\true\theta}}{\true\theta}  
  = 
  \PartialDerivative{\ELSCost[0]{\theta} + \ELSCost[n]{\theta} }{\true\theta} 
  = 
  \PartialDerivative{\ELSCost[n]{\theta} }{\true\theta} 
\end{equation}
since $\ELSCost[0]{\theta}$ is minimized in $\true\theta$ by definition.
As such, the bias on the parameters can be simplified to
\begin{equation}
  \Delta\theta = 
  - \left(   \PartialDerivative[2]{\ELSCost{\true\theta}}{\true\theta^2} \right)^{-1} 
     \left( \PartialDerivative{\ELSCost[n]{\true\theta}}{\true\theta} \right)^{\T}
  \text{.}
  \label{eq:nparam:bias-basic}
\end{equation}

Do note that the estimated parameters $\theta \in \ComplexMatrix{\nth \times 1}$ but that $\ELSCost{\theta}$ is a real-valued function.
In contrast to a complex-valued function, this requires additional attention when the derivatives are calculated~\citep{Messerschmitt2006} and~\citep[Section 15.9]{Pintelon2012} since $\ELSCost{\theta}$ is not an analytic function.
Particularly, the function $\ELSCost{\theta}$ is regarded as a two-dimensional function $\ELSCost{\theta,\conj{\theta}}$ where $\conj{\theta}$ and $\theta$ are considered as independent variables~\citep{Hjorungnes2007,Hjorungnes2011}.
The derivatives are then computed with respect to these variables.
% Let us introduce the notations
% \begin{equation}
%   \ReIm{X} 
%   \isdef 
%   \begin{bmatrix}
%     \real{X}\\
%     \imag{X}
%   \end{bmatrix}
%   \quad  \text{,} \quad
%   \ReImImRe{X} 
%   \isdef 
%   \begin{bmatrix}
%     \real{X} & -\imag{X}\\
%     \imag{X} & \hphantom{-} \real{X}
%   \end{bmatrix}
% \end{equation}
% and the shorthand $\ReIm{\theta} = \ReImTheta$.
% E.g. in \citet[Section 15.9]{Pintelon2012} and \citet[Section 5.1]{Seber2007}, it is stated that $\ReImImRe{X}$ is isomorphic with $X$ such that operations on a complex matrix $X$ can be written as equivalent operations on the real matrix $\ReImImRe{X}$.
% As a result,
% \begin{align}
%   \Delta\theta &=
%   \left( \mat{1 & j} \kron \Identity{\nth[]}  \right) 
%   \ReIm{\Delta\theta}
%   \\
%   \ReIm{\Delta\theta} &= 
%   - \ReImImRe{ \left(   \PartialDerivative[2]{\ELSCost{\true\theta}}{\true\theta^2} \right)^{-1} }
%     \ReIm{ \left( \PartialDerivative{\ELSCost[n]{\true\theta}}{\true\theta} \right)^{\T}}
% \end{align}
% where $\kron$ denotes the Kronecker product~\citep{Brewer1978}.
To facilitate the derivations, we rewrite the expected value of the cost function in terms of $\theta$ and $\conj{\theta}$:
\begin{align}
   \ELSCost[0]{\theta,\conj{\theta}} & =
      \frac{1}{\nWind}
           \sum_{r}
           \LocalModel{Q}(      r,        \LocalVector{\theta},  \true{      \LocalVector{Y}},  \true{      \LocalVector{U} })
           \LocalModel{Q}(\conj{r}, \conj{\LocalVector{\theta}}, \true{\conj{\LocalVector{Y}}}, \true{\conj{\LocalVector{U}}})
   \\
   \LocalModel{Q}(r, \LocalVector{\theta},  \true{\LocalVector{Y}},  \true{ \LocalVector{U} }) & \isdef
                 \LocalModel{A}(r, \LocalVector{\theta}) \true{\LocalVector{Y}}(r)
               - \LocalModel{B}(r, \LocalVector{\theta}) \true{\LocalVector{U}}(r)
               - \LocalModel{C}(r, \LocalVector{\theta})
    \\             
    \ELSCost[n]{\theta,\conj{\theta}} & =
              \frac{\sigma_V^2}{\nWind} 
                     \sum_{r} 
                               \LocalModel{A}(      r,        \LocalVector{\theta} ) 
                               \LocalModel{A}(\conj{r}, \conj{\LocalVector{\theta}}) 
\end{align}
since $\conj{ \LocalModel{A}(r,\theta)} =  \LocalModel{A}(\conj{r},\conj{\theta}) $, and similar for $\LocalModel{B}$ and $\LocalModel{C}$ and $\LocalModel{Q}$.
Remember also that $\theta = \mat{\theta_A^{\TT} & \theta_B^{\TT} & \theta_C^{\TT}}$.

\paragraph{Contributions to the first order derivative}
The last term in \eqref{eq:nparam:bias-basic}, i.e. the first order derivative,  is given by
\begin{equation}
  \PartialDerivative{\ELSCost[0]{\true\theta, \conj{\true\theta}}}{\theta}
  = 
  \begin{bmatrix}
  \PartialDerivative{\ELSCost[0]{\true\theta, \conj{\true\theta}}}{\theta_A} &
  \PartialDerivative{\ELSCost[0]{\true\theta, \conj{\true\theta}}}{\theta_B} &
  \PartialDerivative{\ELSCost[0]{\true\theta, \conj{\true\theta}}}{\theta_C}
  \end{bmatrix}
\end{equation}
and split into the parts pertaining to respectively $\LocalModel{A}$, $\LocalModel{B}$ and, $\LocalModel{C}$.
The respective components are given by:
\begin{align}
  \PartialDerivative{\ELSCost[n]{\theta,\conj{\theta}}}{a_i} 
     &= 
     \frac{\sigma_V^2}{\nWind}
     \sum_r r^i \LocalModel{A}(\conj{r},\conj{\LocalVector{\theta}}) 
     && \forall i \in \set{1,\ldots,\order{A}}
     \\
   \PartialDerivative{\ELSCost[n]{\theta,\conj{\theta}}}{b_i} 
   &= 0
   && \forall i \in \set{0,\ldots,\order{B}}
   \\
   \PartialDerivative{\ELSCost[n]{\theta,\conj{\theta}}}{c_i} 
   &= 0
   &&\forall i \in \set{0,\ldots,\order{C}}
   \text{.}
\end{align}
It can be seen that only the factors pertaining to $\theta_{A}$ are present.
As this term occurs linearly in expression \eqref{eq:nparam:bias-basic}, only $\theta_A$ can incur a bias.
Consequently, only the poles can shifted by the presence of noise.
As such, the bias term $\Delta \theta$ can be written in the following sparse form:
\begin{equation}
  \Delta \theta = 
  \begin{bmatrix}
    \Delta \theta_A\\
    \Delta \theta_B\\
    \Delta \theta_C
  \end{bmatrix}
  =
  \begin{bmatrix}
    \Delta \theta_A\\
    \deemph{\Zero{\order{B}+1\times1}}\\
    \deemph{\Zero{\order{C}+1\times1}}
  \end{bmatrix}
  \text{.}
  \label{eq:nparam:lrm:bias:sparse}
\end{equation}

\TODO{write out subexpressions}
\paragraph{Bias on the \glsentrydesc{FRF}}
As a first order approximation, the bias on the \gls{FRF} can be written as
\begin{equation}
  \Delta \LocalModel{G}(\omega) 
     \approx 
        \PartialDerivative{\LocalModel{G}(\theta, \omega)}
                                          {\theta} 
      \Delta \theta
      \text{.}
\end{equation}
The derivative in that expression can be evaluated easily
\begin{align}
    \PartialDerivative{\LocalModel{G}(\theta, r)}{\theta} 
     &=
     \begin{bmatrix}
       \PartialDerivative{\LocalModel{G}(\theta,r)}{\theta_A} &
       \PartialDerivative{\LocalModel{G}(\theta,r)}{\theta_B} &
       \PartialDerivative{\LocalModel{G}(\theta,r)}{\theta_C} 
     \end{bmatrix}
     \\
     \PartialDerivative{\LocalModel{G}(\theta, r)}{\theta_A} 
     &=
     - \frac{\LocalModel{B}(\theta, r)}{\LocalModel{A}^2(\theta, r)}
     \begin{bmatrix}
         r^1 & r^2 & \deemph{\cdots} & r^{\order{A}}
     \end{bmatrix}
     \\
     \PartialDerivative{\LocalModel{G}(\theta, r)}{\theta_B} 
     &=
     \frac{1}{\LocalModel{A}(\theta, r)}
     \begin{bmatrix}
         1 & r^1 & r^2 & \deemph{\cdots} & r^{\order{B}}
     \end{bmatrix}
     \\
     \PartialDerivative{\LocalModel{G}(\theta, r)}{\theta_C} 
     &=
     \Zero{1 \times \order{C}+1}
     \text{.}
\end{align}

However, the sparsity of $\Delta\theta$, see \eqref{eq:nparam:lrm:bias:sparse}, can be exploited such that the bias on the \gls{FRF} can be expressed as:
\begin{equation}
  \Delta \LocalModel{G}(\theta,r)
  \approx
  \frac{\LocalModel{G}(\theta, r)}{\LocalModel{}}
\end{equation}

\section{Simulation}
\label{sec:simulations}

In the simulations, we consider a discrete-time second order system with transfer function
\begin{equation}
\true{G}(z) = \frac{0.64587 z + 0.64143}
                                      {47.9426 z^2 - 51.2955 z + 46.9933}
\end{equation}


\begin{figure}
 \centering
  \input{\thisDir/figs/generalized-output-error.tikz}
  \caption{Block schematic used in the simulations}
\end{figure}

\TODO{H0}    

The \gls{SNR} in the system bandwidth $\BW$ is defined as
\begin{equation}
  \SNR_{\BW} \isdef
  \frac{\int_{\BW} \abs{Y_0(\omega)}^2 \dd{\omega}}
            {\int_{\BW} \abs{V(\omega)}^2 \dd{\omega}}
\end{equation}
where $\BW$ is the frequency range of the $3 \unit{dB}$ bandwidth of the system, i.e. where $\abs{G_0} \geq \norm[\infty]{\true G} - 3 \unit{dB}$.

\begin{figure}
  \centering
  \setlength{\figurewidth}{0.85\columnwidth}
  \setlength{\figureheight}{0.62\figurewidth}
  \TODOfig[Example spectra]
  \label{fig:nparam:spectra}
\end{figure}

We then compute basic statistics of the estimated \glspl{FRF} $\model[\atSimulation{i}]{\bullet}$ over all  Monte Carlo runs.
Specifically, the following values
\begin{align}
  \sampleMean{\bullet}(\omega_k) & 
  \isdef
    \frac{1}{\nMC}
    \sum_{i=1}^{\nMC}
    \model[\atSimulation{i}]{\bullet}(\omega_k)
  \\
  \sampleBias{\bullet}(\omega_k) &
    \isdef
    \frac{1}{\nMC}
    \sum_{i=1}^{\nMC}
    \model[\atSimulation{i}]{\bullet}(\omega_k) - \true{G}(\omega_k)
    = 
    \sampleMean{\bullet}(\omega_k) - \true{G}(\omega_k)
    \\
    \sampleVariance{\bullet}(\omega_k) &
    \isdef
    \frac{1}{\nMC - 1}
    \sum_{i=1}^{\nMC}
                   \left({\model[\atSimulation{i}]{\bullet}}(\omega_k) - \true{G}(\omega_k) \right)
    \conj { \left({\model[\atSimulation{i}]{\bullet}}(\omega_k) - \true{G}(\omega_k) \right) }
\end{align}
are computed and are the sample estimates of respectively the expected value $\E{\model{\bullet}}$, bias $\bias{\model{\bullet}}$ and variance $\Var{\model{\bullet}}$ of the model.
Remark, that the standard deviation on the $\sampleBias{\bullet} \approx \sqrt{\sampleVariance{\bullet} / \nMC}$ which limits the precision with which we can hence observe $\sampleBias{\bullet}$.

  % \subsection{Implications of the model structure}
  \subsection{Implications of the noise level}
  % \subsection{Implications of the noise coloring}

\begin{figure}
  \centering
  \setlength{\figurewidth}{0.85\columnwidth}
  \setlength{\figureheight}{0.62\figurewidth}
  \TODOfig[Y(Y,E,T) vs freq of single simulation]
\end{figure}

\begin{figure}
  \centering
  \setlength{\figurewidth}{0.85\columnwidth}
  \setlength{\figureheight}{0.62\figurewidth}
  \TODOfig[RMSE, var, bias for (SNR, NBW) over freq]
\end{figure}

\TODO{describe aggregation over $\omega$}

\begin{table}
\centering
\caption{Local modeling methods used in the Monte Carlo analysis.}
\label{tbl:nparam:methods}
\begin{tabular}{lcccr} \toprule
\textsc{Method} & \DOF & $\nWind$ & $\nth$ & $\Delta$ SNR (dB)\\
\midrule
\color{etfe} $\ETFE$ & 1 & 0  & 1 & 0 \\
\color{lrm7111} $\lrm{7,1,1,1}$ & 15 & 10 & 5 & 4.77 \\
\color{lric7222} $\lric{7,2,2,2}$ &  15&7 & 8 & 2.73 \\
\color{lrm7222} $\lrm{7,2,2,2}$ &  15&7 & 8 & 2.73 \\
\color{lrm6222} $\lrm{6,2,2,2}$ &  13&5 & 8 & 2.11 \\
\color{lpm744} $\lpm{7,4,4}$ &  15&5 & 10 & 1.76 \\
\bottomrule
\end{tabular}
\end{table}


\paragraph{Noiseless case}
In the noiseless case, one can observe the approximation error that is made by using the \gls{LRM}, \gls{LPM} and \gls{ETFE}.
For the \gls{ETFE}, it can be seen that the error is very high.
This can be expected since the \gls{ETFE} does not separate transient and leakage contributions.
this error only disappears very slowly by measuring a longer record due to the presence of the leakage term.
As such, \TODO{afschrijven}

\begin{figure}
  \centering
  \setlength{\figurewidth}{0.85\columnwidth}
  \setlength{\figureheight}{0.62\figurewidth}
  \input{\thisDir/figs/MC-SNR-Inf.tikz}
  \caption[Comparison of local models for $\SNR = \infty$]{Local modeling of a discrete-time system results in an approximation error that is low enough for many practical uses.}
  \label{fig:nparam:comparison:noiseless}
\end{figure}

\paragraph{Good signal-to-noise ratios}

\begin{figure}
  \centering
  \setlength{\figurewidth}{0.85\columnwidth}
  \setlength{\figureheight}{0.62\figurewidth}
  \input{\thisDir/figs/MC-SNR-60.tikz}
  \caption[Comparison of local models for $\SNR = 60 \unit{dB}$]{The \gls{RMSE}, bias and variance of different local modeling methods is compared as a function of the number of points in the $3\unit{dB}$ bandwidth of the system. A relative approximation error of $-100 \unit{dB}$ is observed.}
  \label{fig:nparam:comparison:hiSNR}
\end{figure}

\paragraph{Poor signal-to-noise ratios}
\begin{figure}
  \centering
  \setlength{\figurewidth}{0.85\columnwidth}
  \setlength{\figureheight}{0.62\figurewidth}
  \input{\thisDir/figs/MC-SNR-20.tikz}
  \caption[Comparison of local models for $\SNR = 20 \unit{dB}$]{The \gls{RMSE}, bias and variance of different local modeling methods is compared as a function of the number of points in the $3\unit{dB}$ bandwidth of the system when $\SNR = 20\unit{dB}$. The \gls{LRM} outperforms the \gls{LPM}.}
  \label{fig:nparam:comparison:lowSNR}
\end{figure}

\begin{figure}
  \centering
  \setlength{\figurewidth}{0.85\columnwidth}
  \setlength{\figureheight}{0.62\figurewidth}
  \input{\thisDir/figs/MC-SNR-10.tikz}
  \caption[Comparison of local models for $\SNR = 10 \unit{dB}$]{The \gls{RMSE}, bias and variance of different local modeling methods is compared as a function of the number of points in the $3\unit{dB}$ bandwidth of the system when $\SNR = 20\unit{dB}$. An approximation error of $-100 \unit{dB}$ is observed.}
  \label{fig:nparam:comparison:terribleSNR}
\end{figure}

\begin{guideline}[Use the LRM when $\SNR \geq 20 \unit{dB}$ for resonant systems]
\end{guideline}

\begin{guideline}[Use the LPM when $\SNR \leq 20 \unit{dB}$]
\end{guideline}

\begin{guideline}[For long measurements, LPM and LRM are similar]
\end{guideline}

\begin{remark}
In all the simulations, it can be seen that for $\nWind \to \infty$, the error tapers off.
\TODO{andere pieken gaan in de praktijk alles om zeep helpen en dus bovengrens stellen}
\end{remark}

  \section{Measurements}
  \label{sec:measurements}
   \subsection{AVIS}
   % \subsection{F16}

