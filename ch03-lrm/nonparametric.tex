\chapter{Non-parametric Modeling}
\def\thisDir{ch03-lrm}
\tikzsetfigurename{ch03fig}
\label{sec:nonparametric}
\myEpigraph{I love deadlines. I like the whooshing sound they make as they fly by.
}{Douglas Adams}{}

\section{Introduction}
\label{sec:nonparametric:introduction}

Whereas many areas of system identification focus on parametric identification and/or parameter estimation, non-parametric techniques have received renewed interest from the community over the last decade.
One of the reasons for this is that non-parametric techniques typically require less assumptions and knowledge of the system than building parametric models.
As a consequence, non-parametric methods are often more robust than fully parametric methods.
Besides, \glspl{FRF} have been shown to provide a quick insight into the basic dynamic properties of \gls{LTI} systems.
This capability is very useful for system design, manual controller tuning, model validation and model selection for parametric models~\citep{Pintelon2012}.

The measurement of \glspl{FRF} of dynamic systems is an important step in many technical applications, often used as a simple visualization of the system dynamics.
However, obtaining a good non-parametric \gls{FRF} from a measured input-output data set can be challenging due to the presence of noise, leakage and measurement time limitations to observe lightly-damped dynamics well.
\gls{FRF} measurement techniques are discussed, for instance, in
\citep{Schoukens1998,Schoukens2006LPM,Guillaume1996,Broersen1995,Pintelon2010LPM1,Antoni2007FRF,Pintelon2012}, and applied to practical devices and systems~\citep{Lim2010,Robinson1990,Behjat2010}, among others.

In this chapter, we will investigate local modeling techniques to capture the \gls{FRF} of resonant systems.
In particular, a comparison between the \gls{LPM} and \gls{LRM} will be made such that it can be decided which method is to be preferred over the other in what conditions of \gls{SNR} and frequency resolution (or, equivalently, measurement time).

\paragraph*{Outline}
\secref{sec:theory} introduces theory of estimating \glspl{FRF} using local modeling techniques.
In \secref{sec:biascalc}, the bias of the \gls{LRM} is derived.
In \secref{sec:simulations} the performance of the different modeling methods are compared by means of simulations.
In \secref{sec:nparam:orderSelection}, some options for model order selection are discussed.
In \secref{sec:nonparametric:truncation}, an approach is developed to further reduce the variance of the \gls{LPM} estimate by truncation of the impulse response.
Finally, conclusions are presented in \secref{sec:conclusion}.

\section{Local modeling approaches}
\label{sec:theory}

Consider a discrete-time generalized output-error \gls{LTI} set-up, excited by the input signal $u[n]$.
For an infinite data length, such a system can be described in the time domain by
\begin{equation}
  y[n] = \ezbrace{\true{G}(q) u[n]}{\true{y}[n]} + \ezbrace{\true{H}(q) \true{e}[n]}{v[n]} \text{ with } n \in \IntegerNumbers
  \label{eq:output-error-TD-infinite}
\end{equation}
where $v[n]$ is filtered white noise, $v[n] = \true{H}(q) \true{e}[n]$ where $q^{-1}$ is the backward shift operator, i.e. $q^ {-1}x[n] = x[n-1]$.
The transfer functions $\true{G}$ and $\true{H}$ are rational functions that are causal and stable.

During typical measurements, however, $y[n]$ and $u[n]$ are only measured over a limited time span, i.e. $n \in \set{0,1,\ldots,N-1}$.
This introduces transient terms $t_{\bullet}$ in this relationship~\citep{Pintelon1997ARB}:
\begin{equation}
y[n] = \true{G}(q) u[n] + \true{H}(q) \true{e}[n] + \ezbrace{t_G[n] + t_H[n]}{t[n]}
\label{eq:output-error-TD-finite}
\end{equation}
where $t_G[n]$ and $t_H[n]$ are due to the different conditions of respectively $\true{G}$ and $\true{H}$ at the beginning and end of the measurement record~\citep{Pintelon1997ARB}.
Both terms can be lumped together as $t[n]$, which is often~\citep{Pintelon2010LPM1} determined predominantly by $t_G[n]$.

\begin{definition}\label{def:DFT}
The $N$-points \glsfirst{DFT} of a signal $x(n\Ts) = x[n]$ with $n \in \set{0,\ldots,N-1}$ is
\begin{equation}
  X \left[ k \right] =
  X \left( \omega_k \right)
  \isdef
  \frac{1}{\sqrt{N}}\sum_{n=0}^{N-1} x \left( n \Ts \right)  \exponent{- j \omega_k n \Ts }
  \label{eq:DFT}
\end{equation}
where $\omega_k \isdef \tfrac{2 \pi k }{N \Ts}$, $k \in \set{0,\ldots,N-1}$ and $\Ts$ is the sampling time.
\end{definition}

By applying the \gls{DFT}~\eqref{eq:DFT} to both sides of \eqref{eq:output-error-TD-finite}, one obtains its frequency-domain counterpart:
\begin{align}
    Y \left( \omega_k \right) 
    & = \true{G} \left( \omega_k \right) U \left( \omega_k \right) 
      + \true{H} \left( \omega_k \right) E \left( \omega_k \right)
      + T \left( \omega_k \right)\\
      &= \true{G} \left( \omega_k \right) U \left( \omega_k \right)  + V(\omega_k) + T(\omega_k)
      \text{.}
  \label{eq:output-error-FD}
\end{align}

Here, we focus on using local modeling to separate the different terms, i.e.
\begin{itemize}
  \item the transfer function $\true{G}$,
  \item the leakage term $T$, and
  \item the noise term $V$.
\end{itemize}
The main term of interest, however, is the transfer function $\true{G}$ and the noise variance $\sigma^2_{V}$.

\subsection{Local modeling}
Local modeling methods  exploit the `smoothness' (or conversely `roughness') of the different terms of \eqref{eq:output-error-FD} over the frequency $\omega$.
In particular, we will assume that $U(\omega)$ is `rough' over the frequency~\citep{Schoukens2009LPM}, e.g. as is the case for noise excitations or random phase multisines (see further).
On the other hand, it is well-known that the transfer functions $\true{G}$, $\true{H}$ and hence also the corresponding transient contributions $T = T_G + T_H$ are smooth over the frequency.
As such, these smooth contributions can be approximated well around each frequency $\omega_k$ using a local rational model:
\begin{align}
  \true{G}(\omega_{k}+d) 
  &\approx
  \frac{\Sum_{i=0}^{\order{B}} b_i(k) d^i}
            {1 + \Sum_{i=1}^{\order{A}} a_i(k) d^i}
    &\isdef
    \frac{\LocalModel[k]{B}(d)}%
           {\LocalModel[k]{A}(d)} 
           = \LocalModel[k]{G}(d)
  \text{,}
  \label{eq:LRM:model:G}
  \\
  \true{T}(\omega_k + d) &\approx
  \frac{\Sum_{i=0}^{\order{C}} c_i(k) d^i}
            {1 + \Sum_{i=1}^{\order{A}} a_i(k) d^i}
    &\isdef 
      \frac{\LocalModel[k]{C}(d)}%
           {\LocalModel[k]{A}(d)}
      = \LocalModel[k]{T}(d)
  \text{.}
  \label{eq:LRM:model:T}
\end{align}

In these expressions, we denote $\LocalModel[k]{G}$ and $\LocalModel[k]{T}$ for the local model for respectively transfer function and the transient.
Note that such local quantities (which depend on the frequency bin $k$) are denoted in bold.
To alleviate the notations, the subscript $k$ is omitted whenever this does not cause ambiguity. 

To estimate the local parameters $\theta$  
\begin{equation}
\theta \isdef 
  \begin{bmatrix}
  \theta_A\\ 
  \theta_B\\
  \theta_C
  \end{bmatrix}
\qquad \text{ with }
\theta_A \isdef
\begin{bmatrix}
a_1\\ \vdots\\ a_{\order{A}}
\end{bmatrix}
\text{, }
\theta_B \isdef
\begin{bmatrix}
b_0\\ \vdots\\ b_{\order{B}}
\end{bmatrix}
\text{, and }
\theta_C \isdef
\begin{bmatrix}
c_0\\ \vdots\\ c_{\order{C}}
\end{bmatrix}
\end{equation}
in \eqref{eq:LRM:model:G} and \eqref{eq:LRM:model:T}, we consider a local window
\begin{equation}
  \LocalWindow[k] 
  \isdef
  \Set{
    \omega_{k+r} 
    | 
    r \in \LocalShifts{k}
  }
\end{equation}
with $\LocalShifts{k} \isdef \Set{-\numel{W}, -\numel{W}+1, \ldots, 0, \ldots, \numel{W}}$
such that the local window around $\omega_k$ consists of $\nWind = 2 \numel{W} + 1$ bins. 
If one denotes the input/output spectra in such a local frequency window $\LocalWindow[k]$ as
\begin{equation}
  \LocalVector{U}_k \isdef 
  \mat{
    U(\omega_{k-\numel{W}})\\
    \vdots\\
    U(\omega_{k})\\
    \vdots\\
    U(\omega_{k+\numel{W}})\\
  }
  \text{ and }
  \LocalVector{Y}_k \isdef 
  \mat{
    Y(\omega_{k-\numel{W}})\\
    \vdots\\
    Y(\omega_{k})\\
    \vdots\\
    Y(\omega_{k+\numel{W}})\\
  }
  \text{,}
\end{equation}
and similarly for the other quantities, equation \eqref{eq:output-error-FD} limited to $\LocalWindow[k]$ can be written as
\begin{equation}
  \LocalVector{Y}_k  = {\LocalVector{G}}_k \hadamard \LocalVector{U}_k + \LocalVector{T}_k + \LocalVector{V}_k
\text{,}
\end{equation} 
where $\hadamard$ denotes the element-wise product (also, Hadamard product).
Substituting $\true{\LocalVector{G}}$ and $\LocalVector{T}$ with the local models $\LocalModel{G}$ and $\LocalModel{T}$ and neglecting the influence of $\LocalVector{V}$ yields
\begin{equation}
  \LocalVector{Y} 
  \approx 
  \hat{\LocalVector{Y}} 
    =
      \LocalModel{G} \hadamard \LocalVector{U} + \LocalModel{T}
      \text{.}
      \label{eq:local-system-equations}
\end{equation}
Note that this encompasses $\nWind$ complex equations in the $\nth = \order{A} + \order{B} + \order{C} + 2$  unknown complex model parameters ($a_i$, $b_i$ and $c_i$ in \eqref{eq:LRM:model:G} and \eqref{eq:LRM:model:T}).
Consequently, a necessary condition to compute the model parameters is that
\begin{equation}
  \DOF = \nWind - \nth
       = 2 \numel{W} - \order{A} - \order{B} - \order{C} - 1
       \label{eq:nparam:DOF}
\end{equation}
is positive.
To effectively compute the local model parameters, the equation error in \eqref{eq:local-system-equations} is minimized by formulating a quadratic cost function:
\begin{equation}
  \LocalVector{\CostFunc{\LRIC}} 
    \isdef 
      \left( \LocalVector{Y}  -  \LocalModel{G} \hadamard \LocalVector{U} - \LocalModel{T} \right)^{\HT} 
      \left( \LocalVector{Y}  -  \LocalModel{G} \hadamard \LocalVector{U} - \LocalModel{T} \right)
   \label{eq:costfunc:LRIC}
   \text{.}
\end{equation}
Due to the presence of the denominator of $\LocalModel{G} = \frac{\LocalModel{B}}{\LocalModel{A}}$, the equation error is not linear in the parameters $a_i$ and hence \eqref{eq:costfunc:LRIC} requires starting values and time-consuming iterative optimization schemes to obtain a parameter estimate.
We hence call this method the \gls{LRIC} and denote specific configurations of this method as $\lric{\numel{W},\order{B},\order{A},\order{C}}$.

Note that the estimated \gls{FRF} is given by evaluating the local model $\LocalModel{G}$:
\begin{equation}
  \hat{G}_{\LRIC}(\omega_k) \isdef \LocalModel[k]{G}(d=0)
  \text{.}
\end{equation}
In \figref{fig:nparam:illustration}, the local window $\LocalWindow[k]$, the local input-output spectra ($\LocalVector[k]{U}$ and $\LocalVector[k]{U}$) are illustrated.

\begin{figure}[htb] %  figure placement: here, top, bottom, or page
   \centering
   \setlength{\figurewidth}{0.8\columnwidth}
   \setlength{\figureheight}{0.68\figurewidth}
   \input{\thisDir/figs/LPMfig.tikz}
   \caption[Illustration of local modeling.]{Illustration of local modeling. Top figures: input spectrum (left) and output spectrum (right). 
   A small frequency window $\LocalWindow[k]$ is selected, in which the local model $\LocalModel[k]{G}$ is estimated. Its central value $\hat{G}(\omega_k)$ is an estimate of the \gls{FRF}. This procedure is repeated for all frequencies $\omega_k$. \disclaimer{Figure based on \citep[Fig.~2]{Lumori2014TIM}.}}
   \label{fig:nparam:illustration}
\end{figure}

\begin{remark}
It should be noted that \eqref{eq:LRM:model:G} and \eqref{eq:LRM:model:T} are essentially isomorphic to a continuous-time transfer function model with complex coefficients.
The discrete-time counterparts (i.e. substituting $d^i \to \exp (ji d \Ts)$ in the expressions), have been tried as well.
However, these preliminary experiments did not yield estimates that were numerically reliable.
This is in correspondence with the remark from \citet{Pintelon2006BJ1}.
\end{remark}

\subsection{The Local Rational Method}
\label{sec:nparam:LRM}
The \gls{LRM} as first introduced by \citet{McKelvey2012LRM}, overcomes the computational burden of an iterative procedure by weighting the equation error by the denominator polynomial $\LocalModel{A}$  akin to the procedure in \citep{Levy1959}.
I.e. the \gls{LRM} procedure tries to minimize the equation error in
\begin{equation}
  \LocalModel{A} \hadamard \LocalVector{Y} = \LocalModel{B} \hadamard \LocalVector{U}  + \LocalVector{C} + \LocalVector{V}
  \text{,}
\end{equation}
for which the corresponding cost function is
\begin{equation}
  \LocalVector{\CostFunc{\LRM}}
  \isdef 
  \left( \LocalModel{A} \hadamard \LocalVector{Y}  -  \LocalModel{B} \hadamard \LocalVector{U} - \LocalModel{C} \right)^{\HT} 
      \left( \LocalModel{A} \hadamard \LocalVector{Y}  -  \LocalModel{B} \hadamard \LocalVector{U} - \LocalModel{C} \right)
      \text{.}
      \label{eq:nparam:LRM:costFunc}
\end{equation}
where $\LocalVector{V}$ is vector consisting of \gls{iid} complex normally distributed variables~\citep{Gallager2008} that each have a variance $\sigma_V^2$, i.e. the disturbing noise is assumed white over the local frequency window.
Equivalently, the last equation can be rewritten as a linear regression problem
\begin{equation}
  \LocalVector{Y} = \LocalMatrix{K} \LocalVector{\theta} + \LocalVector{V}
  \label{eq:nparam:linearRegression}
\end{equation}
where $\LocalMatrix{K}$ is the so-called design matrix (or observation matrix):
\begin{align}
  \label{eq:nparam:designMatrix}
  \LocalMatrix{K} 
    & \isdef 
    \phantom{+}
  \mat{
     \LocalMatrix{K}_A &
     \LocalMatrix{K}_B & 
     \LocalMatrix{K}_C
  }\\
  \LocalMatrix{K}_A 
    &\isdef
    \phantom{+}
    \mat{
      \LocalVector{Y} \hadamard \LocalVector{d}^1 &
      \cdots &
      \LocalVector{Y} \hadamard \LocalVector{d}^{\order{A}}
    }\\
  \LocalMatrix{K}_B 
    &\isdef
    - \mat{
      \LocalVector{U} \hadamard \LocalVector{d}^0 &
      \cdots &
      \LocalVector{U} \hadamard \LocalVector{d}^{\order{B}}
    }\\
  \LocalMatrix{K}_C
    &\isdef
    - \mat{
      \LocalVector{d}^0 &
      \cdots &
      \LocalVector{d}^{\order{C}}
    }
    \text{.}
\end{align}
In this formulation, we have used $\LocalVector{d}^{n}$ to denote the $n^{\text{th}}$ Hadamard power of $\LocalVector{d}$ (this corresponds to \mcode{d.^n} in \MATLAB) such that
\begin{equation}
    \LocalVector{d}^{n} 
    \isdef
    \mat{
      (-\numel{W})^{n} &
      \cdots &
      (\numel{W})^{n}
    }^{\T}
    \text{ with $n \in \mathbb{Z}$}
\end{equation}
where every element corresponds to a value of $d^{n}$ in accordance with $\LocalShifts{k}$.

This formulation facilitates to solve the problem in a one-step approach:
\begin{equation}
  \estimated{\LocalVector{\theta}}_{\LRM} 
    \isdef \pinv{\LocalMatrix{K}} \LocalVector{Y}
    = \KTK{\LocalMatrix{K}} \LocalVector{Y}
    \label{eq:nparam:LRM:normalEquations}
\end{equation}
where $\pinv{\LocalMatrix{K}}$ denotes the Moore-Penroose pseudo-inverse of $\LocalMatrix{K}$.
Furthermore, it is possible to define the relationship between the measured output spectrum $\LocalVector{Y}$ and the periodic output spectrum $\estimated{\LocalVector{Y}}$:
\begin{equation}
  \estimated{\LocalVector{Y}} 
  = \LocalMatrix{K} \LocalVector{\theta}_{\LRM} 
  = \LocalMatrix{K} \pinv{\LocalMatrix{K}} \LocalVector{Y}
  = \LocalMatrix{H} \LocalVector{Y}
\end{equation}
where $\LocalMatrix{H}$ is sometimes called the `hat' matrix since it projects measurements ($\LocalVector{Y}$) onto the estimates ($\estimated{\LocalVector{Y}}$).
It should be noted that the hat matrix is idempotent~\citep[Section 2.1.1]{Cook1982}.

Following the approach in \citet[equation (12) and further]{Pintelon2010LPM1} or \citet[Chapter 2]{Cook1982}, the (ordinary) residuals $\LocalVector{E}$ of the fit are given by
\begin{equation}
  \LocalVector{E} = \LocalVector{Y} - \estimated{\LocalVector{Y}}
  = \LocalVector{Y} - \LocalMatrix{H}\LocalVector{Y}
  =  \left( \Identity{\nWind} - \LocalMatrix{H} \right) \LocalVector{Y}
  \text{.}
\end{equation}
Substitution of \eqref{eq:nparam:linearRegression} in this equation yields
\begin{align}
  \LocalVector{E}  
  &= (\I - \LocalMatrix{H})(\LocalMatrix{K} \LocalVector{\theta} + \LocalVector{V})\\
  &= (\I - \LocalMatrix{H}) \LocalVector{V} + (\I - \LocalMatrix{H})\LocalMatrix{K}\LocalVector{\theta}\\
  &= (\I - \LocalMatrix{H}) \LocalVector{V} + \LocalMatrix{K}\LocalVector{\theta} - \LocalMatrix{K} \pinv{\LocalMatrix{K}} \LocalMatrix{K} \LocalVector{\theta}\\
   &= (\I - \LocalMatrix{H}) \LocalVector{V}
\end{align}
since $\LocalMatrix{K} \pinv{\LocalMatrix{K}} \LocalMatrix{K} = \LocalMatrix{K}$ by construction of the pseudo-inverse~\citep{Penrose1955}.
This equation relates the residuals $\LocalVector{E}$ to the noise $\LocalVector{V}$, which aids to estimate the noise variance:
\begin{equation}
  \estimated{\sigma}^2_{\LocalVector{V}} =
    \frac{1}{\DOF} 
     \LocalVector{E}^{\HT} \LocalVector{E}
\end{equation}
as elaborated in~\citep[Appendix B]{Pintelon2010LPM1}.
Note that $\DOF$ indicates the degrees of freedom of the residuals as in \eqref{eq:nparam:DOF} and could also be computed as either the rank or the trace of the idempotent matrix $(\I - \LocalMatrix{H})$.

\begin{remark}
This system can only be solved reliably if the columns in \eqref{eq:nparam:designMatrix} are linearly independent.
In practice, this corresponds to the requirements on the input spectrum as stated in \citet{Schoukens2009LPM}: the input spectrum should be sufficiently `rough' to separate the transient contribution from the contribution $\LocalModel{G} \LocalModel{U}$.
This can be obtained e.g. using random phase multisines or random input signals.
\end{remark}

\begin{remark}
Since it is well-known that the Vandemonde structure in the design matrix \eqref{eq:nparam:designMatrix} leads to numerical ill-conditioning for higher model complexities, additional measures are taken to improve numerical conditioning.
To improve numerical conditioning~\citep{Pintelon2005} of the estimation problem, we substitute $d = \dw{r}{k}$ in equations \eqref{eq:LRM:model:G} and \eqref{eq:LRM:model:T}
\begin{equation}
\dw{r}{k} \isdef \frac{\omega_{k+r} - \omega_k}{\Delta \omega_k}
\label{eq:freqScaling}
\end{equation}
where
\begin{equation}
  \Delta \omega_k \isdef
  \max
  \Set{
    \abs{ \omega_k - \omega_j} |  \vphantom{\frac{a}{b}}  \omega_j \in \LocalWindow[k]
  }
\end{equation}
such that $\abs{\dw{r}{k}} \leq 1$ when $r \in \LocalShifts{k}$.
\end{remark}

We denote a method that solves \eqref{eq:nparam:LRM:costFunc} as $\lrm{\numel{W}, \order{B},\order{A},\order{C}}$ where $\numel{W}$ is the half-bandwidth, and $\order{B}$, $\order{A}$ and $\order{C}$ are the local model orders.

\subsection{The Local Polynomial Method}
\label{sec:nparam:LPM}
The \gls{LPM} is method that predates the \gls{LRM} and has been devised by~\citet{Schoukens2006LPM}.
The \gls{LPM} approximates both the \gls{FRF} and transient contribution by means of complex-valued local polynomials in the frequency domain.

The \gls{LPM} can be understood as a specific case of the \gls{LRM}: for  $\order{A} = 0$, the \gls{LRM} reduces to the \gls{LPM}.
As such, we use the notation $\lpm{\numel{W}, \order{B}, \order{C}}$ as a synonym for $\lrm{\nWind, \order{B}, 0, \order{C}}$.
In the remainder of this text, we refer to \gls{LRM} as a group of methods which have $\order{A} \neq 0$ to make the distinction with \gls{LPM} more obvious.

\begin{remark}
The trivial setting for local `modeling' where a window with a total width of $1\unit{bin}$ and no transient estimation ($\order{C} = -1$), actually corresponds to the simple \gls{ETFE}~\citep{Broersen1995,Stenman2000ASETFE,Stenman2001ASFRF} when no further smoothing or windowing is applied:
\begin{equation}
  \estimated{G}_{\ETFE}(\omega_k) \isdef \frac{Y(\omega_k)}{U(\omega_k)}
  \label{eq:nparam:ETFE}
\end{equation}
As such, $\ETFE \equiv \lrm{0,0,0,-1} \equiv \lpm{0,0,-1}$.
\end{remark}

\begin{remark}
The methods proposed by \citet{Stenman2001ASFRF,Stenman2000ASETFE} also refer to `local polynomial regression' to smoothen the \gls{FRF}.
It should be noted, however, that such methods differ significantly from the \gls{LPM}.
Specifically, those methods operate directly on the raw \gls{ETFE}, in contrast to the \gls{LPM} which uses the input-output spectra and can hence estimate both an \gls{FRF} and a transient contribution.
Moreover, the methods proposed by \citet{Stenman2001ASFRF} build upon `local regression` approaches such as \gls{LOWESS}.
See e.g. \citet{Loader1999} for more information regarding these `local regression' approaches.
\end{remark}

\section{Derivation of the Bias of the \glsentrytext{LRM}}
\label{sec:biascalc}
Since the local design matrix $\LocalMatrix{K}$, see \eqref{eq:nparam:designMatrix}, of the \gls{LRM} contains the noisy output spectrum $\LocalVector{Y}$ when $\order{A} > 0$, the \gls{LRM} is expected to be a biased estimator.
In this section, we will derive expressions for the bias such that it can be quantified how important this bias is in different situations.

To determine the bias, an approach similar to \citep[Appendix A]{Guillaume1995} is followed.
Let us denote the expected value of the cost function over all measurements $\LocalVector{Z} = \mat{\LocalMatrix{Y}, \LocalMatrix{U}}$ as
\begin{align}
  \ELSCost{\theta}
              & \isdef 
                   \E[\LocalVector{Z}]%
                         {    
                                \LSCost{\LocalVector{\theta}, \LocalVector{Z}}} \\
              & = \ELSCost[0]{\theta} + \ELSCost[n]{\theta} \\
    \ELSCost[0]{\theta} & \isdef 
            \frac{1}{\nWind} 
                \sum_{\dW} 
                     \left| 
                             \LocalModel{A}(\dW, \LocalVector{\theta}) \true{\LocalVector{Y}}
                          - \LocalModel{B}(\dW, \LocalVector{\theta}) \true{\LocalVector{U}}
                          - \LocalModel{C}(\dW, \LocalVector{\theta}) 
                    \right|^2 \\
    \ELSCost[n]{\theta} & \isdef 
              \frac{1}{\nWind} 
                     \sum_{\dW} 
                              \left| \LocalModel{A}(\dW,\LocalVector{\theta}) \right|^2 
                              \sigma_V^2
\end{align}
where $\true{\LocalVector{U}}$ and $\true{\LocalVector{Y}}$ denote the `true' (noiseless) input-output spectra in the local window.
Note that this function is a quadratic function of $\LocalVector{\theta}$ and hence it can be represented exactly by its second order Taylor series around $\true\theta$:
\begin{multline}
  \ELSCost{\theta} = \ELSCost{\true\theta} 
  + \PartialDerivative{\ELSCost{\true\theta}}{\true\theta}  \left( \theta - \true\theta \right) %\\
  + \frac{1}{2} \left( \theta - \true\theta \right)^{\T} 
  \PartialDerivative[2]{\ELSCost{\true\theta}}{\true\theta} 
   \left( \theta - \true\theta \right)
   \text{.}
\end{multline}
We use the notation
\begin{equation}
  \PartialDerivative{\ELSCost{\true\theta}}{\true\theta}
  \isdef
  \evaluate{\PartialDerivative{\ELSCost(\theta)}{\theta}}{\theta = \true\theta}
\end{equation}
for the derivative of $\ELSCost{\theta}$ with respect to $\theta$, evaluated in $\true\theta$.
The minimizer $\estimated\theta$ of this function can be obtained by equating the derivative
\begin{equation}
  \PartialDerivative{\ELSCost{\estimated\theta}}
                                          {\estimated\theta} 
  = 
  \left( \PartialDerivative{\ELSCost{\true\theta}}{\true\theta} \right)^{\T} 
  + \PartialDerivative[2]{\ELSCost{\true\theta}}{\true\theta} \left( \estimated\theta - \true\theta \right)
\end{equation}
to zero.
This is equivalent to 
\begin{equation}
  \left( \estimated\theta - \true\theta \right) 
  = 
  - \left(   \PartialDerivative[2]{\ELSCost{\true\theta}}{\true\theta} \right)^{-1}  
     \left( \PartialDerivative{\ELSCost{\true\theta}}{\true\theta} \right)^{\T}
  \text{.}
\end{equation}
which expresses the bias $\Delta\theta = \left( \estimated\theta - \true\theta \right)$ of the estimated parameters.
Remark that 
\begin{equation}
  \PartialDerivative{\ELSCost{\true\theta}}{\true\theta}  
  = 
  \PartialDerivative{\ELSCost[0]{\theta} + \ELSCost[n]{\theta} }{\true\theta} 
  = 
  \PartialDerivative{\ELSCost[n]{\theta} }{\true\theta} 
\end{equation}
since $\ELSCost[0]{\theta}$ is minimized in $\true\theta$ by definition.
As such, the bias on the parameters can be simplified to
\begin{equation}
  \Delta\theta = 
  - \left(   \PartialDerivative[2]{\ELSCost{\true\theta}}{\true\theta^2} \right)^{-1} 
     \left( \PartialDerivative{\ELSCost[n]{\true\theta}}{\true\theta} \right)^{\T}
  \text{.}
  \label{eq:nparam:bias-basic}
\end{equation}

Note that the estimated parameters $\theta \in \ComplexMatrix{\nth \times 1}$ but that $\ELSCost{\theta}$ is a real-valued function.
In contrast to a complex-valued function, this requires additional attention when the derivatives are calculated~\citep{Messerschmitt2006} and~\citep[Section 15.9]{Pintelon2012} since $\ELSCost{\theta}$ is not an analytic function.
Particularly, the function $\ELSCost{\theta}$ is regarded as a two-dimensional function $\ELSCost{\theta,\conj{\theta}}$ where $\conj{\theta}$ and $\theta$ are considered as independent variables~\citep{Hjorungnes2007,Hjorungnes2011}.
The derivatives are then computed with respect to these variables.

To facilitate the derivations, we rewrite the expected value of the cost function in terms of $\theta$ and $\conj{\theta}$:
\begin{align}
   \ELSCost[0]{\theta,\conj{\theta}} & =
      \frac{1}{\nWind}
           \sum_{\dW}
           \LocalModel{Q}(      \dW,        \LocalVector{\theta},  \true{      \LocalVector{Y}},  \true{      \LocalVector{U} })
           \LocalModel{Q}(\conj{\dW}, \conj{\LocalVector{\theta}}, \true{\conj{\LocalVector{Y}}}, \true{\conj{\LocalVector{U}}})
   \\
   \LocalModel{Q}(\dW, \LocalVector{\theta},  \true{\LocalVector{Y}},  \true{ \LocalVector{U} }) & \isdef
                 \LocalModel{A}(\dW, \LocalVector{\theta}) \true{\LocalVector{Y}}(\dW)
               - \LocalModel{B}(\dW, \LocalVector{\theta}) \true{\LocalVector{U}}(\dW)
               - \LocalModel{C}(\dW, \LocalVector{\theta})
    \\             
    \ELSCost[n]{\theta,\conj{\theta}} & =
              \frac{\sigma_V^2}{\nWind} 
                     \sum_{\delta} 
                               \LocalModel{A}(      \dW,        \LocalVector{\theta} ) 
                               \LocalModel{A}(\conj{\dW}, \conj{\LocalVector{\theta}}) 
\end{align}
since $\conj{ \LocalModel{A}(\dW,\theta)} =  \LocalModel{A}(\conj{\dW},\conj{\theta}) $, and similar for $\LocalModel{B}$ and $\LocalModel{C}$ and $\LocalModel{Q}$.

\paragraph{Contributions to the first order derivative}
The last term in \eqref{eq:nparam:bias-basic}, i.e. the first order derivative,  is given by
\begin{equation}
  \PartialDerivative{\ELSCost[n]{\true\theta, \conj{\true\theta}}}{\theta}
  = 
  \begin{bmatrix}
  \PartialDerivative{\ELSCost[n]{\true\theta, \conj{\true\theta}}}{\theta_A} &
  \PartialDerivative{\ELSCost[n]{\true\theta, \conj{\true\theta}}}{\theta_B} &
  \PartialDerivative{\ELSCost[n]{\true\theta, \conj{\true\theta}}}{\theta_C}
  \end{bmatrix}
\end{equation}
and split into the parts pertaining to respectively $\LocalModel{A}$, $\LocalModel{B}$ and, $\LocalModel{C}$.
The respective components are given by:
\begin{align}
  \PartialDerivative{\ELSCost[n]{\theta,\conj{\theta}}}{a_i} 
     &= 
     \frac{\sigma_V^2}{\nWind}
     \sum_{\dW} \dW^i \LocalModel{A}(\conj{\dW},\conj{\LocalVector{\theta}}) 
     && \forall i \in \set{1,\ldots,\order{A}}
     \\
   \PartialDerivative{\ELSCost[n]{\theta,\conj{\theta}}}{b_i} 
   &= 0
   && \forall i \in \set{0,\ldots,\order{B}}
   \\
   \PartialDerivative{\ELSCost[n]{\theta,\conj{\theta}}}{c_i} 
   &= 0
   &&\forall i \in \set{0,\ldots,\order{C}}
   \text{.}
\end{align}
It can be seen that only the factors pertaining to $\theta_{A}$ are present.
As this term occurs linearly in expression \eqref{eq:nparam:bias-basic}, only $\theta_A$ can incur a bias.
Consequently, only the poles can shifted by the presence of noise.
As such, the bias term $\Delta \theta$ can be written in the following sparse form:
\begin{equation}
  \Delta \theta = 
  \begin{bmatrix}
    \Delta \theta_A\\
    \Delta \theta_B\\
    \Delta \theta_C
  \end{bmatrix}
  =
  \begin{bmatrix}
    \Delta \theta_A\\
    \deemph{0}\\
    \deemph{0}
  \end{bmatrix}
  \text{.}
  \label{eq:nparam:lrm:bias:sparse}
\end{equation}

\paragraph{Contributions to the Hessian}
Computing the Hessian in \eqref{eq:nparam:bias-basic} also boils down to recognizing the block structure.
This results in the following block matrix:
\begin{equation}
  \PartialDerivative[2]{\ELSCost{\true\theta}}{\true\theta^2} =
  \begin{bmatrix}
    \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{\theta}_A \pd \theta_A } &
    \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{\theta}_B \pd \theta_A } & 
    \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{\theta}_C \pd \theta_A } \\
    \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{\theta}_A \pd \theta_B } &
    \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{\theta}_B \pd \theta_B } & 
    \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{\theta}_C \pd \theta_B } \\
    \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{\theta}_A \pd \theta_C } &
    \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{\theta}_B \pd \theta_C } & 
    \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{\theta}_C \pd \theta_C }
  \end{bmatrix}
  \label{eq:nparam:lrm:bias:hessian}
\end{equation}
where every sub-block consists of the different second-order derivatives and exhibits a similar pattern.

The different contributions are given below:
\begin{align}
  \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{a_{i_1}} \pd a_{i_2}} & = 
  \frac{1}{\nWind} \sum_{\dW} \dW^{i_1+i_2} \abs{\true{\LocalVector{Y}}(\dW)}^2\\
  \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{b_{i_1}} \pd b_{i_2}} & = 
  \frac{1}{\nWind} \sum_{\dW} \dW^{i_1+i_2} \abs{\true{\LocalVector{U}}(\dW)}^2\\
  \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{c_{i_1}} \pd c_{i_2}} & = 
  \frac{1}{\nWind} \sum_{\dW} \dW^{i_1+i_2}\\
  \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{b_{i_1}} \pd a_{i_2}}  =
  \conj{ \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{a_{i_2}} \pd b_{i_1}} }& = 
  \frac{1}{\nWind} \sum_{\dW} \dW^{i_1+i_2} \conj{\true{\LocalVector{U}}(\dW)} \true{\LocalVector{Y}}(\dW)\\
  \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{c_{i_1}} \pd a_{i_2}}  =
  \conj{ \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{a_{i_2}} \pd c_{i_1}} }& = 
  \frac{1}{\nWind} \sum_{\dW} \dW^{i_1+i_2} \true{\LocalVector{Y}}(\dW)\\
  \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{c_{i_1}} \pd b_{i_2}}  =
  \conj{ \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{c_{i_2}} \pd b_{i_1}} }& = 
  \frac{1}{\nWind} \sum_{\dW} \dW^{i_1+i_2} \true{\LocalVector{U}}(\dW)
  \text{.}
\end{align}

\begin{remark}
Computing the inverse of this Hessian cannot, to the knowledge of the author, be reduced to a form that yields more insight.
However, if the non-diagonal blocks can be neglected, this would lead to an inverse that is also block-diagonal:
\begin{equation}
   \begin{bmatrix}
    \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{\theta}_A \pd \theta_A } &
    \deemph{0} & 
    \deemph{0} \\
    \deemph{0} &
    \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{\theta}_B \pd \theta_B } & 
    \deemph{0} \\
    \deemph{0} &
    \deemph{0} & 
    \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{\theta}_C \pd \theta_C }
  \end{bmatrix}^{-1}
  \!\!\!=
  \begin{bmatrix}
    \left( \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{\theta}_A \pd \theta_A }\right)^{-1} &
    \deemph{0} & 
    \deemph{0} \\
    \deemph{0} &
    \left( \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{\theta}_B \pd \theta_B }\right)^{-1} & 
    \deemph{0} \\
    \deemph{0} &
    \deemph{0} & 
    \left( \PartialDerivative[2]{\ELSCost{\theta}}{ \conj{\theta}_C \pd \theta_C }\right)^{-1}
  \end{bmatrix}
  % \text{.}
\end{equation}
\end{remark}
\paragraph{Bias on the \glsentrydesc{FRF}}
As a first order approximation, the bias on the \gls{FRF} can be written as
\begin{equation}
  \Delta \LocalModel{G}(\omega_k) 
  =
  \Delta \LocalModel{G}(\dw{0}{k}) 
     \approx 
        \PartialDerivative{\LocalModel{G}(\dw{0}{k},\theta)}
                                          {\theta} 
      \Delta \theta
      \text{.}
\end{equation}
The derivative in that expression can be evaluated easily
\begin{align}
    \PartialDerivative{\LocalModel{G}(\dW, \theta)}{\theta} 
     &=
     \begin{bmatrix}
       \PartialDerivative{\LocalModel{G}(\dW,\theta)}{\theta_A} &
       \PartialDerivative{\LocalModel{G}(\dW,\theta)}{\theta_B} &
       \PartialDerivative{\LocalModel{G}(\dW,\theta)}{\theta_C} 
     \end{bmatrix}
     \\
     \PartialDerivative{\LocalModel{G}(\dW, \theta)}{\theta_A} 
     &=
     - \frac{\LocalModel{B}(\dW, \theta)}{\LocalModel{A}^2(\dW, \theta)}
     \begin{bmatrix}
         \dW^1 & \dW^2 & \deemph{\cdots} & \dW^{\order{A}}
     \end{bmatrix}
     \\
     \PartialDerivative{\LocalModel{G}(\dW, \theta)}{\theta_B} 
     &=
     \frac{1}{\LocalModel{A}(\dW, \theta)}
     \begin{bmatrix}
         1 & \dW^1 & \dW^2 & \deemph{\cdots} & \dW^{\order{B}}
     \end{bmatrix}
     \\
     \PartialDerivative{\LocalModel{G}(\dW, \theta)}{\theta_C} 
     &=
     \Zero{1 \times \order{C}+1}
     \text{.}
\end{align}

However, the sparsity of $\Delta\theta$, see \eqref{eq:nparam:lrm:bias:sparse}, can be exploited such that the bias on the \gls{FRF} can be expressed as:
\begin{equation}
  \Delta \LocalModel{G}(\dW,\theta)
  \approx
  \frac{\LocalModel{G}(\dW, \theta)}
           {\LocalModel{A}(\dW, \theta)}
  \begin{bmatrix}
    \dW^1 & \deemph{\cdots} & \dW^{\order{A}}
  \end{bmatrix}
  \Delta \theta_A
  \text{.}
\end{equation}
Equivalently, the relative bias of the \gls{FRF} can be written as
\begin{align}
\frac{\Delta \LocalModel{G}(\dw{\omega}{}, \theta)}{\LocalModel{G}(\dw{\omega}{}, \theta)}
&\approx
\LocalModel{A}(\dw{\omega}{}, \theta)^{-1}
\begin{bmatrix}
    \dw{\omega}{}^1 & \deemph{\cdots} & \dw{\omega}{}^{\order{A}}
  \end{bmatrix}
  \left(  \PartialDerivative[2]{\ELSCost{\theta}}{ \theta^2 } \right)^{-1}_{A}
  \PartialDerivative{\ELSCost[n]{\true\theta}}{\theta_A}^{\TT}
  \\
  &=
  \frac{\sigma_V^2}{\nWind}
\begin{bmatrix}
    \dw{\omega}{}^1 & \deemph{\cdots} & \dw{\omega}{}^{\order{A}}
  \end{bmatrix}
  \left(  \PartialDerivative[2]{\ELSCost{\theta}}{ \theta^2 } \right)^{-1}_{A}
  \begin{bmatrix}
     \Sum_{\dW} \dW^1 
     \frac{\LocalModel{A}(\conj{\dW},\conj{\LocalVector{\theta}})}
               {\LocalModel{A}(\dw{\omega}{}, \theta)}\\
  \deemph{\vdots}\\
     \Sum_{\dW} \dW^{\order{A}} 
     \frac{\LocalModel{A}(\conj{\dW},\conj{\LocalVector{\theta}})}
               {\LocalModel{A}(\dw{\omega}{}, \theta)}\\
  \end{bmatrix}
\end{align}
where $\left(  \PartialDerivative[2]{\ELSCost{\theta}}{ \theta^2 } \right)^{-1}_{A}$ denotes the upper left $\order{A}\times\order{A}$ subblock of $\left(  \PartialDerivative[2]{\ELSCost{\theta}}{ \theta^2 } \right)^{-1}$ which pertains to the parameters of $\LocalModel{A}$.
From the leading factor, it can be seen that the bias on the \gls{FRF} is $\bigO{\sigma_V^2}$.
Consequently, one can expect that the bias is inherently linked to the \gls{SNR} of the output signal in the frequency domain.

\section{Simulations}
\label{sec:simulations}

In the simulations, we consider a discrete-time second order system with transfer function
\begin{equation}
\true{G}(z) = \frac{0.64587 z + 0.64143}
                                      {47.9426 z^2 - 51.2955 z + 46.9933}
                                      \text{,}
\end{equation}
this is a resonant system with a resonance near $\omega=1\unit{rad/sample}$, $\damping=0.05$.
The system is scaled such that its peak amplitude is $3\unit{dB}$ and hence its $3\unit{dB}$ bandwidth can be read easily from bode plots.
As indicated in \figref{fig:nparam:blockH0}, the noise filter $\true{H}' = 1$ is used such that white noise is disturbing the output.
The gain $\kappa$ of this filter is adapted such that the \gls{SNR} in the $3\unit{dB}$ bandwidth of the system, i.e.
\begin{equation}
  \SNR_{\BW} \isdef
  \frac{\int\limits_{\BW} \powerspec{\true{Y}}(\omega) \dd\omega}
            {\int\limits_{\BW} \powerspec{V}(\omega) \dd{\omega}}
  % \frac{\int_{\BW} \abs{\true{Y}(\omega)}^2 \dd{\omega}}
  %           {\int_{\BW} \abs{V(\omega)}^2 \dd{\omega}}
\end{equation}
where $\powerspec{\bullet}$ denotes the \gls{PSD} of $\bullet$ and $\BW$ is the $3 \unit{dB}$ bandwidth of the system, is fixed to \[
\SNR_{\BW} \in \set{10, 20, 40, 60, \infty} \unit{dB}\text{.}
\]
This ensures that the excited bins in the $3\unit{dB}$ bandwidth of the system each receive a pre-defined \gls{SNR}.

\begin{figure}
 \centering
  \input{\thisDir/figs/generalized-output-error.tikz}
  \caption{Block schematic used in the simulations. }
  \label{fig:nparam:blockH0}
\end{figure}

The input signal used is a random-phase multisine with a full linear grid.
A number of samples of the input- and output are measured, in such a way that the \gls{DFT} grid excited $\nBW$ bins within the $3\unit{dB}$ bandwidth of the system.
The different values for 
\[\nBW \in \set{2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 16, 32, 40, 64, 80, 92, 128, 256}\] are tried. 
The use of $\nBW$ makes that the experiment length is normalized to take the bandwidth of the resonance into account.
This means that the obtained results generalize to systems with a different relative damping.

The \gls{FRF} is estimated using the local modeling approaches indicated in \tabref{tbl:nparam:methods}.
This table also indicate $\Delta\SNR$ which indicates the expected gain in \gls{SNR} if the error is mainly determined by the variance.
Concretely,
\begin{equation}
  \Delta\SNR \isdef \sqrt{\frac{\nth}{\nWind}}
  \label{eq:nparam:deltaSNR}
\end{equation}
accounts for the smoothing action of local modeling.

% \begin{figure}
%   \centering
%   \setlength{\figurewidth}{0.85\columnwidth}
%   \setlength{\figureheight}{0.62\figurewidth}
%   \TODOtikzfig[figure with G and errors for 1 simulation]
%   \caption[Estimated FRF using local modeling]{The figure shows the \gls{FRF} of a single simulation for $\SNR= 40 \unit{dB}$ and $\nBW =  64$, together the the error ($\model{\bullet} - \true{G}$) for the different methods. The $3\unit{dB}$ bandwidth of the system is highlighted.}
% \end{figure}

We then compute basic statistics of the estimated \glspl{FRF} $\model[\atSimulation{i}]{\bullet}$ over $\nMC=\num{2000}$ Monte Carlo runs of the simulations above.
Specifically, the following values
\begin{align}
  \sampleMean{\bullet}(\omega_k) & 
  \isdef
    \frac{1}{\nMC}
    \sum_{i=1}^{\nMC}
    \model[\atSimulation{i}]{\bullet}(\omega_k)
  \\
  \sampleBias{\bullet}(\omega_k) &
    \isdef
    \frac{1}{\nMC}
    \sum_{i=1}^{\nMC}
    \model[\atSimulation{i}]{\bullet}(\omega_k) - \true{G}(\omega_k)
    = 
    \sampleMean{\bullet}(\omega_k) - \true{G}(\omega_k)
    \\
    \sampleVariance{\bullet}(\omega_k) &
    \isdef
    \frac{1}{\nMC - 1}
    \sum_{i=1}^{\nMC}
                   \left({\model[\atSimulation{i}]{\bullet}}(\omega_k) - \sampleMean{\bullet}(\omega_k) \right)
    \conj { \left({\model[\atSimulation{i}]{\bullet}}(\omega_k) - \sampleMean{\bullet}(\omega_k) \right) }
    \\
    \RMSE_{\bullet}(\omega_k) & \isdef \sqrt{\sampleVariance{\bullet}(\omega_k) + \sampleBias{\bullet}^{2}(\omega_k)}
\end{align}
are computed and are the sample estimates of respectively the expected value $\E{\model{\bullet}}$, bias $\bias{\model{\bullet}}$ and variance $\Var{\model{\bullet}}$ of the model.
Remark, that the standard deviation on the $\sampleBias{\bullet} \approx \sqrt{\sampleVariance{\bullet} / \nMC}$ which limits the precision with which we can hence observe $\sampleBias{\bullet}$. 
For $\nMC=\num{2000}$, this means that we can detect the bias only when it is at most $20 \log \sqrt{\num{2000}} \unit{dB} \approx 33 \unit{dB}$ smaller than $\sampleStd{\bullet}$ and smaller biasses cannot be detected by the simulations.


  % \subsection{Implications of the model structure}
  % \subsection{Implications of the noise level}
  % \subsection{Implications of the noise coloring}

\begin{figure}[p]
  \centering 
  \setlength{\figurewidth}{0.85\columnwidth}
  \setlength{\figureheight}{0.62\figurewidth}
  \input{\thisDir/figs/MC-SNR40-NBW64and8.tikz}
  \caption[Bias, variance for $\nBW \in \set{8,64}$ at $\SNR=40\unit{dB}$]{The bias, variance and \gls{RMS} error of the \gls{FRF} using the different estimators is shown for $\SNR= 40 \unit{dB}$ and $\nBW \in \set{8,64}$ samples in the $3 \unit{dB}$ bandwidth of the system (as highlighted). See \tabref{tbl:nparam:methods} for the color legend.}
  \label{fig:nparam:mc:singleTuple}
\end{figure}

\begin{table}[p]
\centering
\caption{Local modeling methods used in the Monte Carlo analysis.}
\label{tbl:nparam:methods}
\begin{tabular}{lccccr} 
\toprule
\textsc{Method} & \textsc{Color} & $\nWind$ & $\nth$ & \DOF & $\Delta\SNR \axisunit{dB}$ \\
\midrule
 $\ETFE$ & \ref{leg:nparam:etfe} & 1  & 1  & 0 & 0 \\
 $\lrm{7,1,1,1}$ &  \ref{leg:nparam:lrm7111} & 15  & 5 & 10  & 4.77 \\
 $\lric{7,2,2,2}$ & \ref{leg:nparam:lric7222} &   15 & 8 & 7  & 2.73 \\
 $\lrm{7,2,2,2}$ & \ref{leg:nparam:lrm7222}  &   15 & 8 & 7  & 2.73 \\
 $\lrm{6,2,2,2}$ & \ref{leg:nparam:lrm6222}  &   13 & 8 & 5  & 2.11 \\
 $\lpm{7,4,4}$ & \ref{leg:nparam:lpm744} &   15 & 10 & 5  & 1.76 \\
\bottomrule
\end{tabular}
\end{table}

As the Monte Carlo simulations yield the estimated \gls{FRF}, the bias and variance for the different methods as a function of frequency, a huge amount of data is generated.
In \figref{fig:nparam:mc:singleTuple}, two such sets of data are shown as an example.
It can be seen that the \gls{RMSE} of \gls{LRIC} behaves erratically and exhibits spikes over the whole frequency band.
For a short data length ($\nBW=8$), the \gls{LPM} and \gls{ETFE} shows a significant bias, whereas the \gls{LRM}-based methods have a very small bias and variance that lies below the noise floor of each experiment.
However, the variance of the \gls{LRM} is still elevated by a few decibels near the resonance frequency.
For long data lengths ($\nBW=64$), both \gls{LPM} and \gls{LRM} perform comparably.

To interpret the data from the whole Monte Carlo simulation, we combine the observed values over the frequencies to obtain the \term{aggregated error}, respectively:
\begin{align}
  \sampleVariance{\bullet} &= \frac{1}{\nBW} \sum_{\omega_k \in \BW} \abs{\sampleVariance{\bullet}(\omega_k)} \\
  \sampleBias{\bullet} &= \frac{1}{\nBW} \sum_{\omega_k \in \BW} \abs{\sampleBias{\bullet}(\omega_k)}\\
  \RMSE_{\bullet} &= \sqrt{\sampleBias{\bullet}^2 + \sampleVariance{\bullet}}
\end{align}
which can be visualized for the different values of the $\SNR$, $\nBW$ and for each method.
Note that this aggregation is limited to the $3\unit{dB}$ bandwidth of the system, which is where the worst-case behavior is typically encountered.

\paragraph{Noiseless case}
In the noiseless case, one can observe the approximation error that is made by using the \gls{LRM}, \gls{LPM}, \gls{LRIC} and \gls{ETFE} as shown in \figref{fig:nparam:comparison:noiseless}.
For the \gls{ETFE}, it can be seen that the error is very high.
This can be expected since the \gls{ETFE} does not separate transient/leakage contributions.
Consequently, the leakage contributions (which change over the different Monte Carlo runs), lead to a large variance error for the \gls{ETFE}.
The approximation error of the \gls{LPM} for short datasets ($\nBW \leq 8$) is dominated by the bias error.
The is to be expected since the resonance in the band of interest is modeled using only polynomials instead of the full rational form.
For longer datasets, the error decays approximately as $\bigO{\nBW^{-6}}$.
This is in line with the theoretical derivation from \citet{Schoukens2013LPMerror} that state that the approximation error decays as $\bigO{ \nBW^{-(\order{B}+2)}}$.
For the \gls{LRM} and \gls{LRIC}, the \gls{RMS} error is already very low for small datasets ($-64 \unit{dB}$ for the first-order method, $-114\unit{dB}$ for second-order methods).
For short datasets ($\nBW \leq 16$), the $\lrm{7,1,1,1}$ error has a bias contribution that is slightly larger ($1$ to $4\unit{dB}$) than the variance contribution.
For larger datasets ($\nBW \geq 16$), this bias contribution starts to diminish significantly as $\bigO{\nBW^{-2}}$.
For second-order rational methods, the error is dominated by the variance and is about $50\unit{dB}$ smaller than for the first-order methods.
Practically, this approximation error is very likely to be negligible in measurements as such an error level corresponds to the quantization noise level of a $19\unit{bit}$ \gls{ADC}.

\begin{figure}[htb]
  \centering
  \setlength{\figurewidth}{0.85\columnwidth}
  \setlength{\figureheight}{0.62\figurewidth}
  \input{\thisDir/figs/MC-SNR-Inf.tikz}
  \caption[Comparison of local models for $\SNR = \infty$]{Local modeling of a discrete-time system results in an approximation error that is small enough for many practical uses. }
  \label{fig:nparam:comparison:noiseless}
\end{figure}

\paragraph{Good signal-to-noise ratios}
The next situation we consider is when $\SNR=60\unit{dB}$ as shown in \figref{fig:nparam:comparison:hiSNR}.
Here, the \gls{LPM} remains severly biased for small datasets ($\nBW < 12$).
However, for sufficiently long experiments ($\nBW > 32$), the bias becomes negligible.
For the \gls{LRM} and \gls{LRIC}, the bias is always much smaller than the variance.
In contrast to \gls{LPM}, the \gls{LRM} already reaches a constant variance level when $\nBW > 10$.
It should also be noted that for the \gls{LRIC}, the variance is almost $10\unit{dB}$ more than the corresponding \gls{LPM} method.
It is hence not advantageous to use the \gls{LRIC} as-is.
For all methods (except the \gls{ETFE} and \gls{LRIC}), the simulations show that the \gls{RMSE} reaches the noise floor minus $\Delta \SNR$.

For the more noisy situation where $\SNR = 40 \unit{dB}$ (\figref{fig:nparam:comparison:midSNR}), the situation is qualitatively equal except the bias incurred by the \gls{LRM} is similar to the noise level.

\begin{figure}[p]
  \centering
  \setlength{\figurewidth}{0.85\columnwidth}
  \setlength{\figureheight}{0.62\figurewidth}
  \input{\thisDir/figs/MC-SNR-60.tikz}
  \caption[Comparison of local models for $\SNR = 60 \unit{dB}$]{Comparison of bias and variance of local modeling for $\SNR=60\unit{dB}$. The shaded area denotes the noise floor.}
  \label{fig:nparam:comparison:hiSNR}
\end{figure}

\begin{figure}[p]
  \centering
  \setlength{\figurewidth}{0.85\columnwidth}
  \setlength{\figureheight}{0.62\figurewidth}
  \input{\thisDir/figs/MC-SNR-40.tikz}
  \caption[Comparison of local models for $\SNR = 40 \unit{dB}$]{Comparison of bias and variance of local modeling for $\SNR=40\unit{dB}$. The shaded area denotes the noise floor.}
  \label{fig:nparam:comparison:midSNR}
\end{figure}

\paragraph{Poor signal-to-noise ratios}
In \figref{fig:nparam:comparison:lowSNR} and \figref{fig:nparam:comparison:terribleSNR}, the results are shown for $\SNR=20\unit{dB}$ and $\SNR=10\unit{dB}$ respectively.
Again, it can be seen that \gls{LRIC} is plagued by high variance and is hence not directly usable.
However, its bias for small datasets is much lower than for the other methods.
At $\SNR=20\unit{dB}$ the \gls{LPM} and the \gls{LRM} is significantly biased for small datasets ($\nBW < 6$), however, the \gls{LRM} still perform better than the \gls{LPM}.
For larger datasets ($\nBW \geq 16$), all methods perform similarly and at a nearly constant level of (variance) error.
The \gls{LRM}, in contrast to the \gls{LPM}, still contains an observable bias in the simulations.
At even lower $\SNR=10\unit{dB}$,  most methods perform quite poorly for short datasets ($\nBW < 6$).
However, here it can be seen that the $\lpm{7,4,4}$ outperforms both the $\lrm{7,1,1,1}$ and $\lrm{7,2,2,2}$ and slightly worse than $\lrm{6,2,2,2}$.
It can be seen that the bias error of the \gls{LPM} drops off more quickly ($\bigO{\nBW^{-3}}$) than the \gls{LRM} bias which drops in approximately $\bigO{\nBW^{-1}}$.

\begin{figure}[p]
  \centering
  \setlength{\figurewidth}{0.85\columnwidth}
  \setlength{\figureheight}{0.62\figurewidth}
  \input{\thisDir/figs/MC-SNR-20.tikz}
  \caption[Comparison of local models for $\SNR = 20 \unit{dB}$]{Comparison of bias and variance of local modeling for $\SNR=20\unit{dB}$.
  The shaded area denotes the noise floor.}
  \label{fig:nparam:comparison:lowSNR}
\end{figure}

\begin{figure}[p]
  \centering
  \setlength{\figurewidth}{0.85\columnwidth}
  \setlength{\figureheight}{0.62\figurewidth}
  \input{\thisDir/figs/MC-SNR-10.tikz}
 \caption[Comparison of local models for $\SNR = 10 \unit{dB}$]{Comparison of bias and variance of local modeling for $\SNR=10\unit{dB}$. The shaded area denotes the noise floor.}
  \label{fig:nparam:comparison:terribleSNR}
\end{figure}

\subsection{Summary}
\TODO{discussie}

\begin{guideline}[Use the LRM for resonant systems when $\SNR > 10 \unit{dB}$]
For resonant systems, the \gls{LRM} offers better performance than \gls{LPM} when a good \gls{SNR} is obtained in the frequency domain.
When the \gls{SNR} is poor ($\leq 10\unit{dB}$), the \gls{LPM} may perform a bit better than \gls{LRM}.
The \gls{LRIC} is unsuitable due to a high variance, but has a much smaller bias error than the non-iterative solutions.
\end{guideline}

\begin{guideline}[For long measurements, LPM and LRM are similar]
For very long measurements, the difference between \gls{LPM} and \gls{LRM} becomes negligible if the different resonances are well-separated.
\end{guideline}

  % \section{Measurements}
  % \label{sec:measurements}
  %  \subsection{AVIS}
   % \subsection{F16}

\section{Trading off the bias and variance}

\TODO{introduction, probleem, oplossingen die besproken worden}

\subsection{Order selection}
\label{sec:nparam:orderSelection}
Selecting suitable values of $\nWind$, $\order{B}$, $\order{A}$ and $\order{C}$ obviously affects the quality of the obtained local models.
For local rational models, there are a few insights from parametric system identification that can be retained to make these choices easier.
In particular~\citep{Pintelon2010LPM1}:
\begin{equation}
\order{C} \leq \max\set{\order{B},\order{A}}
\end{equation}
 and from statistical point of view~\citep{Mahata2006}, the degrees of freedom should remain positive such that 
 \begin{equation}
 \nWind \geq \order{B} + \order{A} + \order{C} + 2
 \text{.}
 \end{equation}
 
Different heuristics exist to select the local bandwidth $\nWind$, e.g.~\citet{Fan1995,Thummala2012LPMBW,Stenman2000ASETFE} in the framework of local modeling approaches.
As such, we focus mainly on the selection of the model order hyper-parameters ($\order{A}$, $\order{B}$ and $\order{C}$) for a fixed value of $\nWind$.

Here, we take an approach that decides amongst a group of candidate models $\Set{M_1, \ldots, M_N}$, which model $M_i$ is the best model according to some criterion: i.e. one that captures that system dynamics but remains parsimonious.
To do so, one needs to define a criterion that both takes `goodness-of-fit' (e.g. value of the cost function, $R^2$, \ldots) and model complexity into account to ensure that the model captures only the systematic behavior and not random variations of the particular measurement.
The latter is often done using `cross-validation'.

In statistical learning, many relevant cross-validation approaches exist.
Within the system identification community a few simple approaches have been widely adopted~\citep[Chapter 11]{Pintelon2012} to ensure that one does not overmodel the data.
In the following, a short overview is given of different approaches.

\paragraph{Plug-in methods}
In plug-in methods, a goodness-of-fit criterion (e.g. the log-likelihood function $\LogLikelihood$ of the estimated local model $\LocalModel{M}$ given the estimation dataset $\LocalVector{Z}$), is penalized with a term accounting for the complexity of the model~\citep{Burnham2002}.
E.g. a common choice is the \gls{AIC}~\citep{Akaike1974} or the \gls{AICc}~\citep{Hurvich1989}, which are defined as
\begin{align}
L_{\mathrm{\AIC}} &=
 - 2  \loglikelihood{\LocalModel{M} | \LocalVector{Z}} 
 + 2 \nth
\label{eq:nparam:AIC}\\
L_{\mathrm{\AICc}} &= 
- 2  \loglikelihood{\LocalModel{M} | \LocalVector{Z}} 
+   \frac{2 \nth\nWind}{\nWind - \nth - 1}
\label{eq:nparam:AICc}
\end{align}
where $\nth$ is the number of  estimated parameters in the model $\LocalModel{M}(\theta)$.
In the context of local modeling, the \gls{AIC} is a poor choice since it relies on asymptotic arguments of the data set, i.e. $\nWind \to \infty$, which is obviously an unwarranted choice in this context.
The best model among a set of $q$ possible models $\Set{M_1, \ldots, M_q}$ is then
\begin{equation}
  M_{\star} \isdef 
  \arg_{M} \min 
  \Set{
    L_{\mathrm{AIC(c)}}(M)
  }
  \text{,}
\end{equation}
which is the only model that is hence retained.

\begin{remark}
From a theoretical point of view, equations \eqref{eq:nparam:AIC} and \eqref{eq:nparam:AICc}, require full knowledge of the likelihood function.
This means specifically that the noise variance $\sigma^{2}_{V}$ needs to be known beforehand.
When the variance is unknown, however, it is more robust to incorporate the \gls{AIC} as a penalty factor on the cost function as indicated in \citet[Section 7.4]{Ljung1999} and \citet[Section  11.3 and 19.7]{Pintelon2012}.
\end{remark}

\begin{remark}
It should be noted that due to $\loglikelihood{\LocalModel{M} | \LocalVector{Z}}$ , the value of $L_{\mathrm{AIC(c)}}(\LocalModel{M},\LocalVector{Z})$ is inherently coupled with the particular dataset $\LocalVector{Z}$.
Practically, this means that such criteria can only be used to compare local models that are estimated from the same local dataset (i.e. they have the same $\nWind$ and center frequency).
Consequently, the \gls{AICc} criterion cannot be used to select $\nWind$, but only $\order{A},\order{B},\order{C}$ when $\nWind$ is fixed.
\end{remark}

\paragraph{Holdout method}
The so-called `holdout' cross-validation method ensures a good model by first splitting the dataset into two smaller disjoint datasets: e.g. $70\%$ of the data  is used to estimate the model and a goodness-of-fit criterion (e.g. cost function) is used on the remaining data to determine which model has the best out-of-sample performance.
By using independent datasets and assessing the performance of the model on (unseen) validation data, overfitted models can be rejected.

\begin{remark}
The typical approaches, such as the holdout and plug-in methods, either assume that an abundantly large dataset is available or  assume the models are estimated on a fixed dataset.
As such, these methods are not ideal for local modeling where the (local) dataset is rather small by design and could change by means of the local bandwidth $\nWind$ over the frequency.
\end{remark}

\paragraph{Leave-out cross-validation}
Instead, we use a better-suited cross-validation approach known as \gls{LOOCV} that is related to the \gls{AIC}~\citep{Stone1977}.
Plainly, \gls{LOOCV} cross-validates the model by in turn leaving out each data point, estimating the local model parameters and observing the difference between the omitted data point and the estimate.
In effect, such an approach calculates the \gls{PRESS} for each local model:
\begin{equation}
 \PRESS = 
 \frac{1}{\nWind} 
  \sum_{r \in \LocalShifts{}}
    \abs{\ignoring{r}{\LocalVector{E}}}^2
    \text{,}
\label{eq:PRESS-general}
\end{equation}
where $\ignoring{r}{\LocalVector{E}} = \LocalVector{Y}_{r} - \ignoring{r}{\LocalVector{\estimated{Y}}}$, i.e. the difference between the observed (local) output spectrum at the $r^{\text{th}}$ line and the predicted output spectrum at the same line when the corresponding data point is removed from the estimation procedure.
To select the best model from a set of candidate models, only the model with the lowest \gls{PRESS} value is retained.
In general, this means that \gls{LOOCV} requires estimating $\nWind$ additional models (one for each omitted data point) and hence can be very time-consuming.

For linear models (i.e. $\LocalVector{Y} = \LocalMatrix{K} \LocalVector{\theta}$, such as for the \gls{LRM} and \gls{LPM}), however, the \gls{PRESS} can be calculated in a single step without estimating any additional models~\citep[Sec.~12.3.2]{Seber2003}:
\begin{equation}
\PRESS = \frac{\LocalVector{E}^{\HT} \LocalVector{W}^{-1} \LocalVector{E}}{2N_W + 1}
\end{equation}
where $\LocalVector{E}$ is the residual vector of the full estimation and $\LocalVector{W}$ is a diagonal weighting matrix with $\LocalVector{W}_{ii} = (1 - \LocalVector{H}_{ii})^{\HT} (1 - \LocalVector{H}_{ii})$ and
$\LocalVector{H} = \LocalMatrix{K} \pinv{\LocalMatrix{K}}$ is the so-called `hat-matrix' that is known to be idempotent.

Note that since the \gls{PRESS} can be interpreted similar to the residuals, this forms a convenient metric to select models on.
In particular, when there is a large difference between the level of the residuals and the \gls{PRESS}, this indicates that a poor model has been selected.

\subsection{Results}

%TODO: discussie: 1 globale modelorder of verschillende
% voor parametrische schatting: vensterbreedte constant houden, DOF constant houden over alle geselecteerde modellen
% voor niet-parametrisch: orde kan per frequentielijn verschillen, geeft betere smoothing \citep{Thummala2012LPMBW,Fan1996}

% \begin{figure}
%   \centering
%   \setlength{\figurewidth}{0.85\columnwidth}
%   \setlength{\figureheight}{0.62\figurewidth}
%   \TODOtikzfig[PRESS and noise for single simulation]
%   \caption[Output spectrum for local modeling, together the PRESS statistic]{It is convenient to compare the variance of the residuals and the \glsentrytext{PRESS} statistic to select a good local model.}
%   \label{fig:nparam:PRESS}
% \end{figure}

\section{LPM with impulse response truncation}
\input{\thisDir/truncation.tex}

\section{Conclusions}
\label{sec:conclusion}
In this chapter, the performance of non-parametric \gls{LPM} and \gls{LRM} estimators were compared to observe the \gls{FRF} of resonant systems.
Since the \gls{LRM} is a biased estimator while the \gls{LPM} is asymptotically unbiased, it is important to compare their behavior.
Hence, expressions for the bias of the \gls{LRM} were derived.
%TODO: tom: "inherently biased": verregaande uitspraak in relatie met Levy: is normaal ook biased, maar voor ARX niet. Hoe zit dat hier?
For good \glspl{SNR} in resonant systems, it was seen that the \gls{LRM} is always superior to the \gls{LPM}.
For extremely poor \gls{SNR}, i.e. $\SNR \approx 10\unit{dB}$, the \gls{LPM} may perform slightly better than the \gls{LRM} when short measurement records are available.
In this chapter, the \gls{LRIC} was introduced.
This is an extension of the \gls{LRM} which solves the non-linear optimization problem and should thus be able to avoid a bias altogether.
However, it is seen that this estimator is extremely noisy but seems to provide a smaller bias than the \gls{LRM}.
Since the overall \gls{RMS} error for the \gls{LRIC} is much larger than most other methods, the \gls{LRIC} cannot be used in its current form.

Relatedly, two approaches to aid in performing the bias-variance trade-off for these local methods are illustrated.
First, a short study of cross-validation for local models is given.
Secondly, an approach to truncate the impulse response obtained from the \gls{LPM} is illustrated.
The latter can improve the performance of the \gls{LPM} considerably if a large number of noisy samples is measured.


% \input{\thisDir/proofLOOCV.tex}
