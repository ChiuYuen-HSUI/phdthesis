\chapter{Non-parametric modeling}
\def\thisDir{ch03-lrm}
\tikzsetfigurename{ch03fig}
\label{sec:nonparametric}

\section{Introduction}
\label{sec:nonparametric:introduction}

Whereas many areas of system identification focus on parametric identification and/or parameter estimation, non-parametric techniques have recently received new research interest from the community.

\begin{itemize}
\item \TODO{explain non-parametric}
\item \TODO{explain why non-parametric is good}
\end{itemize}

The measurement of \glspl{FRF} of dynamic systems is an important step in many technical applications, often used as a simple visualization of the system dynamics.
\gls{FRF} measurement techniques are discussed, for instance, in
\citep{Schoukens1998,Schoukens2006LPM,Guillaume1996,Broersen1995,Pintelon2010LPM1,Antoni2007FRF,Pintelon2012}, and applied to practical devices and systems~\citep{Lim2010,Robinson1990,Behjat2010}, among others.
Besides, \glspl{FRF} have been shown to provide a quick insight into the dynamic properties of \gls{LTI} systems.
This capability is very useful for model validation and/or model selection~\citep{Pintelon2012}.

\input{\thisDir/lrm-paper.tex}

\section{LPM with impulse response truncation}
\input{\thisDir/LPMSmooth_ML_EG_JS_JL.tex}

\section{\TODO{Bias-variance trade-off }}


\subsection{\TODO{Tuning rules}}
\subsubsection{\TODO{Tuning using an Oracle}}
\subsubsection{\TODO{Cross-validation}}

\subsection{\TODO{Application}}

\subsubsection{\TODO{AVIS}}

\begin{subappendices}
  \section{Proof of the \glsentrytext{LOOCV} statistic for linear models}

  We follow an approach similar to \citet{Hyndman2014LOOCV} and \citet[Section 12.3.2]{Seber2003} to prove that for a linear model, one can easily compute the \gls{PRESS}/\gls{LOOCV} statistic by means of the so-called `hat-diagonals`, the diagonal entries of the hat matrix $H$.
  This is relevant, e.g. for local linear models used in the \gls{LRM} and hence also the \gls{LPM}.

  We consider the linear model
  \begin{equation}
    Z = K \theta + V
  \end{equation}
  with $Z \in \ComplexMatrix{N\times1}$, $\theta \in \ComplexMatrix{\nth \times 1}$, $K \in \ComplexMatrix{N \times \nth}$.
$V \in \ComplexMatrix{N \times 1}$ is a \gls{iid} complex gaussian random variable.
The least-squares solution of such a linear model is:
\begin{equation}
  \hat{\theta} = \left(K^{\HT} K \right)^{-1} K^{\HT} Z = \pinv{K} Z
\end{equation}
such that the estimated output $\hat{Z}$ is given by
\begin{equation}
  \hat{Z} = K \pinv{K} Z = H Z
\end{equation}
where $H$ is the so-called `hat matrix' that projects the measurements $Z$ onto the estimates $\hat{Z}$.

The \gls{PRESS} statistic used in \gls{LOOCV} is defined as~\citep[Chapter 12]{Seber2003}
\begin{equation}
  \PRESS \isdef 
    \frac{1}{N} 
      \sum_{i=1}^{N}
      \abs{
        Z_i - \ignoring{i}{\hat{Z}}
      }^2
\end{equation}
where $\hat{Z}_i$ is the $i^{\text{th}}$ row of $\hat{Z}$ and $\ignoring{i}{\hat{Z}}$ denotes the estimated $\hat{Z}_i$ when the $i^{\text{th}}$ data point (i.e. row) is removed from the estimation problem.
For linear models, this is equivalent to
\begin{equation}
  \PRESS_{\mathrm{linear}} = 
     \frac{1}{N}
     \sum_{i=1}^{N}
     \abs{
       \frac{Z_i - \hat{Z}_i}
                {1 - H_{ii}}
     }^2
     \text{.}
\end{equation}
Note that in this last expression, no terms $\ignoring{i}{\hat{Z}}$ occur, such that the statistic can be computed without having to compute $N$ additional linear models.
In this appendix, we prove that these last two expressions are equivalent for linear models.

\paragraph{Proof}
\TODO{proof}
\TODO{matrix inversion lemma}
\TODO{evt. Seber2003/ChristensenXXXX: QR decomposition to compute $H$}


\end{subappendices}
