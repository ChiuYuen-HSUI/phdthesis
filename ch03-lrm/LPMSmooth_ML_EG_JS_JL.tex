\researchBasedOn[This section]{Lumori2014TIM}
\TODO{labels into standardized form}

\subsection{Introduction}

This section uses nonparametric models in conjunction with the \gls{LPM}~\citep{Pintelon2010LPM1} to estimate the complex-valued \gls{FRF} in a linear \gls{LS} fashion.
The advantage of using the nonparametric models is basically that the `models` often are very simple and hence require little user interaction to construct.
Among the current nonparametric methods, the \gls{LPM} method is chosen due to its superiority over the classical nonparametric methods (windowing) in suppressing leakage errors~\citep{Bendat1993,Oppenheim1983,Schoukens2006LPM}. 
Consequently, the \gls{LPM} estimate of the \gls{FRF} is not hampered by leakage errors \citep{Pintelon2010LPM1,Schoukens2009LPM}. 
Note that a more general discussion on the \gls{LPM}, applied to noisy, real-valued data is found in \citep{Fan1996} with a specific focus on parametric tuning.

The main contribution of this section is to come up with an improved \gls{FRF} by developing a method for smoothing the \gls{FRF} estimated via the \gls{LPM}. This is done by truncating the associated impulse response, as in~\citep{Schoukens1998}, but with the following additional extension: the determination of the optimal truncation time, without any user interaction, in conjunction with the use of the \gls{LPM} for leakage reduction. 
The truncation point (cut-off index) is determined by one of two statistically inspired methods.
It is then shown that a smooth \gls{LPM} estimate of the \gls{FRF} lowers the variance, thus improving the assessment level of the dynamic properties of a given \gls{LTI} system.

Based on \citep{Schoukens2009LPM}, the nonparametric method developed in this work is formulated in an \gls{OE} framework. 
A \gls{SISO} \gls{LTI} system is considered.
The input signal is a known random noise signal and the output signal is corrupted by measurement noise. This is depicted in \figref{lpmtdrep} as a linear dynamic discrete-time \gls{SISO} system, whose full mathematical model is of the form:
\begin{equation}\label{eq:nparam:trunc:LPM:TD}
y(t)=G_0(q)u_0(t)+H_0(q)e(t)=G_0(q)u_0(t)+v(t)
\end{equation}
where $G_0(q)$ represents the dynamics of the system to be estimated, $u_0(t)$ is the input signal, $v(t)= H_0(q)e(t)$ is the noise source at the output, $H_0(q)$ is the noise dynamics, $e(t)$ is white noise, and $q^{-m}$ is the backwards shift operator ($q^{-m}x(t)$ = $x(t-m)$  with $m$ a positive integer).

\begin{figure}
\centering
\setlength\figurewidth{0.85\columnwidth}
\setlength\figureheight{0.68\figurewidth}
\input{\thisDir/figs/oesetup.tikz}
\caption{SISO LTI discrete-time system in an output error setup.}
\label{lpmtdrep}
\end{figure}

Numerous parametric identification techniques are devoted to the development of parametric plants $G(q,\theta)$ and parametric noise models  $H(q,\theta)$, where  $\theta$ is the model parameters vector  \citep{Ljung1999,Soderstrom1989}.

In this section, however, a nonparametric technique is developed, and
formulated in the frequency domain, consistent with \citep{Pintelon2012,Mahata2006}. 

The following  choices are made:

\begin{itemize}

\item The work in this section is for discrete-time systems. Denote the $k^{th}$ discretized frequency as $\Omega_k$ = $e^{-j2{\pi}kf_s/N}$, with $f_s$ the sample frequency and $N$ the total number of sample points.

\item  Describe the parametric plant model  $G(q,\theta)$ by the nonparametric \gls{FRF} counterpart  $G(\Omega_k)$  at the $k^{th}$ frequency.

\item Describe  the parametric noise model $H(q,\theta)$ (associated with the output noise source $v(t)$) by a nonparametric noise variance contribution $\sigma^2_v(\Omega_k)$ at the $k^{th}$ frequency.

\end{itemize}

The rest of the section is structured as follows. 
\secref{se:theoryLPMandWindowing} covers the theory on the \gls{LPM} and windowing for an \gls{OE} setup.
\secref{se:smoothingFRFestimate} discusses the novel smoothing method.
Ultimately, in \secref{se:simResults}, simulation results of the following FRF estimates and their corresponding variances are compared: (i) the smooth \gls{LPM} estimate $\hat{G}_\text{sm-poly}(\Omega_k)$, (ii) the \gls{LPM} estimate $\hat{G}_\text{poly}(\Omega_k)$.

These FRF estimates are also compared with the true \gls{LTI} system ${G}_0(\Omega_k)$.
Discussion of the simulation results is then followed by a conclusion in \secref{se:conclusion}.

\subsection{Theory on the LPM and Windowing: \glsentrytext{OE} Setup}
\label{se:theoryLPMandWindowing}

Details of the theory pertaining to this section may be found in \citep{Schoukens2009LPM}, basic aspects of which are presented below.


\subsubsection{Spectral Nonparametric Model and Assumptions}

Using convolution, the noiseless output signal in \figref{lpmtdrep} is $y_0(t) = g_0(t)*u_0(t)$, where the signals are sampled at $t = 0, 1, 2,...,N-1$ and the excitation signal $u_0(t)$ is random noise. As shown in \citep{Pintelon2012} and \citep{Pintelon1997} the \gls{DFT} spectra of the sampled signals at the frequency $\Omega_k$ are related as follows:
\begin{equation}\label{eq:nparam:trunc:LPM:leakage}
Y_0(k)=G_0(\Omega_k)U_0(k)+T_G(\Omega_k)
\end{equation}
where $U_0(k)$ is the \gls{DFT} spectrum of the random noise excitation,  $G_0(\Omega_k)$ is the transfer function and $T_G(\Omega_k)$ is the transients leakage term.
In this section, the \gls{DFT} of a signal $x(t)$ is defined explicitly as  \citep{Oppenheim1983}

\begin{equation}\label{eq:defDFT}
X(\Omega_k) = \frac{1}{\sqrt{N}}\sum_{t=0}^{N-1}x(t)e^{-\frac{j2\pi kt}{N}}
\end{equation}


The following assumptions are consistent with \citep{Schoukens2009LPM} for an OE setup (\figref{lpmtdrep}):
\begin{assumption}
The \gls{SISO} system $G_0$ and  the transients leakage spectrum $T_G$ are both smooth functions of the frequency.
\end{assumption}



\begin{assumption}

The spectrum of the  input signal $U_0$ is not a smooth function of the frequency, but a rough function, for example $U_0(k+1) - U_0(k)$ should not vanish to zero. %i.e. $U_0(k+1) - U_0(k) \neq 0$.
\TODO{assumption is not complete! some expectation is missing!}
\end{assumption}

In other words, for the excitation signal to be rough: the magnitude of the spectral difference $|U_0(k+1) - U_0(k)|$ must have a probability of 1 (unity) for it to remain in the same order of magnitude as $|U_0(k)|$, irrespective of the record length $N\rightarrow\infty$ and the corresponding  frequency resolution $f_0\rightarrow{0}$.


\begin{assumption}
The measured output signal is $y(t) = y_0(t) + v(t)$ with the exact input signal $u_0(t)$ known.
\end{assumption}


\begin{assumption}
The filtered white noise $v(t) = H_0(q)e(t)$ is the disturbing noise source at the output.
\end{assumption}


\subsubsection{The Hanning Window and the \glsentrytext{LPM}}\label{se:LPMFRFest}%\label{se:FRFhanningLPM}
\paragraph{Windowing}
The effect of windowing may be  exemplified by use of the Hanning window, which is very popular.
In a nutshell, the Hanning window is effective in reducing  the leakage errors, but  with a trade off of increased interpolation errors. %This is because the term $G_0(\Omega_k)U_0(k)$ in equation \eqref{eq:nparam:trunc:LPM:TD} is not smooth and $G_0(\Omega_k)$ varies with the frequency.
Pertinent details of the Hanning window are discussed in \citep{Schoukens2006LPM,Antoni2007FRF,Schoukens2009LPM,Wellstead1981} and \citep{Harris1978}.

The anomalies due to interpolation errors, associated with windowing, can easily be mitigated or circumvented by use of the \gls{LPM}. As shown in \citep{Pintelon2012}, the leakage errors are reduced from  $\mathcal{O}({N}^{-1})$, for Hanning windowing, to $\mathcal{O}({N}^{-3})$ or better, for the \gls{LPM}.


\paragraph{Formulation of the Linear LS LPM}
The \gls{LPM} is formulated as a nonparametric local linear \gls{LS} estimate using the full data record of length $N$ as outlined below \citep{Schoukens2009LPM}.
\citet[Section 7.2.2]{Pintelon2012} gives the general formulation of the \gls{LPM} for \gls{MIMO} systems.
The formulation in this section is a summary for a \gls{SISO} system.

The \gls{DFT} spectra at the output of the \gls{SISO} system in \figref{lpmtdrep} are derived from equations \eqref{eq:nparam:trunc:LPM:TD} and \eqref{eq:nparam:trunc:LPM:leakage}, \emph{viz}:
\begin{equation}\label{lpm1spectra}
Y(k)=G_0(\Omega_k)U_0(k)+T(\Omega_k)+V_0(k)
\end{equation}
where $V_0(k) = H_0(\Omega_k)E(k)$ is a noise term, and $T(\Omega_k)$ is the leakage term. The latter is the sum of the system and the noise leakage terms, i.e. $T(\Omega_k) = T_G(\Omega_k) + T_H(\Omega_k)$.

The smooth characteristics of $G_0(\Omega_k)$ and $T(\Omega_k)$ allow for the use of the Taylor series representations at the frequency lines $k+r$ in a frequency domain window of width $2n+1$, with a remainder $\bigO{\left(\frac{r}{N}\right)^{R+1}}$ of the first order Taylor series expansion \citep{Schoukens2009LPM}, \emph{viz}:
\begin{align}\label{lpmGTaylorS}
G_0(\Omega_{k+r})&=G_0(\Omega_k)+\sum_{p=1}^{R}g_p(k)r^p+
\bigO{\left(\frac{r}{N}\right)^{R+1}},
\\
\label{lpmTTaylorS}
T(\Omega_{k+r})&=T(\Omega_k)+\sum_{p=1}^{R}t_p(k)r^p+N^{-1/2}
\bigO{  \left(\frac{r}{N}\right)^{R+1}  }
,
\\
\text{where}&\ r\in\mathbb{W}_n,\quad \mathbb{W}_n = \{-n,-n+1,\dots,n\}
\end{align}

The choice of the tunable parameters $n$ and $R$ is discussed later on.

Neglecting the remainder, the Taylor series approximation of \eqref{lpm1spectra} can be written in a matrix vector form, \emph{viz}:
\begin{equation}\label{lpmGTMatrix}
Y(k+r)\approx{K(k, r)\theta(k)+V_0(k+r)}
\end{equation}
where $\theta(k)\in\mathbb{C}^{2R+2}$ is the column vector of the  parameters ($G_0(\Omega_k)$, $T(\Omega_k)$, $g_p$, $t_p$, and $p = 1, 2, ...., R$) and $K(k, r)$ is the row vector of their respective coefficients:
\begin{align}
K(k,r) = &\left[
\begin{matrix}
U_0(k+r) & U_0(k+r)r & \dots
\end{matrix}
\right.
\\
&\quad\left.\begin{matrix}U_0(k+r)r^R & 1& r&\dots&r^R\end{matrix}\right]\nonumber
\end{align}


Equation \eqref{lpmGTMatrix} can be written in the more compact (stacked vectors) form:
\begin{equation}\label{lpmGTMatrixStack}
Y_n(k)\approx K_n(k)\theta(k)+V_n(k)
\end{equation}
where $Y_n(k), V_n(k)\in\mathbb{C}^{2n+1}$, and $K_n(k)\in\mathbb{C}^{(2n+1)\times(2R+2)}$ are the stacked values of $Y(k+r)$, $K(k,r)$, and $V_0(k+r)$, respectively, for $r\in\mathbb{W}_n$.
The estimates of the parameters $\theta(k)$ are obtained as the minimizers of the least squares problem, \emph{viz}:
\begin{align}\label{eq:LSsolutionLPM}
\hat\theta(k) = \argmin_{\theta(k)} \left\|Y_n(k) - K_n(k)\theta(k)\right\|_2^2
\end{align}

The formulation of equation \eqref{eq:LSsolutionLPM} must take into account the fact that  a full rank of $K_n$ requires $n \geq R + 1$. The choice $n = R + 1$ results in the highest variance on the estimate, but with the smallest interpolation error \citep{Schoukens2009LPM}.


The values $R = 2$, and $n=R+1=3$ will be used in the \gls{LPM}.

When substituted into equations \eqref{lpmGTaylorS} and \eqref{lpmGTMatrix},  these yield the following quadratic local polynomials of $G_0$ and $T$ around a central frequency $\Omega_{k}$:
\begin{align}
G_0(\Omega_{k+r})
  &\approx 
  G_0(\Omega_k)+g_1r+g_2r^2
  \label{eq:lpm:expansionG:quadratic}
\\
  T(\Omega_{k+r})
    &\approx 
    T(\Omega_k)+t_1r+t_2r^2
    \label{eq:lpm:expansionT:quadratic}
\end{align}
with ($r\in\mathbb{W}_3$) and the vector of parameters:
\begin{equation}\label{lpmThetaEst}
\theta^\top(k)=\left[G_0(\Omega_k) \ g_1 \ g_2\  T_G(k)\  t_1 \ t_2\right]
\end{equation}
The \gls{LPM} estimate of the \gls{FRF} is the first element of the estimate $\hat\theta(k)$, obtained from \eqref{eq:LSsolutionLPM}, \emph{viz}:
\begin{equation}
\hat{G}_\text{poly}(\Omega_k) = \hat\theta_1(k)
\end{equation}


The estimation of $\hat{G}_\text{poly}(\Omega_k)$ at a single frequency $\Omega_k = 0.586\unit{Hz}$ is illustrated in \figref{LPM_Schematic_EG}.% centered at $k$, where $r = 0$.
\begin{figure}[htb] %  figure placement: here, top, bottom, or page
   \centering
   \setlength{\figurewidth}{0.8\columnwidth}
   \setlength{\figureheight}{0.68\figurewidth}

   \input{\thisDir/figs/LPMfig.tikz}
   \caption[Illustration of the LPM.]{Illustration of the \gls{LPM}. Top figures: input spectrum (left) and output spectrum (right). 
   A small band is selected (gray area), in which $\hat G_\mathrm{poly}(\Omega_{k+r})$ is estimated (black thick line, bottom figure) as a local polynomial. Only its central value $\hat G_\mathrm{poly}(\Omega_k)$ (white circle) is retained. This procedure is repeated for all frequencies $\Omega_k$.}
   \label{LPM_Schematic_EG}
\end{figure}
This procedure is repeated at each central frequency $\Omega_k$ for  $k\in \Set{1,\dots,N/2}$ (i.e. the entire frequency band, including Nyquist, but excluding \gls{DC}, and regularizing at the boundaries). This completes the \gls{LPM}, yielding an intrinsic averaging of the spectra of the estimated \gls{FRF}, $\hat{G}_\text{poly}(\Omega_k)$.

\section{Smoothing the \gls{FRF} estimate}\label{se:smoothingFRFestimate}
The method for smoothing (improving) the \gls{LPM} estimate of the frequency response function  is presented in this section together with pertinent assumptions. 
After obtaining the \gls{LPM} estimate of the \gls{FRF} (denoted as $\hat{G}_{\mathrm{poly}}(\Omega_k)$) from \secref{se:LPMFRFest}, the smoothing method is decomposed into the following procedures, which will be elaborated on later:
\begin{enumerate}
\item The impulse response $\hat g_\mathrm{poly}(t)$  is computed from the \gls{IDFT} of $\hat{G}_{\mathrm{poly}}(\Omega_k)$.

\item Assuming that the impulse response decays with the passing of time, the noise is bound to predominate after a certain time interval. 
In particular, when $t > \truncTime$, the impulse response has decayed below the noise floor.
In \secref{sec:nonparametric:truncation:Ftest} and \secref{sec:nonparametric:truncation:exponentialFit}, two approaches are introduced to estimate $\truncTime$.

\item An accurate estimate of the \gls{DC} value of the \gls{FRF} is computed by inspecting the average value of the tail of the estimated impulse response.

\item A truncation is effected at the point beyond which the data record (impulse response) is buried in noise.

This action results in smoothing the \gls{LPM} estimate of the \gls{FRF}.
\end{enumerate}

These procedures require that the following assumptions are satisfied.

\begin{assumption}
The estimate $\hat G_\mathrm{poly}(\Omega_k)$ is available at all frequencies $\Omega_k$ for $k\in\{1,2,\dots,N/2\}$.
\end{assumption}

This assumption ensures that the impulse response corresponding to the \gls{FRF} can be computed (up to its mean value). It requires that the input signal excites the whole unit circle. This is satisfied, for instance, by using white noise as an excitation signal.


\begin{assumption}\label{ass:imprespdecay}
The impulse response $g(t)$ decays exponentially over time, i.e. $\exists A \in \PositiveRealNumbers, \exists a \in \NegativeRealNumbers \without \Set{0}, \forall t \in \PositiveRealNumbers: \abs{g(t)} \leq A \exp(- a t)$.
\end{assumption}

\begin{assumption}\label{ass:decay90perctime}
Within $90\%$ of the measurement window, the impulse response decays
 to a level that is indistinguishable from the noise.
\end{assumption}

Note that \assref{ass:imprespdecay} and \assref{ass:decay90perctime} exclude the possibility of considering a system that is a pure integrator.

\subsubsection{Obtaining the Impulse Response Function}

The estimated impulse response function  $\hat g_{\mathrm{poly}\setminus \mathrm{DC
}}(t)$ is obtained explicitly via the \gls{IDFT} of the \gls{LPM}-estimate $\hat G_\mathrm{poly}(\Omega_k)$ of the \gls{FRF} in accordance with equation \eqref{eq:defDFT}, \emph{viz}:

\begin{align}\label{eq:impRespiDFT}
\hat g_{\mathrm{poly}\setminus \mathrm{DC
}}(t) &= \frac{1}{\sqrt{N}}\sum_{k=1}^{N-1}\hat G_\mathrm{poly}(\Omega_k)e^{\frac{j2\pi kt}{N}}
\end{align}
where the estimated \gls{FRF} in the frequency band between  the Nyquist and the sample frequencies is obtained as follows:

\begin{align}
\hat G_\mathrm{poly}(\Omega_{N-k}) = \overline{\hat G_\mathrm{poly}(\Omega_k)},\ \text{for}\ k=1,\dots,N/2
\end{align}
(with $\conj{\hat G_\mathrm{poly}}$ the complex conjugate of $\hat G_\mathrm{poly}$) to ensure $\hat g_\mathrm{poly}(t)$ to be real.

\TODO{fix reference Section II}
Smoothing of the estimated \gls{FRF} requires the correct estimate of the impulse response. Unfortunately, the \gls{LPM} presented in \secref{se:theoryLPMandWindowing} and in \citep{Schoukens2009LPM} does not estimate the \gls{FRF}  at the frequency $\Omega_0$ (i.e. \gls{DC} value of the \gls{FRF}), hence the subscript $\setminus\mathrm{DC}$ in equation \eqref{eq:impRespiDFT}. Consequently, the mean value of the corresponding estimated impulse response given in equation \eqref{eq:impRespiDFT} is not correct. 
This limitation is lifted by developing a simple estimator of the mean value, presented below.

\subsubsection{Estimating the \glsentrytext{DC} Value of the \glsentrytext{FRF}}\label{se:DCvalueEst}
The correct mean value of the impulse response accounts for (i.e. estimates) the \gls{DC} value of the \gls{FRF}. A time-domain method is proposed for estimating the mean value of the impulse response in equation \eqref{eq:impRespiDFT}. %The first is a frequency-domain technique and the second is a time-domain technique.

According to \assref{ass:imprespdecay}, the true impulse response tends to zero asymptotically; and by \assref{ass:decay90perctime}, the last $10\%$ of the estimated impulse response is noise, plus a constant value due to an inaccuracy in the    average value of the impulse response. The correct estimate of the mean value is obtained by shifting the whole signal such that the last $10\%$ of the impulse response is centered around 0.
To this end, the following procedure is executed:


\begin{enumerate}
\item Compute the mean value $m_{g10}$ of the last 10\% of the impulse response $\hat g_{\mathrm{poly}\setminus \mathrm{DC
}}(t)$, estimated by the \gls{LPM}, as %$\hat g_\mathrm{poly}(t)$ as
\begin{align}
m_{g10} = \frac{1}{\lceil0.1N - 1\rceil}\sum_{t=\lfloor0.9N\rfloor}^{N-1}\hat g_{\mathrm{poly}\setminus \mathrm{DC
}}(t)
\end{align}

\item Next, subtract the computed mean value $m_{g10}$ from $\hat g_{\mathrm{poly}\setminus \mathrm{DC
}}(t)$ to obtain the improved impulse response $\hat g_\mathrm{poly}(t)$, \emph{viz}:


\begin{align}
\hat g_\mathrm{poly}(t) = \hat g_{\mathrm{poly}\setminus \mathrm{DC}}(t) - m_{g10},\ \text{for}\ t=0,1,\dots,N-1
\end{align}
\end{enumerate}

\subsection{Truncation Via the $F$-Test}
\label{sec:nonparametric:truncation:ftest}
To distinguish the part of the measured impulse response where the actual impulse response dominates from the part where the noise dominates, the estimated impulse response $\hat{g}_\mathrm{poly}$ is split into smaller segments. 
One can then gradually determine whether these segments are noise or the actual impulse response.

% 1) cut into NS segments of minimally NM samples
% 2) assume last segment == noise
%    start from back
% 3) F-test on Var, Var==RMS if dc == 0 (i.e. noise)
%    H0: s²(1) =  s²(2)
%    H1: s²(1) >  s²(2)
%    alpha >> ==> beta <<
% 4) if H1 and RMS(signal)/RMS(noise) > f
%    robustness, SNR
%    select segment i
% 5) refine until wanted resolution is obtained

\begin{enumerate}
  \item The impulse response $\hat{g}_\mathrm{poly}(t)$ is split into $N_S$ segments of approximate equal lengths
$L_S = \floor{\frac{N}{N_S}}$. 
If necessary, the length of the first segment is reduced to allow for the others to have equal lengths.
The $i^{\text{th}}$ segment is denoted $\segm{\hat{g}}{i}$, for $i \in \left\{1,\ldots,N_S\right\}$.
  By \assref{ass:decay90perctime}, the last segment, $\segm{\hat{g}}{N_S}$, is noise when $L_S \leqslant 0.1N$.
  
  \item Iteration is then carried out from the last segment ($i = N_S$) towards the beginning of the impulse response. To determine whether the preceding segment $\segm{\hat{g}}{i-1}$ is completely dominated by noise,  both an $F$-test and an absolute criterion on the RMS are used.

  The $F$-test is suited to compare the variances of different segments $\segm{\hat{g}}{i}$ and $\segm{\hat{g}}{i-1}$ \citep{Parsons1974}, while accounting for the degrees of freedom in each segment.
  For zero-mean noise, these variances are the \gls{RMS} value of each noise segment.
  This has the advantage that a segment in which the impulse response still has a significant amplitude is likely to have a large \gls{RMS} value, which is more likely to be detected.
  Let $s^2(i)$ denote the mean square value of segment $\segm{\hat{g}}{i}$, \emph{viz}:
  \begin{equation}
    s^2(i) 
    \isdef \frac{1}{L_S} \sum_{j=1}^{L_S} \left(\segm{\hat{g}}{i}(j)\right)^2
    = \left( \rms{\hat{g}^{[i]}} \right)^2
           \text{.}
  \end{equation}
  Starting with the last segment ($i = N_S$), the following null hypothesis and the alternative hypothesis are formulated:
  \begin{align}
     \nullHypothesis &: s^2(i-1) = s^2(i)\\
     \altHypothesis    &: s^2(i-1) > s^2(i)
     \text{.}
  \end{align}
  As segment $\segm{\hat{g}}{i}$ has been tested and been classified as noise (or assumed to be noise if $i=N_S$), the null hypothesis effectively states that the segment $\segm{\hat{g}}{i-1}$ is also noise.
  The alternative case occurs when $\segm{\hat{g}}{i-1}$ has a significant change, which indicates the presence of the signal.
  Using the $F$-test on the \gls{RMS} values of the two segments with a high confidence level of $\alpha=0.999$ and the aforementioned hypotheses, we can determine whether $\segm{\hat{g}}{i-1}$ is likely to be part of the actual impulse response.
  In particular, the null hypothesis is rejected when the empirical $F$ statistic is small, i.e.
  \begin{equation}
    F \isdef \frac{s^2(i-1)}{s^2(i)} \leq \ICDF{\FDistribution} \left( \alpha; n_{i-1}, n_{i} \right)
  \end{equation}
  where $ \ICDF{\FDistribution}$ is the inverse \gls{CDF} of the \mbox{$F$-distribution} with $n_{i-1}$ and $n_{i}$ degrees of freedom.
  In this case $n_{i-1}$ and $n_{i}$ are the respective number of samples in the $(i-1)^{\text{th}}$ and $i^{\text{th}}$ segment of the impulse response.

  The high level of confidence ($\alpha=0.999$) ensures that the probability of a Type~II error is smaller than $1-\alpha$ \citep{Parsons1974}.
  In our case, such an error means that a part of the actual impulse response would be falsely classified as noise, which could significantly increase the bias as information of the system is discarded.
  A Type~I error is less detrimental since, in that case, noise is falsely classified as a signal component and kept in the impulse response, thereby causing a sub-optimal value of the variance of the estimate.
  As the \gls{LPM} samples are correlated over a short frequency span, the actual noise present in $\hat{g}$ may be slightly non-stationary.
  To cope with this, one can introduce other criteria which must be satisfied together with the outcome of the $F$-test.
  A criterion that shows good results is to check whether the segment $\segm{\hat{g}}{i-1}$ has an RMS value that is at least a factor $\kappa$ larger than the RMS of the noise.
  Even for a moderate $\kappa = 1.1$, a large improvement in the detection was observed. 
%  \JL{Q: doesn't this increase the probability of a Type II error?}
%  \EG{A: This slightly increases for small $\kappa$ and increases significantly for large values of $\kappa$. But it is needed to overcome nonstationarity of the noise.}

  \item This procedure is repeated until a segment $\segm{\hat{g}}{\starred{i}-1}$ is found that is dominated by the actual impulse response according to the outcome of the $F$-test and the absolute \gls{RMS} value of the segment.
  At that point, it is very likely that the transition from noise to the actual impulse response happens within the segment $\segm{\hat{g}}{\starred{i}-1}$.
  One now has the choice to accept the last sample of $\segm{\hat{g}}{\starred{i}-1}$ to be the last meaningful sample $\truncTime$ for the impulse response.
  The accuracy of this estimate is limited by the length of the segment $L_S$.

  \item
  A more accurate estimate can be obtained by dividing the segment $\segm{\hat{g}}{\starred{i}-1}$ -- which may contain both the signal and noise -- yet again into smaller segments.
  The procedure described above is then repeated until a satisfactory, accurate $\truncTime$ is obtained, or until the subsegments have a length that is too short ($L_{S\min} = 10$) to guarantee the accuracy of the RMS values. %rely on the RMS values to be accurate.
  To start this refinement, the last $\segm{\hat{g}}{\starred{i}}$ should be used to compare the RMS with the $F$-test, since the last subsegment of $\segm{\hat{g}}{\starred{i}-1}$ cannot be asserted to be dominated by noise.

\end{enumerate}
This procedure is illustrated in \figref{fig:nonparametric:truncation:Ftest} for the system described by the following simulation equations, which are relevant to the LPM outlined in Section \ref{se:LPMFRFest}:
\begin{subequations}
\label{eq:systemSimulations}
\begin{align}
y_0(t)  &= 1.5371y_0(t-1)    -0.9025y_0(t-2) + u(t)\\
y(t)       &= y_0(t) + e(t),
\end{align}
\end{subequations}
where $e(t)$ is a white noise sequence, such that the SNR of the output signal is 78.2~dB.
% \eqref{eq:systemSimulations}. 
The figure shows segments of the estimated impulse response. The last two segments are dominated by the noise, while all samples at $t<80$ have large system contributions. The algorithm outlined above selects $\truncTime = 80$, beyond which all samples are set to zero, resulting in a smoothed \gls{FRF} estimate. 

\begin{figure}[tbh]
\centering
 \setlength\figurewidth{0.68\columnwidth}
 \setlength\figureheight{0.68\figurewidth}
  \input{\thisDir/figs/truncRMS.tikz}
\caption[Illustration of impulse response truncation using the $F$-test.]{Illustration of truncating the impulse response by performing an $F$-test on successive segments of the impulse response.}
\label{fig:nonparametric:truncation:Ftest}
\end{figure}

\subsection{Truncation of the Impulse Response Via Exponential Fitting}
\label{sec:nonparametric:truncation:exponentialfit}

The \gls{FRF} estimate is smoothed by truncating the estimated impulse response function. 
The truncation is applied at the time index beyond which the signal is indistinguishable from the noise.

This is done via the fit of an exponential function on the maxima of the impulse response, implemented as follows.

\begin{enumerate}
\item Obtain the estimate of the impulse response $\hat g_\mathrm{poly}(t)$ from the \gls{LPM}, corrected for its \gls{DC} value as discussed in \secref{se:DCvalueEst}. 
Henceforth, $\hat g_\mathrm{poly}(t)$ will simply be denoted as $\hat g(t)$.

\item Obtain an estimate $\hat \sigma_\mathrm{n}$ of the standard deviation of the noise from the last 10\% of the data, \emph{viz}:
\begin{align}
\hat \sigma^2_\mathrm{n}=\frac{1}{\lceil0.1N - 1\rceil}\sum_{t=\lfloor0.9N\rfloor}^{N-1}\hat g^2(t).
\end{align}
This is valid, as per \assref{ass:decay90perctime}.

%This assumes that the impulse response of the system is not longer than 90\% of the length of the measured time interval.

\item Obtain $\mathbb{T}_\mathrm{HSNR}$ as the set of time instants where $\hat g(t)$ is significantly above the standard deviation of the noise. Only samples with absolute values of at least $5\hat\sigma_\mathrm{n}$ are retained: %Only samples with an absolute value of at least $5\hat\sigma_\mathrm{n}$ have been retained:
\begin{align}
\mathbb{T}_\mathrm{HSNR} = \left\{
t:|\hat g(t)|\geqslant 5\hat\sigma_\mathrm{n}
\right\}.
\end{align}
(Subscript HSNR stands for High \gls{SNR}.)

%This choice leaves a probability of $6\times10^{-7}$ of retaining a pure noise sample, in the case of Gaussian noise.

\item Find the set $\mathbb{T}_\mathrm{max}$ of indices corresponding to monotonically decreasing maxima of the impulse response:
\begin{align}\label{eq:TmaxDef}
\mathbb{T}_\mathrm{max} = \left\{
t: \left| \hat g(t)\right|>
\left|\hat g(t')\right|,
t < t' < N \land t'\in\mathbb{T}_\mathrm{HSNR}
\right\},
\end{align}

\item Fit an exponential function $Ae^{at}$ approximating $\hat g(t)$, in $t_\mathrm{max}\in\mathbb{T}_\mathrm{max}$. 
This is done by solving the following expression

\begin{align}\label{eq:expFit}
\ln \left|\hat g(t)\right|\approx \ln A+at,\ \mathrm{with}\ t\in\mathbb{T}_\mathrm{max},
\end{align}
for $\ln A$ and $a$ in a least squares sense.
This is a quadratic problem in the parameters and, thus, amount to a convex optimization problem that can be solved directly.
Since  $\ln \left|\hat g(t)\right|$ decreases for an increasing $t$ in $\mathbb{T}_\mathrm{max}$ (by construction in equation \eqref{eq:TmaxDef}), the estimated $a$ from equation \eqref{eq:expFit} is always negative.

\item
Determine the first time-instant $\truncTime$ at which the estimated exponential gets significantly below the noise floor, \emph{viz}:
\begin{align}\label{eq:truncTimeExpFit}
\truncTime = \min \left\{t:Ae^{at} < \gamma\hat\sigma_\mathrm{n}\right\}
\end{align}
where the parameter $\gamma$ can be tuned such that an appropriate trade-off between the decreased variance and the increased bias on the estimated smoothed FRF is found. This is discussed below. %(see the discussion below).
% The value of $\gamma$ to be used depends on the system. %$\gamma = 0.1$ was found to be a good rule of thumb.

\item
 Truncate the estimated impulse response for $t \geqslant \truncTime$.

\end{enumerate}
  
\begin{figure}[tbh] %top bottom here
\centering
\setlength\figurewidth{0.85\columnwidth}
\setlength\figureheight{0.68\figurewidth}
\input{\thisDir/figs/expfit.tikz}
\caption[Impulse response truncation using exponential fit.]{Truncation of the impulse response $g(t)$ via the fit of  an exponential through the peaks of $\abs{g(t)}$ that are above $5\noiseStd$ with $\noiseStd$ is the estimated noise level, as per \eqref{eq:TmaxDef}.}
\label{fig:nonparametric:trunc:impresp:expfit}
\end{figure}

As an illustration, this procedure was applied to the noisy impulse response of the system described by the following difference equation:

\begin{equation}
y_0(t) = 2.583y_0(t - 1) -2.176y_0(t - 2)+0.592y_0(t-3) + u(t)
\end{equation}

The measured signal was disturbed by random white noise, $y(t) = y_0(t) + e(t)$, such that the \gls{SNR} was $14.2\unit{dB}$.
The result is depicted in \figref{fig:nonparametric:trunc:impresp:expfit}. 
An exponential function (black thick line) is fitted on the maxima (gray circles) of the absolute value of a noisy impulse response (dark gray line). The truncation time (black vertical line) $\truncTime$ was selected as the time instant at which the fitted exponential fell below $0.4\noiseStd$ (i.e. $\gamma = 0.4$ in equation \eqref{eq:truncTimeExpFit}).

\paragraph*{Considerations on the choice of $\gamma$}

\begin{itemize}
\item The tuning parameter $\gamma$ is application- and system-dependent.  A higher value lowers the variance of the estimated \gls{FRF}, but increases its bias, and vice versa.

\item The bias error is highest in the vicinity of (sharp) resonance peaks. 
If the latter is to be estimated with a high accuracy, a value $\gamma \ll 1$ must be used.

\item If one is interested in obtaining a smooth initial estimate of the \gls{FRF}, a (small) bias error is acceptable, and choosing $\gamma \approx 1$ was found to be a good rule of thumb.
\end{itemize}

\subsection{Simulation Results}\label{se:simResults}

\figref{figLPMvsTrunc} and \figref{fig:pdfAndRMSeVStruncTime} compare the \gls{LPM} with and without truncation of the impulse response.
Here, the truncation instant $\truncTime$ is determined using the exponential fitting method.
They were obtained from simulations on  the system described by the following simulation equations, which are relevant to the \gls{LPM} outlined in \secref{se:LPMFRFest}:
\begin{subequations}
\label{eq:systemSimulations}
\begin{align}
y_0(t)  &= 1.5371y_0(t-1)    -0.9025y_0(t-2) + u(t)
\\
y(t) &= y_0(t) + e(t),
\end{align}
\end{subequations}
where $e(t)$ is a white noise sequence, such that the \gls{SNR} of the output signal is $18.3\unit{dB}$.

\begin{figure}
    \centering
    \setlength\figurewidth{0.85\columnwidth}
    \setlength\figureheight{0.68\figurewidth}
    \input{\thisDir/figs/figLPMvsTrunc.tikz}
    \caption[Comparison of FRF estimated using LPM and Truncated LPM.]{The bias $\sampleBias{\placeholder}$ and standard deviation $\sampleStd{\placeholder}$ of the \gls{FRF} estimates using both the \gls{LPM} (blue) and truncated~\gls{LPM} (red) are shown. 
    It can be seen that the truncation reduces the variance at the cost of an increased bias, however, the overall \gls{RMS} error is reduced by the truncation.}
    \label{figLPMvsTrunc}
\end{figure}


In \figref{figLPMvsTrunc} one observes the following:
\begin{itemize}
\item a decrease of the variance on the truncated estimate of about $10 \unit{dB}$ compared to the non-truncated \gls{LPM}, is observed. %i.e. a decrease of the black crosses w.r.t.~the gray ones is observed.

\item the error on the truncated \gls{LPM} estimate is strongly correlated over the frequency. This must be taken into account when formulating a maximum likelihood parametric estimator of the system.

\item an increase of the bias of the truncated estimate, especially in the vicinity of the resonance frequency. Still, this bias lies below the variance of the non-truncated estimate. As such, for a single experiment, the increase in bias still yields a better estimate when truncation is invoked.

This bias depends on the time instant at which the truncation is performed, as discussed below.

\end{itemize}

\begin{figure}
   \centering
        \setlength\figurewidth{0.68\columnwidth}
        \setlength\figureheight{0.68\figurewidth}
        \input{\thisDir/figs/pdfs.tikz}
         \caption[RMS error of the FRF versus truncation time.]{
         The left y-axis shows the \gls{RMS} error of the estimated \gls{FRF} as a function of the chosen truncation time $\truncTime$.
         The corresponding optimal $\truncTime$ is indicated as well.
         Both bar plots (right y-axis) indicate for $\nMC =1000$ realizations of the disturbing noise and excitation signal, the number of times that $\truncTime$ has been selected by means of either the method using the $F$-test or the method that fits an exponential.
         It can be seen that both methods allow to almost halve the \gls{RMS} error and approach the optimal truncation time.}
   \label{fig:pdfAndRMSeVStruncTime}
\end{figure}



In \figref{fig:pdfAndRMSeVStruncTime}, the black graph is the \gls{RMS} error of the estimated \gls{FRF} (without the \gls{DC} value) as a function of the time $\truncTime$ at which the impulse response is truncated. 
To the left of the optimal truncation length, the \gls{RMS} error increases steeply.
In that region, one is essentially using a overly simplified impulse response to represent the system and hence the \gls{RMS} error is dominated by the bias error.
To the right hand side of the minimum, the \gls{RMS} error increases more gradually, due to an increase of the noise variance.

A good practice would be to truncate the impulse response at the minimizer (black dot) of the \gls{RMS} error. 
However, it should be noted that this minimizer is unknown in practice, because it would require the true \gls{FRF}.

The truncation time is determined from the data as described in \secref{sec:nonparametric:truncation:exponentialfit}. 
This was done on $1000$ realizations of the noise, and depicted in \figref{fig:pdfAndRMSeVStruncTime} by the histogram.
Clearly, both methods for selecting the truncation time $\truncTime$ have a good overall performance, based on the mode of their distribution (around the $90^{\text{th}}$ sample). 

For the $F$-test based method, the obtained $\truncTime$ values only take a discrete set of values due to the limitation of the segment length.
While most trials result in $\truncTime \in \Set{90, 80, 70}$, a few outliers can be observed.
This can be attributed to the high power of the $F$-tests that are performed.

For the exponential fitting approach, however, a closely grouped set of (continuous) values for $\truncTime$ is obtained around the $90^{\text{th}}$ sample, without significant outliers.
Although this distribution has a mode quite a bit higher than the method using the $F$-test, due to the absence of outliers, this method is more reliable to use.

From the plot, we can conclude that the \gls{RMSE} increases rapidly when $\truncTime$ is smaller than the optimal.
On the other hand, selection of a value of $\truncTime$ that is too large, is not nearly as detrimental to the modeling error.
The graph also shows that the \gls{RMSE} of the model can be decreased from $0.62$ (without truncation) to $0.18$ when the optimal truncation is applied. Also, one observes a low sensitivity of the \gls{RMSE} w.r.t.~$\truncTime$, when truncating at times beyond that optimum. 
Therefore, a somewhat conservative truncation method is still likely to yield a close to optimal result.

\begin{guideline}[Check and reinforce assumptions in the data.]
As illustrated by the presented smoothing operation, it pays off to check typical assumptions of the system (e.g. smoothness of the \gls{FRF}, finite support of the impulse response) and even artificially reinforce such assumptions to improve the model quality.
\end{guideline}

\subsection{Conclusion}
\label{se:conclusion}

The section introduced a novel time domain method to smooth the \gls{LPM}-estimate of an \gls{FRF}. 
It consisted of, after obtaining the \gls{FRF} from the \gls{LPM}, computing the associated estimated impulse response via the \gls{IDFT}.
Then, it was determined statistically at which time index the impulse response had decayed below the noise floor, yielding a point beyond which the response may be set to zero.

The different methods to determine $\truncTime$ presented in this section, essentially enforce different prior knowledge of the impulse response of the system.
The exponential fitting technique relies on the exponential decay present in most physical systems.
The $F$-test based technique is better suited for systems where a sudden jump in the impulse response is present.

The results clearly indicate that the truncation technique lowers the impact of the noise on the estimate of the \gls{FRF}, resulting in a decreased variance. 
A bias-variance trade-off is possible by tuning the time beyond which the impulse response is indistinguishable from the noise.

\TODO{write about boxcar average, add reference}
\begin{figure}
   \centering
        \setlength\figurewidth{0.68\columnwidth}
        \setlength\figureheight{0.68\figurewidth}
        \input{\thisDir/figs/expFitBoxAverager.tikz}
         \caption{Exponential fit of the impulse response of a  boxcar averager.}
\end{figure}

\begin{figure}
   \centering
        \setlength\figurewidth{0.68\columnwidth}
        \setlength\figureheight{0.68\figurewidth}
        \input{\thisDir/figs/pdfs-rmse-boxcar.tikz}
         \caption{RMSE vs $\truncTime$ for boxcar}
\end{figure}


\TODO{RMSE vs $\truncTime$ for boxcar figure}
