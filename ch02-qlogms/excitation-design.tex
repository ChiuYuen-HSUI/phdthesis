\chapter{Design of excitation signals}
\label{sec:excitation}
\def\thisDir{ch02-qlogms}
\tikzsetfigurename{ch02fig}
\myEpigraph{A common mistake that people make when trying to design something completely foolproof is to underestimate the ingenuity of complete fools.}{Douglas Adams}{Mostly Harmless}
\researchBasedOn{Larsson2012SYSID, Geerardyn2012IMTC, Geerardyn2013TIM, }

\section{Introduction}
\label{sec:excitation:intro}
Input design deals with the task of choosing an excitation signal in a way so that the information obtained from the experiment is maximized. 
This is a problem which has a long history in statistics. 
In system identification, the focus has mostly been on input design for dynamic systems.
A common approach is to design the input spectrum and thereby maximizing the information matrix of the estimates, see e.g. \citep{Fedorov1972,Goodwin1977}.

A complicating issue in identification of dynamic system is the fact that the information matrix often depends on the true parameter values. 
Consequently, an optimal input design, based on the information matrix, will depend on quantities that are not know at the time of the design.
There are two main solutions to this in the engineering literature~\citep{Goodwin2006GBO}:
\begin{enumerate}
\item Iterative input design where the design updated as more information about the system is available, see e.g.~\citep{Hjalmarsson2005,Gevers2005}.
The disadvantage of this approach is that multiple experiments are required to converge to the optimal design.
Moreover, since the signal is optimized for a specific system model, it is likely that changes in the dynamics are hard to observe using such a signal.

\item Robust (optimal) input design makes the input design robust to uncertainty in the prior knowledge of the parameter values, see e.g.~\citep{Rojas2007,Goodwin2006GBO,Rojas2012} and references therein. 
While the signal is more robust towards uncertainty in the prior knowledge, it requires more forethought than (nominal) optimal designs.
E.g. the approach followed by~\citet{Rojas2007} relies on both nominal parameter values and  knowledge of the distribution function of these parameters to design a robust (optimal) excitation signal.
As such, this requires more advanced prior knowledge of the system to cope with the uncertainty of the prior knowledge.
\end{enumerate}

\subsection{General purpose robust excitation signals}
\label{sec:excitation:intro:approach}
In this chapter, the goal is to construct excitation signals that bridge the properties of general purpose and robust excitation signals to alleviate the chicken-and-the-egg problem that optimal strategies typically incur.
In particular, a generic excitation signal will be used, but its properties are tuned in such a way that it attains some properties of robust excitation signals without requiring extensive prior information.
We will consider input design when it is known that the system has resonances, possibly spanning a frequency band that may cover several decades. 
It is however not necessary to know how many resonances are in the system and at what precise frequencies those may occur.

Such problem settings occur in many diverse fields of experimentation, such as the measurement of biochemical impedances \citep{Bragos2001,Sanchez2011}, electrochemical impedance spectroscopy \citep{Niedostatkiewicz2009,VanGheem2004,Breugelmans2010}, electronics \citep{Munir2011}, mechatronics \citep{Steinbuch1998,Oomen2016}, vibration analysis~\citep{Karnopp1995,Voorhoeve2015SYSID} and acoustic testing \citep{GarciaLopez2014,DAmico2014} amongst others. 

  Any \gls{LTI} system of finite order can be expanded into a finite sum of sub-systems of first and second order by means of a partial fraction expansion~\citep{Oppenheim1983}.
  Therefore the performance of the proposed method is assessed on a prototype system of second order.
  This does not limit the generality of the approach as we can add several of such systems to construct a general \gls{LTI} system.
  
  To ensure that any second order system with its resonance frequency in the specified frequency range is measured with an equal accuracy, the system output should contain a comparable power level irrespective of its resonance frequency.
  For a system with a sufficiently low damping, one can intuitively understand that most of the information is contained within the $3 \unit{dB}$ bandwidth of the system.
  Remember that an \gls{LTI} system is naturally described in a logarithmic frequency axis (the slopes of such a system are specified in $\unit{dB}/\text{decade}$).
  Hence, an excitation with a flat power spectrum and a linearly spaced frequency grid will over-excite the high end of the band at the cost of the low frequencies.
  To restore equal excitation signal levels over the complete range of frequencies, we will need to distribute the power in $1/\omega$ over the frequency band~\citep{Goodwin2006,Goodwin2006GBO}.
  In a noise excitation context, this results in pink noise.
  In a periodic setting, this results in a multisine whose frequency components are spaced equidistantly on a logarithmic axis.
  
  The main problem is that network and dynamic signal analyzers typically
  rely on an equidistant frequency grid that comes with the \gls{DFT}.
  A logarithmically spaced signal, on the other hand, does not match well to an equidistant frequency grid.
  More specifically, the lack of frequency resolution at the lowest frequencies hampers the quality of the power distribution at the low edge of the frequency band.
  To circumvent this disadvantage, we will construct quasi-logarithmically spaced signals.
  Their frequency spacing is approximately logarithmic but nevertheless conforms to the linear frequency grid of the \gls{DFT}.

  We will also present a simple method to determine a suitable density of a logarithmically spaced signal for a predefined minimal damping of the system to be tested.

  \paragraph{Contents}
  In \secref{sec:excitation:inputDesign}, we first consider (optimal) input design for an example second-order system.
  \secref{sec:excitation:logarithmic} expounds the use of a signal with $1/f$  power spectrum on some basic sub-systems.
  Next, in \secref{sec:excitation:parametrization} \gls{LTI} systems are parametrized to facilitate a robust input design based on the observations of the preceding section.
  In \secref{sec:excitation:multisine} a comparison of different discrete frequency grids is provided, this is related to the problem of generating a multisine signal with a $1/f$ power spectral density.
  Next, \secref{sec:excitation:optimAlpha} describes the selection of a suitable frequency ratio $\alpha$ for a logarithmically spaced grid such that constant model quality is obtained.
  The usefulness of the proposed signals is illustrated on simulations in \secref{sec:excitation:simulation} and validated on the measurement of a set of band-pass filters in \secref{sec:excitation:measurement}.
  Conclusions are formulated in \secref{sec:excitation:conclusion}.

\section{Input design}
\label{sec:excitation:inputDesign}
For the theoretical analysis, we consider \gls{SISO} \gls{OE} systems, i.e.
\begin{align}
y(t) &=G(s,\theta)u(t)  + e(t),
\end{align}
where $u(t)$ is a known input sequence, $y(t)$ the output sequence, and $e(t)$ is zero mean Gaussian white noise with variance $\sigma_{e}^2$.
$G$ is a rational, time invariant transfer function in continuous time.
For notational simplicity, $s$ is used both as the complex Laplace variable and the differentiation operator; it should be clear from the context which is used. 
The transfer function is parameterized by the vector $\theta$.
These parameters are unknown and hence we are interested in estimating their values. 

The goal of input design is to find an input that under the given experimental conditions, gives as much information as possible about the system.
This can be done by optimizing the input power spectrum such that  a scalar criterion involving the Fisher information matrix is minimized.

In open loop experiments, the scaled average information matrix is given by
\begin{align}
\IF & = \lim_{N\rightarrow\infty}
          \frac{1}{N}
            \sum_{t=1}^{N}
              {\left(\frac{\partial\hat{y}(t)}{\partial\theta}\right)
               \left(\frac{\partial\hat{y}(t)}{\partial\theta}\right)^{\T}},\\
\frac{\partial\hat{y}(t)}{\partial\theta}
    & =\frac{\partial G(s,\theta)}{\partial \theta}u(t).
\end{align}
By Parseval's theorem, $\IF$ can be expressed as
\begin{align}
\IF &= \frac{1}{\pi}
         \int\limits_0^\infty \RealPart
           \left\{      \frac{\partial G(j\omega)}{\partial \theta}
                  \left[\frac{\partial G(j\omega)}{\partial \theta}\right]^{\HT}
           \right\}
           \powerspec{u}(j\omega)
           \dd{\omega} \text{.}
\label{eq:excitation:IF}
\end{align}
The information matrix is a function of the input power spectrum $\powerspec{u}$, which is the only entity that can be used to improve the quality of the estimates. 
A common optimality criterion is D-optimality where $\det{\IF}$ is maximized~\citep{Goodwin1977}.

\begin{remark}
D-optimal input designs are quite popular since this criterion minimizes the volume of uncertainty ellipsoid of the parameters.
Moreover, the D-optimal input design is independent of the parametrization of the system (as long as the different parametrizations can be related by a bijective function)~\citep[Chapter 6]{Goodwin1977}.
\end{remark}

The information matrix tells us how good the individual parameter estimates are. However, if we are interested in estimating a transfer function $G(s,\theta)$ the quality of this estimate is of more importance. 
It is possible to find an expression for the variance of $\hat{G}(j\omega,\theta)$ using the Gauss approximation formula \citep{Ljung1999, Pintelon2012}. This gives
\begin{align}
\var{\hat{G}(j\omega)} &\approx \frac{\partial G^H(j\omega)}{\partial \theta}\IF^{-1}\frac{\partial G(j\omega)}{\partial \theta}.
\label{eq:excitation:varG}
\end{align}

Typically, the input design formulation depends on the true system parameters. This means that the optimal input depends on the \emph{unknown} system that one wants to identify. 
Furthermore, classical input designs, such as the D-optimal design, typically give signals with sparse spectra. 
This means that only a few frequencies are excited. 
A sparse spectrum is bad from a robustness point of view; there is very limited information about what happens at the unexcited frequencies. 
Therefore, a signal design to be optimal for one system may result in very bad estimates if applied to another system.
To increase the robustness of the design, one could take a small fraction $\delta$, from the optimal power and redistribute this power over the whole frequency band of interest.
From \eqref{eq:excitation:IF} and \eqref{eq:excitation:varG} we see that scaling the input power by $1-\delta$, gives the transfer function variance
\begin{align}
\var{\hat{G}(j\omega)}\frac{1}{1-\delta}.
\end{align}
This is a small loss while the increase in robustness can be significant if the added signal is robust. Such a robust signal is studied in the following parts of the chapter.

\section{Logarithmic power distributions}
\label{sec:excitation:logarithmic}
When no information about the system is known, it is desirable that the excitation signal is such that regardless of the true system characteristics, the resulting estimates have equally good statistical properties.
This can be ensured by a suitable choice of the input power spectrum.

 \subsection{A second order system}
Consider the second order system
\begin{align}
y(t) 
  &= 
  G(s,\theta) u(t) 
  = \frac{1}
                {\frac{s^2}{\wn^2} + 2\frac{\damping}{\wn}s + 1}
      u(t),
\label{eq:excitation:2ndOrderSys}
\end{align}
with parameters $\theta=[\wn\;\damping]^{\T}$. 
The resonance frequency of the system is $\wR \isdef \sqrt{1-\damping^2}\wn$.
  
When $\damping < \frac{1}{\sqrt{2}}$, such a second order system exhibits a peak in the amplitude of the transfer function~\citep{Oppenheim1983}:
  \begin{align}
   \left| G\left(\wM \right) \right| & \isdef
          \max_{\omega} \left| G\left( \omega \right)  \right|
        = \frac{1}
               {2 \damping \sqrt{1-\damping^2}}
               \label{eq:excitation:peakAmplitude}\\
   \wM &
        = \sqrt{1-2\damping^2} \wn
        \label{eq:excitation:peakFrequency}
    \text{.}
  \end{align}
  The $3\unit{dB}$ bandwidth around this resonance peak is given by $\wdB = 2\damping\wn$ for systems that have a sufficiently low damping.

Since information is local in the frequency domain, the input signal power inside $\wdB$, contributes significantly to the information matrix while input power outside $\wdB$ has a smaller contribution. This can be seen by considering the optimal design for a second order system. From \citep[Example 6.4.5]{Goodwin1977} we have that the D-optimal design for \eqref{eq:excitation:2ndOrderSys} is a single sinusoid at frequency
\begin{align}
\omega^\star =\frac%
{\wn
  \sqrt{
    \left(1-2\damping^2\right)
    +
    \sqrt{\left(1-2\damping^2\right)^2+15}
    }
  }
{\sqrt{5}}.
\label{eq:excitation:optFreq}
\end{align}
For lightly-damped systems, i.e $\damping \ll 1$ , $\omega^\star\approx \wn$, which is inside the bandwidth of the system. 
From \eqref{eq:excitation:optFreq} it is clear that the optimal design depends on the resonance of the system to be identified. 
Input design can be based on a guess of $\wn$, however if this guess is wrong, the resulting estimates may be unacceptable~\citep{Rojas2007}.

When no information about the system is available, we propose using an input power spectrum with a logarithmic power distribution. This can be achieved either with a continuous band-limited $1/f$ spectrum or a quasi-logarithmic multisine spectrum \citep{Pintelon2012}. 
We choose to work with an equidistant frequency grid with resolution $f_0$ and design the input in the frequency band $[\wn \kMin\; \wn \kMax]$, where the resolution is $k_0=1/\Tm$ and $\Tm$ is the measurement time. 
Hence, a finite measurement time limits the frequency resolution and therefore we have to put a lower limit on the frequency band of interest. Moreover, we can only impose the logarithmic distribution sufficiently well if $\kMin$ is large enough.

First, we look at the band-limited $1/f$ power spectrum given by
\begin{align*}
\label{eq:excitation:pink}
\phi_u(\omega) &=\begin{cases}\frac{1}{|\omega|} &\text{if $|\omega|\in[\wn \kMin\; \wn \kMax]$,}\\
                        0           &\text{otherwise}.
            \end{cases}
\end{align*}
With this input spectrum, the power inside $\wdB \approx \ClosedInterval{(1-\damping)\wn, (1+\damping)\wn}$ is
\begin{align}
  \frac{1}{2\pi}
    \int\limits_{(1-\damping)\wn}^{(1+\damping)\wn}
     \frac{1}{\omega} \dd{\omega}
  = \frac{1}{2\pi}
    \ln{\frac{1+\damping}{1-\damping}},
\end{align}
which is independent of $\wn$. The robustness of the $1/f$ input has been noted previously by e.g. \citet{Rojas2007,Rojas2012,Goodwin2006GBO}.

Alternatively, one could also construct a signal with power spectrum concentrated at discrete frequency lines.
Such signals are elaborated in \secref{sec:excitation:multisine}.

% Second, we consider the power spectrum of the quasi-logarithmic multisine signal [\cite{Pintelon2012}] given by
% \begin{align}
%   \phi_u(\omega) &= \frac{1}{M}\sum_{k=1}^{M}{\dirac{\omega - \omega_k}},
% \end{align}
% where $\omega_{k}/\omega_{k+1} \approx \alpha$ and $\dirac{\omega}$ denotes the Dirac distribution.
% Hence, the number of spectral lines inside \wdB is 
% \begin{align}
%   \floor{
%     \log_{\alpha} \frac{1+\damping}{1-\damping} 
%   }
%   \text{,}
% \end{align}
% where $\floor{\placeholder}$ is the nearest integer rounded downwards. 
% The number of spectral lines is independent of $\wn$. 

% More generally, a multisine can be defined as:
% \begin{equation}
%  u \left[ n\right] = \frac{1}{\sqrt{F}}
%    \sum_{k=1}^{F} 
%      A_k 
%      \sin 
%        \left(\frac{2\pi n k f_0}{f_s} + \psi_k \right)
%   \label{eq:excitation:MS}
% \end{equation}
% where $A_k$ and $\psi_k$ are its amplitude and phase spectrum; $f_s$ is the
% sampling frequency and $f_0 = \frac{f_s}{N}$ the base frequency with $N$ the 
% number of points in a period. 

% To construct a quasi-logarithmic multisine, only frequency lines $f_j$ within the set of available sine components $\left\{f_0, 2 f_0, \ldots, F f_0  \right\}$ are excited for which the ratio of two consecutive frequencies $f_j$ and $f_{j-1}$ is approximately constant: $\frac{f_{j-1}}{f_j} \approx \alpha$.

The result of these two choices of input power spectrum is that for a given damping, $\var{\hat{G}}$ is approximately the same independent of $\wn$. 
This is illustrated in \secref{sec:excitation:simulation}.

\subsection{Sums of second order systems}
This section extends the previous section to more general systems than second order.
Consider a system
\begin{align}
G(s,\theta) &= \sum_{i=1}^{K} G_i(s,\theta_i)
             =\sum_{i=1}^{K}  \frac{1}
                                   {   \frac{s^2}{\wn[i]^2}
                                    + 2\frac{\damping[i]}{\wn[i]}s
                                    + 1},\\
\theta_i&=[\wn[i]\;\damping[i]]^{\T}.
\label{eq:excitation:sumSys}
\end{align}
The systems  $G_i$ will be called the \emph{sub-systems} of $G$. 
Without loss of generality, we assume that $\wn[1] < \wn[2] < \cdots < \wn[K]$. 
To properly identify the system $G$, excitation is required such that all subsystems are properly excited, regardless of their individual resonance frequencies.

The information matrix for systems of the type \eqref{eq:excitation:sumSys} will have a block structure commensurate with the dimension of $\theta_i$ given by
\begin{align}
\IF &= \frac{1}{\pi}
\begin{bmatrix}
  \partial G_{1,1}        &\partial G_{1,2}       &\deemph{\cdots}         &\partial G_{1,K}\\
  \partial G_{2,1}        &\partial G_{2,2}       &\deemph{\cdots}         &\partial G_{2,K}\\
  \deemph{\vdots}                  &\deemph{\vdots}                 &\deemph{\ddots}         &\deemph{\vdots}          \\
  \partial G_{K,1}        &\partial G_{K,2}       &\deemph{\cdots}         &\partial G_{K,K}
\end{bmatrix},\\
\partial G_{i,j} &= \int_0^\infty
                      \RealPart\left\{
                          \frac{\partial G_i}
                               {\partial \theta_i}
                          \frac{\partial G_j^{\HT}}
                               {\partial \theta_j}
                                \right\}
                      \phi_u(\omega)\dd\omega.
\end{align}

Furthermore, when the damping of the systems is low or the resonances of the sub-systems are well separated, the estimates decouple in the sense that the elements off-diagonal blocks become small.
The effect is that the variance of the estimated transfer function approximately becomes the sum of the variances of the sub-systems.
To illustrate this we look at a system with $K=2$, i.e. the sum of two second order systems.
The conclusions from the example are easily extended to systems consisting of more than two sub-systems.

\subsubsection{Example: Sum of two second-order systems}
\label{sec:excitation:ex:sumOfSecondOrderSipmle}
Consider the system
\begin{align}
G(s,\theta) &= G_1(s,\theta_1) + G_2(s,\theta_2)%\\
%&=\frac{1}{\frac{s^2}{\wn[1]^2} + 2\frac{\damping[1]}{\wn[1]}s + 1} + \frac{1}{\frac{s^2}{\wn[2]^2} + 2\frac{\\damping[2]}{\wn[2]}s + 1},
\end{align}
with $\theta_1=[\wn[1]\;\damping[1]]^{\T}$ and $\theta_2=[\wn[2]\;\damping[2]]^{\T}$.
The information matrix for this system is
\begin{align}
\IF &=
       \begin{bmatrix}
         \partial G_{1,1}  & \partial G_{1,2}\\
         \partial G_{2,1}  & \partial G_{2,2}
       \end{bmatrix}.
\label{eq:excitation:infoEx}
\end{align}
To investigate the decoupling, we study the $(1,1)$ elements of blocks $\partial G_{1,1}$ and $\partial G_{1,2}$ of the information matrix \eqref{eq:excitation:infoEx}, when the input spectrum \eqref{eq:excitation:pink} is used; similar arguments can be made for the other elements of the two matrix blocks.

The $(1,1)$ element of the $\partial G_{1,1}$ entry of the information matrix is
\begin{align}
  \int_0^\infty\RealPart\left\{\frac{\partial G_1}{\partial \wn[1]}\frac{\partial G_1^{\HT}}{\partial \wn[1]}\right\}\frac{1}{\omega}\dd\omega
\end{align}
which evaluates to
\begin{equation}
\frac{\pi }
          {16  \sqrt{1 - \damping^2}\wn^{2} \damping^{3}} 
+ 
\frac{ 2\arctan  
                               \left(
                                       \frac{(2  \damping^{2} - 1) \sqrt{1-\damping^{2}}}
                                                {2  (\damping^{3} - \damping)}
                              \right)}
         {16  \sqrt{1-\damping^{2}} \wn^{2}  \damping^{3}}
+
\frac{{\left(2  \damping^{2} + 1 \right)} }
         {4  \sqrt{1-\damping^{2}} \wn^{2} \damping^{2}}
\label{eq:excitation:auto:full}
\end{equation}
after lengthy calculations.
For $\damping \ll 1$ this can be simplified to
\begin{equation}
\label{eq:excitation:auto}
\int_0^\infty\RealPart\left\{\frac{\partial G_1}{\partial \wn[1]}\frac{\partial G_1^{\HT}}{\partial \wn[1]}\right\}\frac{1}{\omega}\dd\omega
 = 
 \frac{\pi}
          {8\wn[1]^2}\damping^{-3} 
+ \bigO{\damping^{-2}}
\end{equation}
since the $\arctan$ in \eqref{eq:excitation:auto:full} tends to $\frac{\pi}{2}$ for $\damping \to 0$.
For the corresponding element in $\partial G_{1,2}$ we start by bounding the absolute value of the integral by
\begin{gather}
\left|\int_0^\infty\RealPart\left\{\frac{\partial G_1}{\partial \wn[1]}\frac{\partial G_2^{\HT}}{\partial \wn[2]}\right\}\frac{1}{\omega}\dd\omega\right|
\qquad \qquad \qquad \qquad \qquad
\label{eq:excitation:cross1} \\
\leq
\int_0^\infty\left|\RealPart\left\{\frac{\partial G_1}{\partial \wn[1]}\frac{\partial G_2^{\HT}}{\partial \wn[2]}\right\}\right|\frac{1}{\omega}\dd\omega
\leq \\
\qquad \qquad \qquad \qquad \qquad
\int_0^\infty\left|\frac{\partial G_1}{\partial \wn[1]}\frac{\partial G_2^{\HT}}{\partial \wn[2]}\right|\frac{1}{\omega}\dd\omega \text{.}\label{eq:excitation:cross}
\end{gather}
To proceed, consider
\begin{align}
\frac{\partial G_1}{\partial \wn[1]} 
  &= \frac{2\damping j\omega\left[1+\frac{j\omega}{\wn[1]\damping}\right]}
  {\wn[1]^2\left[1+2\frac{\damping j\omega}{\wn[1]} + \left(\frac{j\omega}{\wn[1]}\right)^2\right]^2},
\end{align}
which has zeros in $j\omega=\left\{0, \damping\wn[1]\right\}$. 
Hence $\left|\frac{\partial G_1}{\partial \wn[1]} \right|$ has asymptotic slope 20\unit{dB/\decade} for low frequencies, at $j\omega=\damping\wn[1]$ the slope increases to 40\unit{dB/\decade}. 
There are four poles which give a $-40$\unit{dB/\decade} roll-off after the peak. 
The shape of $\frac{\partial G_1}{\partial \wn[1]} $ is illustrated in \figref{fig:excitation:derivatives}.

A crude, albeit useful, bound on the integrand in \eqref{eq:excitation:cross} is
\begin{align}
&f\left(\omega,\theta_1,\theta_2\right)=
  \begin{cases}
   \frac{\omega^2}{\wn[1]^2}
   \frac{1}{\omega}
   \partial G_1^\infty
                                       & \text{if $\omega \leq \wn[1]$,}\\
   \phantom{\frac{\omega^2}{\omega^2}}
   \frac{1}{\omega}
   \partial G_1^\infty
                                       & \text{if $\wn[1] < \omega < \wn[2]$,}\\
   \frac{\wn[2]^2}{\omega^2}
   \frac{1}{\omega}
   \partial G_1^\infty
                                       & \text{if $\omega \geq \wn[2]$,}
  \end{cases}
\end{align}
where $\partial G_1^\infty = \left\|\frac{\partial G_1}{\partial \wn[1]}\right\|_\infty$. 
Again, direct calculations give
\begin{align}
\int_0^\infty f\left(\omega,\theta_1,\theta_2\right)\dd\omega = \frac{\wn[1]\left(2-\ln{\frac{\wn[1]^2}{\wn[2]^2}}\right)}{\wn[2]^2}\damping^{-2} + \mathcal{O}(1).
\end{align}
Hence \eqref{eq:excitation:cross1} cannot grow faster than $\mathcal{O}(\damping^{-2})$ as $\damping\rightarrow0$. From this analysis we conclude that:
\begin{itemize}
\item the integral \eqref{eq:excitation:cross1} decreases as $\wn[2]-\wn[1]$ increases, and
\item lower damping further decreases the integral \eqref{eq:excitation:cross1} in comparison to \eqref{eq:excitation:auto}.
\end{itemize}

To further illustrate the decoupling of the two systems, we calculate \eqref{eq:excitation:infoEx} for different values of the damping $\damping$ and separations of the two resonances $\wn[1]$ and $\wn[2]$. If the separation is taken in terms of $\wdB$-units for the lower resonance, the result only depends on the difference $\wn[1]-\wn[2]$ and not on the actual frequencies. The resulting information matrix is then normalized so that the largest element is $1$.

In \figref{fig:excitation:coupling} the largest element in $\partial G_{1,2}$ is plotted for different dampings and separations. It is clear that highly resonant systems have a large degree of decoupling and that resonances that are well-separated increases the decoupling. 
The diagonal blocks of the normalized $\IF$ in all cases have some elements close to $1$. 
In this example, the $\damping$ is set to be the same in both sub-systems. 
Using different dampings in the sub-systems would, however, not change the argument, the systems with the lowest damping will limit the separation.

Based on the above arguments, we approximate the information matrix for lightly-damped systems as a block-diagonal matrix:
\begin{align}
\IF&\approx \begin{bmatrix}\partial G_{1,1} &\deemph{0}\\
                        \deemph{0}&\partial G_{2,2}\end{bmatrix}.
\end{align}
Hence the variance of the estimate $\hat{G}(s,\theta)$ becomes
\begin{align}
\var{\hat{G}(\omega)} &\approx \frac{\partial G^{\HT}(e^{j\omega})}{\partial \theta}\begin{bmatrix}\partial G_{1,1} &\deemph{0}\\
                        \deemph{0}&\partial G_{2,2}\end{bmatrix}^{-1}\frac{\partial G(e^{j\omega})}{\partial \theta}\\
&=\var{\hat{G}_1(\omega)} + \var{\hat{G}_2(\omega)}.
\end{align}

This results extends \latin{mutatis mutandis} to systems consisting of more sub-systems.

\begin{figure}
\centering
\setlength\figurewidth{0.68\columnwidth}
\setlength\figureheight{0.68\figurewidth}
\input{\thisDir/figs/coupling.tikz}
\caption[Relative amplitudes of the off-diagonal blocks of the Fisher information matrix.]{The element with the largest absolute value in the off-diagonal blocks of the normalized information matrix for different dampings.
         The different plots are for different separations of the resonance frequencies, from $4\wdB$ to $64\wdB$.
         The arrow indicates the direction of increasing separation.}
\label{fig:excitation:coupling}
\end{figure}

\section{System parametrization} 
\label{sec:excitation:parametrization}

  We consider the transfer function models of proper \gls{LTI} systems of arbitrary but finite orders $\order{B} / \order{A}$:
  \begin{align}
    G\left( \Omega, \theta \right) 
    = \frac{B\left( \Omega,\theta \right)}
                  {A\left( \Omega,\theta \right)}
    = \frac{\sum\limits_{i=0}^{\order{B}} b_i \Omega^i}
                  {\sum\limits_{i=0}^{\order{A}} a_i \Omega^i}
    \text{.} 
    \label{eq:excitation:model}
  \end{align}
  
The model is parametrized by the vector $\theta = \left[b_0 \,\cdots\, b_{n_b-1} \; a_0 \,\cdots\, a_{n_a-1} \right]^{\T}$ and  is evaluated at different frequencies $\Omega$ in the complex plane.
For a continuous time model $\Omega = s$, while for the discrete time case $\Omega = z$.
For the sake of notational simplicity, $\Omega = s$ will be used further on, but the results are similar for $\Omega=z$.
Remember that the rational transfer function of any strictly proper \gls{LTI} system $G$ can be decomposed in its partial fraction form~\citep[Appendix]{Oppenheim1996}:
\begin{align}
  G(s) &= 
    \frac{\sum\limits_{i=0}^{\numel{B}} b_i s^i}
              {\sum\limits_{i=0}^{\numel{A}} a_i s^i} 
        =    
   \frac{\sum\limits_{i=0}^{\numel{B}} b_i s^i}
              {\prod\limits_{i=1}^{I_1} \left( s - \pole[i] \right)^{M_{1,i}} 
               \prod\limits_{i=1}^{I_2} \left(s^2 + 2 \damping[i] \wn[i] s + \wn[i]^2 \right)^{M_{2,i}}}
        \\
        &  = 
        \sum_{i=1}^{I_1} 
          \sum_{\multiplicity=1}^{M_{1,i}} 
            \frac{\dc[i]}
                      { \left( s - \pole[i] \right)^{\multiplicity}}
            + 
        \sum_{i=1}^{I_2} 
          \sum_{\multiplicity=1}^{M_{2,i}}
           \frac{\dc[i,\multiplicity] \wn[i]^{2\multiplicity} \left( s + \wz[i] \right)}
                     {\wz[i] \left(s^2 + 2 \damping[i] \wn[i] s + \wn[i]^2 \right)^{\multiplicity}}
        \label{eq:excitation:PFE}
\end{align}
where $I_1$ denotes the number of real-valued poles, $I_2$ denotes the number of complex conjugated pole pairs.
$M_{1,i}$ and $M_{2,i}$ denotes the multiplicity of the corresponding poles, such that $\numel{A} = \sum_i^{I_1} M_{1,i} + 2 \sum_{i}^{I_2} M_{2,i}$.
Each term in \eqref{eq:excitation:PFE} represents a \term{sub-system} of the system $G(s)$.
  Within these sub-systems, we can distinguish first-order subsystems and second-order subsystems, the latter of which describe resonant phenomena.
  Since such resonances are often a limiting factor, we first focus on these resonant sub-systems.
\begin{remark}
  For non-proper systems, one can write the transfer function $G_{\mathrm{non-proper}}(s) = P(s) + G_{\mathrm{proper}}$ where $P(s)$ is a polynomial and $G_{\mathrm{proper}}(s)$ a proper rational function.
  As such, the polynomial $P(s)$ should be added to equation \eqref{eq:excitation:PFE} for non-proper systems.
\end{remark}

Consider the second-order subsystem with multiplicity $\multiplicity$:
\begin{equation}
  G_2(s, \theta) = \frac{\dc \wn^{2 \multiplicity} \left( s + \wz \right)}{\wz \left(s^2 + 2 \damping \wn s + \wn^2 \right)^{\multiplicity}}
\label{eq:excitation:secondOrderSubsystem}
\end{equation}
parametrized in $\theta =  \left[K, \wn, \wz, \damping \right]^{\T}$.
To determine the Fisher Information matrix, the derivative $\PartialDerivative{G(s,\theta)}{\theta}$ is required; its components are given by:
\begin{align}
  \PartialDerivative{G_2}{\dc} 
    &= 
    \frac{G_2}{\dc}
    =
  \frac{\wn^{2\multiplicity}}{\wz}
  \frac{s + \wz}{\left(s^2 + 2 \damping \wn s + \wn^2\right)^{\multiplicity}}
  \\
  \PartialDerivative{G_2}{\wz} 
    &= 
    \frac{-\dc \wn^{2\multiplicity}}
              {\wz} 
    \frac{s}
              {\left(s^2 + 2 \damping \wn s + \wn^2\right)^{\multiplicity}} 
  \\
  \PartialDerivative{G_2}{\wn} 
  &= 
  \frac{2 \dc \wn^{2\multiplicity-1} \multiplicity}
            {\wz} 
  \frac{s \left( s + \wz \right) \left( s+ \damping \wn \right)}
            {\left( s^2 + 2 \damping \wn s + \wn^2 \right)^{\multiplicity + 1}}
  \\
  \PartialDerivative{G_2}{\damping}
  &= 
  \frac{-2 \dc \wn^{2\multiplicity + 1} \multiplicity}
            {\wz}
  \frac{s \left( s + \wz \right)}
             {\left( s^2 + 2 \damping \wn s + \wn^2 \right)^{\multiplicity + 1}}
  \text{.}
\end{align}

The poles of these derivatives coincide with the system poles (possibly with increased multiplicity).
Consequently, this means that for lightly-damped systems ($\damping \ll \sqrt{2}$), these derivatives exhibit a sharp resonance peak at the same frequency as the actual sub-system $G_2$, see also \figref{fig:excitation:derivatives}.
The roll-off near the resonance is determined mainly by the damping $\damping$ and the multiplicity $\multiplicity$ of the sub-system.
The relationship between the frequency of the zero $\wz$ and the resonance frequency $\wn$ alters the shape of these derivatives, but has little impact in the vicinity of the resonance peak.
As such, the Fisher information matrix is pre-dominantly determined by the input power spectrum near $\wn$.
Note that $\PartialDerivative{G_2}{\wn}$ and $\PartialDerivative{G_2}{\damping}$ have the steepest roll-off near the resonance.
As such, those parameters are the most important for the experiment design.

\begin{figure}
  \centering
  \setlength\figurewidth{0.9\columnwidth}
  \setlength\figureheight{\figurewidth}
  \input{\thisDir/figs/derivs.tikz}
  \caption[Derivatives of a second-order sub-system towards each of its parameters.]{Bode plots of the elements of $\PartialDerivative{G_2}{\theta}$ for small $\damping$. 
  % The numbers indicate the (asymptotic) slope as a multiple of $20 \unit{dB/\decade}$.
  The roll-off near $\wn$ is significant and depends strongly on $\damping$.
  It can be seen that the derivatives are dominated by the resonance peak, an effect that is even more prominent when $\multiplicity$ is large.
  This means that for a good input signal, it is critical to have power near the resonance peak. 
  Note also that $\PartialDerivative{G_2}{\wn}$ and $\PartialDerivative{G_2}{\damping}$ exhibit a sharper peak and hence $\damping$ and $\wn$ are the most important parameters of the model.
  In this example, $\damping = \varXi$, $K = \varK$, $\frac{\wz}{\wn} \in \Set{\varWzSmall, 1, \varWzBig}$, $\multiplicity=1$ and $\wn = \varWn \unit{rad/s}$ were used.
}
\label{fig:excitation:derivatives}
\end{figure}

Moreover, the results from \secref{sec:excitation:ex:sumOfSecondOrderSipmle} for the sum of simplified second order systems, i.e. equation \eqref{eq:excitation:sumSys}, extend directly to the generic \gls{LTI} system in \eqref{eq:excitation:PFE} for the second-order sub-systems under the assumptions given below.

\begin{assumption}
The sub-systems in \eqref{eq:excitation:PFE} all are either lightly damped ($\damping \ll \sqrt{1/2}$) or are not of interest to the user.
\end{assumption}

\begin{assumption}
The resonance frequencies $\wn[i]$ and $\wn[j]$ of all the different sub-systems in \eqref{eq:excitation:PFE} are well-separated in the frequency domain.
\end{assumption}

Under these assumptions, the Fisher information matrix will be approximately a diagonal block matrix, where the $i^{\text{th}}$ block corresponds to the parameters $\left[\wn[i], \wz[i], \damping[i], \dc[i,1], \ldots, \dc[i,M_{2,i}] \right]$ of a particular sub-system.
Using the same logic as in \secref{sec:excitation:ex:sumOfSecondOrderSipmle}, this means that the variance of the complete system is approximately equal to the variance obtained for each sub-system, due to the decoupling of the Fisher information matrix.

\section{Multisine Excitations}
\label{sec:excitation:multisine}
In contrast to signals with a continuous $1/f$ \gls{PSD} presented in \secref{sec:excitation:logarithmic}, similar results can be obtained by using signals that have a discrete frequency grid.
Such signals are presented in this section.

  To excite the system under test we consider a generalization of multisine excitations \citep{Pintelon2012}:
  \begin{definition}[Generalized multisine] \label{def:excitation:generalized-MS}
  A generalized multisine $u(t)$ is a signal consisting of $F$ sine waves with a different frequencies $f_k$, amplitudes $A_k$ and phase shifts $\phi_k$:
  \begin{equation}
    u \left( t\right) = \frac{1}{\sqrt{F}}
   \sum_{k=1}^{F} 
     A_k 
     \sin 
       \left(2\pi f_k t + \phi_k \right)
  \text{.}
  \label{eq:excitation:MultisineCT}
  \end{equation}
  Without loss of generality, one can stipulate that the frequency grid is sorted, i.e. $f_1 < f_2 < \ldots < f_F$.
  \end{definition}

  For \gls{LTI} systems, it is well-known that the information matrix only depends upon the amplitude spectrum of the input and not upon its phase spectrum as indicated in \eqref{eq:excitation:IF}.
  As such, the results presented in this chapter also do not depend on the particular choice of the phase spectrum.
  This means a user is free to make an appropriate choice for their application.
  
  In this chapter, random-phase multisines are used since this is a reasonable choice for systems that are dominantly linear~\citep{Schoukens2004}.
  Hence, $\phi_k$ is the outcome of a uniform random process over $\left[0,2\pi\right[$.
  This leaves two sets of free parameters to create a signal that suits our needs:
  \begin{itemize}
    \item the amplitude of the spectral lines $A_k$, and
    \item the frequency grid $\left\{f_1, \ldots, f_F \right\}$.
  \end{itemize}
  Note that the $k^{\text{th}}$ term in \eqref{eq:excitation:MultisineCT} is periodic with period $T_k \isdef f_k^{-1}$.

  Consequently, the complete multisine has a period $T_{u(t)}$ that is the (least) common multiple of the period of its components:
  \begin{equation}
    T_{u(t)}  = \lcm \set{T_1, \ldots, T_F}
      = \lcm \set{f_1^{-1}, \ldots, f_F^{-1}}
  \end{equation}
  if a common multiple exists, i.e. $T_{u(t)} = n_1 T_1 = \ldots = n_F T_F$ with $n_i \in \NaturalNumbersWithoutZero$ for $i \in \set{1,\ldots,F}$.

  \begin{example}
   Consider a generalized multisine where $f_1 = 1 \unit{Hz}$ and $f_2 = \tfrac{2}{3} \unit{Hz}$.
   The corresponding periods are $T_1 = f_1^{-1} = 1 \unit{s}$ and $T_2 = f_2^{-1} = 1.5 \unit{s}$.
   The overall period of the multisine is $T_{u(t)} = n_1 T_1 = n_2 T_2 = 3 \unit{s}$, i.e. $n_1 = 3$ and $n_2=2$.
  \end{example}

  \begin{example}\label{eg:excitation:non-periodic}
   Consider the generalized multisine with $f_1 = 1 \unit{Hz}$, $f_2 = \pi \unit{Hz}$, and $f_3 = \pi^2 \unit{Hz}$.
   The corresponding periods are then $T_1 = 1 \unit{s}$, $T_2 = \pi^{-1} \unit{s}$, and $T_3 = \pi^{-2} \unit{s}$.
   However, a common multiple of those periods cannot be found since there exist no positive integers $n_1$, $n_2$, and $n_3$ that ensure that $T_{u(t)} = n_1 T_1 = n_2 T_2 = n_3 T_3$.
   As such, this generalized `multisine' is not periodic.
  \end{example}

  This last example illustrates that a least common multiple of real numbers is not guaranteed to exist.
  Consequently, the generalized multisine of \defref{def:excitation:generalized-MS} is not guaranteed to be periodic.
  
\subsection{Equidistant (Linear) Grid Multisine}
  In many applications, an equidistant frequency grid is used for $u\left( t\right) $ because this fits best with the classical \gls{DFT} analysis \citep{OppenheimDT,Mandal2007}.
  It allows to analyze periodic signals on an equidistant frequency grid very efficiently.
  For such a frequency grid, it is guaranteed that the signal is periodic in the time domain, unlike for the generalized multisines of the previous section.
  Moreover, such a signal can be created easily using a waveform generator that has a constant sampling frequency $f_s$.
  
  An equidistant frequency grid $\set{f_1, \ldots, f_F}$ with spacing $\Delta f$
  consisting of $F$ lines conforms to the following relation:
  \begin{equation}
    f_k = k \Delta f + k_0 \Delta f
    \emspace \forall k \in \set{1,\ldots,F},
    \emspace k_0 \in \NN
    \text{.}
  \label{eq:excitation:linGrid}
  \end{equation}
  
  Using such linear grid, we can easily construct a representation of a multisine
  in discrete time that is suited for use with the \gls{DFT} by sampling at a given sampling rate $f_s$:
  \begin{equation}
     u \left[ n\right] = \frac{1}{\sqrt{F}}
     \sum_{k=1}^{F} 
       A_k 
       \sin 
         \left(\frac{2\pi n \left( k + k_0 \right) \Delta f}{f_s} + \phi_k \right)
    \label{eq:excitation:MultiSineDT}
  \end{equation}
  
  Such an equidistant grid is a very common choice.
  Note that it has more frequency lines per decade (or octave) at higher frequencies.
  Therefore, the excitation power per octave is larger at higher frequencies
  when a constant amplitude spectrum ($A_k = A_{\mathrm{in}}$) is used.
  This is a rather common choice in multisine excitations when no other constraints are taken into account.
  For a constant amplitude spectrum, this means that a model of the low-frequency dynamics is typically plagued by a higher uncertainty than for models of high-frequency dynamics.
  This is caused by the fact that the lower frequency bands receive less signal power than the high frequency bands.
  
  Therefore a signal with an equidistant frequency grid is undesirable for
  our situation as it wastes signal power at high frequencies at the cost of an increased variance in the lower side of the band.
  
\subsection{(Ideal) Logarithmic Grid Multisine}
  For a logarithmic frequency grid $\set{f_1, \ldots, f_F}$ of $F$ lines, the following relation holds between excited frequencies:
  \begin{equation}
    f_k = \alpha \cdot f_{k-1}
    \emspace \forall k \in \set{1,\ldots,F}
  \label{eq:excitation:logGrid}
  \end{equation}
  for a given lowest frequency $f_0$ and frequency ratio $\alpha > 1$.
  Note that for $\alpha \in \NaturalNumbers$, such a frequency grid is a subset of a linear frequency grid and hence the multisine is periodic.
  However, when $\alpha \not\in \NaturalNumbers$, the signal is not guaranteed to be periodic (see \egref{eg:excitation:non-periodic}).

  When designing an excitation signal, it is common only to specify a (large) frequency band by its boundary frequencies $f_{\min}$ and $f_{\max}$.
  For a given $\alpha$ it is easy to determine the number of excited lines within the frequency range of interest:
    \begin{equation}
      F = \floor{
                  \frac{\log{f_{\max}} - \log{f_{\min}}}
                       {\log{\alpha}}
                }
    \label{eq:excitation:logms:F}
    \end{equation}
  where $\floor{\bullet}$ denotes rounding towards the nearest smaller integer value.
  In the Section \ref{sec:excitation:optimAlpha}, the choice of a suitable $\alpha$ is elaborated.

  \begin{property}\label{prop:excitation:log:relBW}
   Each frequency band $B = \ClosedInterval{f, \kappa f}$ with $\kappa \in \PositiveRealNumbers \without \set{0,1}$ that is completely within the range of the excited logarithmic grid of frequencies $\set{f_1, \ldots, f_F}$ contains a constant number of frequency lines
   \begin{equation}
     F_{B} \isdef
     \floor{\frac{\log (\kappa f) - \log( f )}{\log \alpha}}
         = \floor{\log_{\alpha} \kappa}
         \text{.}
   \end{equation}
   \end{property}
  \begin{example}
    For the frequency decade band $\decade(f) \isdef \ClosedInterval{f, 10 f}$, a logarithmic frequency grid contains $F_{\decade} = \floor{\log_{\alpha} 10}$ excited lines.
    Note that this is independent of the frequency $f$.
  \end{example}
  \begin{remark}
    In different contexts, e.g. electrical circuit simulators such as \gls{SPICE}~\citep{Kundert1995}, measurement equipment such as \glspl{DSA} and programming languages, it is more common to specify the spacing of a logarithmic grid using the total number of frequencies ($F$) or the number of frequencies per decade ($F_{\decade}$) instead of the grid ratio $\alpha$.
    Obviously, such alternative specifications are equivalent.
  \end{remark}
  \begin{example} \label{eg:excitation:logarithmicGrid:matlab}
    The \MATLAB code \mcode[mathescape]{f = logspace(log10($f_{\min}$), log10($f_{\max}$), $F$)} produces a logarithmically spaced frequency vector \mcode{f} with $F$ elements in the range $\left[ f_{\min}, f_{\max} \right]$.
    As such, the effective frequency ratio $\alpha = \sqrt[F]{\frac{f_{\max}}{f_{\min}}}$ in this vector.
  \end{example}
  \begin{example}
    The~\citet{HP3562A} has a logarithmic measurement mode that ensures that each decade is covered by $F_{\decade} = 80$ lines~ to measure an \gls{FRF}.
    This is equivalent to a grid spacing $\alpha = \sqrt[80]{10} \approx 1.029$.
  \end{example}

   From \propref{prop:excitation:log:relBW}, it can be seen that for any set of frequency bands with a constant relative bandwidth (e.g. decades~($\kappa=10$), octaves~($\kappa=2$),\ldots ), a logarithmic  frequency grid contains a constant number of lines per frequency band.
   Consequently, the power in each relative bandwidth is identical when a constant amplitude spectrum ($A_k = A_{\mathrm{in}}$) is used.   

  To return to the \gls{FRF}, in \secref{sec:excitation:optimAlpha} we will show that for a dense logarithmic frequency grid, any second order system with damping $\damping$ and a resonance within the bulk of the frequency grid, will receive the same number of frequency lines in its $3 \unit{dB}$ bandwidth regardless of the actual value of the resonance frequency.
  As most of the information of such systems is obtained from the measurements at frequencies inside the $3 \unit{dB}$
  bandwidth, one expects an equal variance for each system, as will be shown in \secref{sec:excitation:bestFrequencyResolution}.
  
  For that reason, a logarithmic multisine is a good candidate for our purposes.
  However, it is more involved both to generate and to analyze such a signal, as the excited frequencies do not lie on a commensurate frequency grid.
  Especially for periodic measurements, this is a major downside since the period of such signal does not always exist (i.e. this leads to infinite measurement times).
  It is therefore not practical to use a perfectly logarithmically spaced frequency grid as the \gls{DFT} or \gls{CZT}~\citep{Rabiner2004} explicitly rely on an equidistantly-spaced frequency grid.
  
\subsection{Quasi-Logarithmic Grid Multisine}
  Since most measurement equipment and analysis methods rely on signals with an equidistant frequency grid, we create a frequency grid
  $\set{f_1, f_2, f_3,\ldots, f_F}$ for which \eqref{eq:excitation:logGrid} is approximately valid but with the strict constraint that the frequencies must be a subset  of an equidistant frequency grid. 
  This yields the following relation for  subsequent frequency lines:
  \begin{equation}
    f_k \approx \alpha \cdot f_{k-1}
    \emspace \forall k \in \set{1,\ldots,F}
  \label{eq:excitation:qlogGrid}
  \end{equation}
  under the constraint that
  \begin{equation}
    f_k = N_k \cdot \Delta f,\; N_k \in \NN
    \text{.}
    \label{eq:excitation:constraintLinGrid}
  \end{equation}
  Such a grid is called a quasi-logarithmic (quasi-log) grid~\citep{Pintelon2012}. 

  \begin{remark}
  When $\alpha \not\in \NaturalNumbers$, it is possible that at the low end of the frequency band, $\Delta f > \left(\alpha -1 \right) f_{k-1}$ for some integer values of $k$.
  In such cases, the commensurate frequency grid is not dense enough to realize a frequency ratio $\alpha$.
  Consequently, the power spectrum at these low frequencies will be less than the one of an ideal logarithmic frequency grid.
  \end{remark}

  \begin{definition} \label{def:excitation:bulk}
  We denote the `\term{bulk}' of the quasi-logarithmic frequency grid as the frequency region where $\frac{f_k}{f_{k-1}} \approx \alpha$ can be attained.
  A necessary condition for $f_k$ to be part of the bulk is that $\Delta f \leq \left(\alpha -1 \right) f_{k-1}$.
  Equivalently, the bulk can be seen as the frequency interval $\RightOpenInterval{\frac{\Delta f}{\alpha - 1} , +\infty}$.
  \end{definition}

  \begin{definition}
  The effective frequency ratio $\alpha_k$ of a frequency grid is
  \begin{equation}
    \alpha_k \isdef  \frac{f_k}{f_{k-1}}
    \text{.}
  \end{equation}
  \end{definition}

  \begin{definition}
  For the implementation of a quasi-logarithmic frequency grid,  the approximate equality \eqref{eq:excitation:qlogGrid} does not define the grid unambiguously.
  The quasi-logarithmic grids depicted in this chapter are constructed using the relationship
  \begin{align}
       \label{eq:excitation:qlog:gridDef}
    f_k & = \alpha_k(n_k) f_{k-1}  
        \quad
            \forall k \in \set{1,\ldots,F}
\end{align}
with
\begin{align}
     \label{eq:excitation:qlog:roundAlpha}
      \alpha_k(N_k) & \isdef 
          \round{   \frac{\alpha^{n_k} f_{k-1}}{\Delta f}}    
           \frac{\Delta f}{f_{k-1}}
      \approx \alpha \text{, and}\\
         \label{eq:excitation:qlog:alphaPowerDef}
          n_k & \isdef 
            \min 
            \set{n | n \in \NaturalNumbers, \alpha_k(n) > 1}
     \text{.}
  \end{align}
  \end{definition}
  In the equation above, $\round{\placeholder}$ denotes rounding towards the nearest integer according to the \IEEEfloat standards and as implemented in the \MATLAB function \mcode{round}.
  This satisfies the approximate definition.
  
  \begin{remark}
  The use of different rounding strategies --- e.g. the \mcode{ceil} function $\ceil{}$  and \mcode{floor} function $\floor{}$ --- in \eqref{eq:excitation:qlog:roundAlpha}, allows for slightly different guarantees on the effective $\alpha_k$ attained by the grid.
  By denoting $\alpha_k^{\placeholder}$ as the effective frequency ratio for a rounding strategy $\placeholder$  in that equation, one can trivially see that the following relationships hold:
  \begin{equation}
  \alpha_k^{\floor{}} \leq \alpha \leq  \alpha_k^{\ceil{}}
  \qquad \text{and} \qquad
  \alpha_k^{\floor{}} \leq \alpha_k^{\round{}} \leq  \alpha_k^{\ceil{}}
  \qquad
  \forall k \in \NaturalNumbersWithoutZero
  \text{.}
  \end{equation}
  This means that rounding using the \mcode{floor} operator favors to strictly attain the specified frequency bin density at the cost of a few more excited lines (and hence less allottable power per line).
  The \mcode{ceil} function produces a grid that has more power per line at the cost of a reduced bin density.
  On the other hand, using \mcode{round} as in \eqref{eq:excitation:qlog:roundAlpha}, provides a grid that is somewhere in-between both extremes.
  For this reason, the grid constructed using $\alpha_k^{\round{}}$, i.e. by using the normal \mcode{round} function, has been used in the remainder of this chapter.
  \end{remark}

  \begin{example} \label{eg:excitation:quasilogarithmicGrid:matlab}
  An easy way to produce a quasi-logarithmic grid in \MATLAB is to first produce the logarithmic grid \code{fLog} as in  \egref{eg:excitation:logarithmicGrid:matlab} and reducing it to a quasi-logarithmic grid with resolution $\Delta f$ using \mcode[mathescape]{f = unique(round(fLog/$\Delta f$))*$\Delta f$}.
  Note that this is a computationally efficient way since both \mcode{logspace} and \mcode{unique} require at most $\bigO{ F }$ time, where \mcode[mathescape]{$F\;$ = numel(fLog)}.
  \end{example}

  As this signal approximates a logarithmically spaced signal,  we expect that it will exhibit comparable properties.
  On the other hand, by imposing \eqref{eq:excitation:constraintLinGrid}, it remains
  possible to use signal techniques which rely on a commensurate frequency grid.

\subsection{Compensated Quasi-Logarithmic Grid Multisine}
  The power density of the quasi-log multisine at the low frequencies is reduced in comparison to that of the logarithmic grid multisine.
  This is due to the limited available frequency resolution if one sticks to the \gls{DFT} frequency grid.
  To compensate for this loss in \gls{PSD}, we can increase the amplitude spectrum $A_k$ at these lines.
  Note that we are not able to restore the frequency resolution, only the power in a certain band is made equal to the power in the logarithmic multisine in the same band.

  First of all, we need to determine the factor $A_k$ needed to correct the power spectrum to match that of a log grid multisine.
  To this end, one first constructs a logarithmic frequency grid $\LOGGRID = \set{\f_1, \ldots, \f_{F'}}$ for a given factor $\alpha$ and the corresponding quasi-log grid $\QLOGGRID = \set{f_1, \ldots, f_F}$ for a resolution $\Delta f$.
  For each frequency $f_k$ in the quasi-log grid, we define $n\left(f_k\right)$ as the number of frequencies in the logarithmic grid that are nearer to $f_k$ than to any other grid line in the quasi-log grid:
  
  \begin{equation}
    n\left(f_k\right) = \numberOf \mathrm{nearestLines}_{f_k}
    \label{eq:excitation:nfk}
  \end{equation}
  \begin{equation}
    \mathrm{nearestLines}_{f_k} =
      \left\{ 
        \f_l \given{\begin{aligned} 
                       \f_l \in \LOGGRID, \\
                       \norm{\f_l - f_k} < \norm{\f_l - f_{k'}},\\
                       \forall k' \neq k: f_k, f_{k'} \in \QLOGGRID
                    \end{aligned}}
      \right\}
    \label{eq:excitation:nearestLines}
  \end{equation}  
  
  When approximating a log multisine with constant amplitude spectrum $A_{\mathrm{in}}$, we choose the amplitude spectrum of the compensated quasi-log multisine as follows
  \begin{equation}
    A_k = A_{\mathrm{in}} \cdot \sqrt{n\left(f_k\right)}\emspace \forall k \in \set{1,\ldots,F}
  \text{.}
  \label{eq:excitation:compensationAk}
  \end{equation}
  This effectively concentrates the total power of the $n\left( f_k \right)$ surrounding lines in the log grid multisine at the frequency $f_k$ in the compensated quasi-log multisine.
  
  The power spectrum of this compensated signal approximates the one of the log grid multisine better than an uncompensated quasi-log grid.
  As the power spectrum approximates the power spectrum of the log grid multisine, the uncertainty of the frequency response function will be approximately independent of the frequency within the measured frequency band.

  \subsection{Pink Quasi-Logarithmic Multisine}
  In \citep{Rojas2007} it is suggested that band-limited $1/f$ noise is a reasonable, albeit not optimal, robust excitation signal.

  For $1/f$ noise (`pink noise'), the power spectral density $S(f)$ is proportional to $1/f$.
  We can again approximate such a signal by means of a quasi-logarithmic grid multisine.
  To do so, we construct a quasi-logarithmic frequency grid $\set{f_1,\ldots,f_k,\ldots,f_F}$ and determine the corresponding amplitude spectrum $A_k$ such that each excited line carries the same power as the corresponding frequency band does for pink noise.

  This allows to define the amplitude spectrum $A_k$ as
  \begin{equation}
    A_k = A_{\mathrm{in}}
                 \sqrt{\int_{\fMin{k}}
                     ^{\fMax{k}}
                     \limits
                     \frac{1}
                          {f}
                     \,\dd{f}}
          = A_{\mathrm{in}}
                \cdot
                \sqrt{
                \ln \frac{\;\fMax{k}\;}{\fMin{k}}
                }
  \end{equation}
  where $\left[ \fMin{k}, \fMax{k} \right] $ denote the frequency range over which \gls{PSD} of the pink noise is to be approximated by the power at frequency line $f_k$.

  To reduce the complexity of the expressions for $\fMin{k}$ and $\fMax{k}$, we extend the frequency grid by one component to the left and to the right by means of the grid relation (equations \eqref{eq:excitation:qlogGrid} and \eqref{eq:excitation:constraintLinGrid}) to obtain the grid $\set{f_0, f_1, \ldots, f_F, f_{F+1}}$.

  Using this extension, the frequency range covered by $f_k$ can then be calculated as
    \begin{align}
      \fMin{k} &= \mean{f_{k-1}}{f_k}\\
      \fMax{k} &= \mean{f_{k+1}}{f_k}
    \label{eq:excitation:fRange}
    \text{.}
    \end{align}
  This range can be seen as the power-spectral density counterpart of the discrete spectrum relation \eqref{eq:excitation:nearestLines}.
  Consequently, both signals can be expected to behave in approximately the same way.

  Note that instead of the geometric mean, the arithmetic mean can be used in \eqref{eq:excitation:fRange} to calculate almost identical boundaries.

  \subsection{Comparison of the frequency grids}
  To gain some insight into these different grids, \figref{fig:excitation:freqGrids} depicts the amplitude spectrum for a linear, log, compensated quasi-log and pink quasi-log grid multisine that all carry the same total power.
  For this example, a frequency spacing $\Delta f = 1\unit{Hz}$ and frequency ratio $\alpha = 1.05$ are used.
  
  \begin{figure}[ht]
    \centering
      \setlength\figurewidth{0.8\columnwidth}
      \setlength\figureheight{0.5\figurewidth}
    \input{\thisDir/figs/freqGrid.tikz}
    \caption[Amplitude spectrum for linear, logarithmic, quasi-logarithmic grids.]{Comparison of the amplitude spectrum of a linear grid multisine, a logarithmic grid multisine, the compensated quasi-logarithmic multisine and the `pink' quasi-log multisine.
    $\omegaLinLog$ indicates the cross-over between the linear and logarithmic regions. }%
    \label{fig:excitation:freqGrids}
  \end{figure}

  \begin{table}
  \centering
  \label{tbl:excitation:gridProperties}
   \caption{Summary of  properties of the different frequency grids.}
    \begin{tabular}{rccc} 
    \toprule
     & \textsc{Linear} & \textsc{Logarithmic} & \textsc{Quasi-logarithmic} \\
    \midrule
    % Limits & \multicolumn{3}{c}{$\ClosedInterval{\fMin, \fMax}$}\\
    Properties 
       & $f_0, \Delta f$ 
       & $f_0, \alpha$ 
       & $f_0, \Delta f, \alpha$\\
    \midrule
    $f_{k+1} \isdef$ 
       & $f_{k} + \Delta f$ 
       & $\alpha f_{k}$
       & $\round{\frac{\alpha^{N_k} f_k}{\Delta f}} \Delta f$,   $N_k \in \NaturalNumbersWithoutZero$
       \\[5pt]
    $\Delta f_k \isdef f_{k+1} - f_k$ 
       & $\Delta f$
       & $(\alpha - 1) f_k$
       & $ \in \NaturalNumbersWithoutZero \Delta f$ \\[5pt]
    $\alpha_k \isdef \frac{f_{k+1}}{f_k}$ 
       & $1 + \frac{\Delta f}{f_k}$ 
       & $\alpha$ 
       & $\approx \alpha $ \\[5pt]
    $F_{\ClosedInterval{a,b}}$
       & $\floor{\frac{b - a}{\Delta f}}$
       & $\floor{\log_{\alpha} \frac{b}{a}}$
       & $\approx \floor{\log_{\alpha} \frac{b}{a}}$\\[5pt]
    $F_{\ClosedInterval{f, \kappa f}}$
      & $\floor{\frac{(\kappa - 1)f}{\Delta f}}$
      & $\floor{\log_{\alpha} \kappa}$
      & $\approx \floor{\log_{\alpha} \kappa}$\\[5pt]
    \bottomrule
    \end{tabular}
  \end{table}

  \begin{figure}
    \centering
    \setlength\figurewidth{0.85\columnwidth}
    \setlength\figureheight{0.68\figurewidth}
    \input{\thisDir/figs/gridProps.tikz}
    \caption[Actual frequency grid spacing and ratio for different grids]{Actual frequency grid spacing $\Delta f_k$ and ratio $\alpha_k$ as a function of the frequency, for the different grid types with $f_0 =1\unit{Hz}$, $\Delta f = 1\unit{Hz}$ and $\alpha =1.05$ as in \figref{fig:excitation:freqGrids}. $\omegaLinLog$ marks the transition between linear and logarithmic regime in the quasi-logarithmic grid.}
    \label{fig:excitation:gridPropertyPlots}
  \end{figure}

  At the lowest frequencies the quasi-log grid and the linear grid frequencies do coincide and each line in the quasi-log grid is surrounded by many lines of the log grid.
  The amplitudes of the compensated and pink quasi-log lines show that the compensation only has effect at the lower frequencies since the uncompensated counterpart has an amplitude spectrum close to $1$.
  The properties of these frequency grids are summarized in \tabref{tbl:excitation:gridProperties} and \figref{fig:excitation:gridPropertyPlots}.

  We can expect a very similar behavior from properly designed compensated quasi-log multisines and from the `pink' quasi-log multisine, since their amplitude spectra are almost identical.
  As such, it is advised to use `pink' quasi-log multisines as those can be constructed using more straightforward code (equation \eqref{eq:excitation:pink}) than the compensated quasi-log multisines (equations \eqref{eq:excitation:nfk}, \eqref{eq:excitation:nearestLines} , and \eqref{eq:excitation:compensationAk}).

\section{Optimization of the frequency spacing} 
\label{sec:excitation:optimAlpha}
  \subsection{Calculation of the model uncertainty}
  In the previous section logarithmically spaced multisines were introduced, but up to now their major parameter, the frequency spacing $\alpha$, has not been designed.

  In this section we show a possible approach to compare the performance of different excitation signals with respect to the theoretically minimal attainable uncertainty of the transfer function as expressed by the \gls{CRLB}.
  Doing so allows to determine a suitable value for the frequency spacing $\alpha$.

  Denote
    $U  \left(\omega\right)$ as the excitation signal,
    $\true{Y}\left(\omega\right)$ the exact, unperturbed system output,
    $E  \left(\omega\right)$ for the output noise source,
    $Y  \left(\omega\right) = \true{Y}\left( \omega \right) + E\left( \omega \right)$ depicts the measured output signal as shown in \figref{fig:excitation:OE}.
    The output noise is assumed to be zero-mean and white with a standard deviation of $\sigma_e$ in the time domain or $\sigma_E$ in the frequency domain.

  \begin{figure}[h]
    \centering
    \input{\thisDir/figs/oesetup.tikz}
    \caption[Output-error]{Output-error set-up.}
    \label{fig:excitation:OE}
  \end{figure}

  As in \citep{Pintelon2012, Gallager2008, matrixcookbook}, and equivalently to the derivation in \secref{sec:excitation:inputDesign}, the covariance matrix $\C_{G\left(\omega_{\interest}\right)}$ of the parametric transfer function $G(\theta,\omega)$ can be shown to be approximately equal to
  \begin{align}
    \C_{G\left( \omega_{\interest}\right) } \approx
      \InnerQuadraticForm[\HT]
                         {\jac{\interest}}
                         {\FisherInformation_{\theta}^{-1}}
     \label{eq:excitation:varPeak}
  \end{align}
  with 
  \begin{equation}
    \FisherInformation_{\theta} =
    2
    \real{
          \InnerQuadraticForm[\HT]
                              {\jac{\excited}}
                              {\C_{X}^{-1}}
          }
    \label{eq:excitation:FisherInfo}
  \end{equation}
  the Fisher information matrix \citep{Pintelon2012}.

  In these expressions, the Jacobian $\jac{\interest}$ matrix of the transfer function evaluated at the frequency of interest $\omega_{\interest}$ with respect to the model parameters $\theta$ is defined as
  \begin{equation}
    \jac{\interest} \isdef
         \PartialDerivative{G\left( \omega_{\interest}, \theta \right) }
                           {\theta}
    \text{.}
  \end{equation}
  Similarly, $\jac{\excited}$ is the Jacobian calculated at the excited frequency lines $\omega_{\excited} = \set{\omega_1, \ldots, \omega_F}$.

  Finally, $\C_{X}$ denotes the covariance matrix of $X\left(\omega\right) = G\left(\omega, \theta_0 \right) - \frac{Y_0\left(\omega \right) + E\left( \omega \right)  }{ U\left( \omega \right) } $, evaluated at the excited frequencies $\omega_{\excited}$.
  By assuming that the $X\left( \omega \right) $ are independently distributed over the frequency $\omega$, the variance-covariance matrix $\C_{X}$ is a diagonal matrix \citep{Pintelon2012}:
  \begin{equation}
    \C_{X} =
               \left[ \left.
               \begin{matrix}
                 \frac{\sigma^2_E\left(\omega_1 \right) }
                      {\abs{ U\left(\omega_1 \right)}^2 } & \deemph{0} & \deemph{0}\\
                 \deemph{0} & \ddots & \deemph{0} \\
                 \deemph{0} & \deemph{0} & \frac{\sigma^2_E\left(\omega_F \right) }
                              {\abs{U\left(\omega_F \right)}^2 }
               \end{matrix}
               \right.^{\vphantom{x}}\right]
    \text{.}
  \end{equation}

  In this chapter, we select a single frequency of interest $\omega_{\interest}$ to obtain a scalar performance criterion.
  This is allowed as the matrix $\C_{X}$ is diagonal.

\begin{remark}
\citet{Goodwin1977} note that for optimal input designs:
\begin{quote}
The choice of criteria is often not critical since it is usually the case that a good experiment according to one criterion will be deemed good by other criteria. 
Naturally this will only be true for sensibly chosen criteria.
\end{quote}
Also for the design of robust excitation signals, this is worthwhile observation.
\end{remark}

  \subsection{Relation between the frequency grid and the system resonance} 
  \label{sec:excitation:relationLogGridSystem}
  To determine an appropriate frequency ratio $\alpha$ for a logarithmically spaced signal, we study the estimated \glspl{FRF} of a set of prototype second-order systems excited by logarithmically spaced excitation signals.

  Instead of studying the response of all possible second-order sub-systems for every possible logarithmically spaced multisine-- a six-dimensional problem with parameters $(\omega_0, \alpha)$ for the grid, and $(\wz, \wn, \damping, \dc)$ for the sub-system-- we argue to simplify the problem in such a way that the results are still valid for the more complicated setting.

  \subsubsection{Exploiting the grid misalignment between the system and the grid}
  Denote $G(\omega; \wn)$ a second-order sub-system as in \eqref{eq:excitation:secondOrderSubsystem} with natural frequency $\wn$, damping ratio $\damping$ and static gain $\dc$, and zero at $\wz$.
  Similarly, $G(\omega; m\wn)$ where $m \in \RealNumbersWithoutZero$ is a second order sub-system that shares the same parameters, except its natural frequency which is situated at $m\wn$.

  If one calculates their \glspl{FRF} by evaluating \eqref{eq:excitation:secondOrderSubsystem}, one sees that $G(\omega; \wn)$ and $G(m\omega; m\wn)$ yield the same numerical values for any value of the pulsation $\omega$.
  Both \glspl{FRF} share the same `shape', but are `located' at different frequencies.

  Denote $\infgrid{\omega}$ to be a logarithmic frequency grid with an infinite number of grid lines
  \begin{equation}
    \infgrid{\omega} \isdef \Set{\alpha^k \omega_0 | k \in \ZN}
    \label{eq:excitation:freqGridLog}
  \end{equation}
  and consequently an infinite span.
  We can define a second grid as the $m$-fold multiple (with $m \in \RealNumbersWithoutZero$) of this grid and denote it $m\infgrid{\omega}$:
  \begin{equation}
    m \infgrid{\omega} \isdef \Set{m \alpha^k \omega_0 | k \in \ZN}
    \text{.}
  \end{equation}
  Both $\infgrid{\omega}$ and $m\infgrid{\omega}$ share the frequency ratio $\alpha$.
  \begin{remark} \label{rem:excitation:exc:gridscaling}
    For $N \in \ZN$ it is obvious that $\infgrid{\omega} = \alpha^{N} \infgrid{\omega}$, i.e. both grids coincide.
  \end{remark}

  \begin{definition}
  To evaluate a function $G$ over the grid $\infgrid{\omega}$ with infinite span, let us introduce the shorthand notation $G(\infgrid{\omega})$ to mean $\set{G(\omega) | \omega \in \infgrid{\omega}}$.
  \end{definition}

  \begin{lemma} \label{lem:excitation:exc:sysscaling}
  Consider the second-order systems $G(\omega; \wn)$ and $G(\omega; m\wn)$ with $m \in \RealNumbersWithoutZero$. 
  For a frequency grid $\infgrid{\omega}$ with infinite span, $G(\infgrid{\omega}; \wn) = G(m \infgrid{\omega}; m\wn)$.
  \end{lemma}
  \begin{proof}
  Consider any $\omega_{\star} \in \infgrid{\omega}$. The frequency $m\omega_{\star}$ is then by definition an element of the grid $m\infgrid{\omega}$.
  Substituting $\omega = \omega_{\star}$ and $\omega = m \omega_{\star}$  into  $G(\omega; \wn)$ and $G(m\omega; m\wn)$  respectively, directly yields
  \begin{equation}
    \frac{\dc}{- \omega_{\star}^2 \wn^{-2} + 2j\damping \omega_{\star} \wn^{-1} + 1} = 
    \frac{\dc}{- (m\omega_{\star})^2 (m\wn)^{-2} + 2j\damping (m\omega_{\star}) (m\wn)^{-1} + 1}
    \text{.}
  \end{equation}
  Since all the factors ``$m$'' cancel in the equation above, this proves the stated.
  \end{proof}

  \begin{theorem}
    Second-order  systems with equal damping $\damping$ and \gls{DC} gain $\dc$ but resonance frequencies that differ by a factor $\alpha^N$ with $N \in \IntegerNumbers$, share an identical `shape` of their \gls{FRF} at the lines of a logarithmically spaced frequency grid with infinite span and a frequency ratio $\alpha$.
    Or, formally: $\forall N \in \NaturalNumbers: G(\infgrid{\omega};\wn) = G(\infgrid{\omega}; \alpha^{N}\wn)$.
  \end{theorem}
  \begin{proof}
    This is an immediate consequence from Lemma~\ref{lem:excitation:exc:sysscaling} for $m= \alpha^{N}$ and Remark~\ref{rem:excitation:exc:gridscaling}.
  \end{proof}

  Taking this into account allows to limit the study of second order systems to a small set of systems that should be considered in the search for a suitable grid spacing $\alpha$.
  We can limit the evaluation of the theoretical uncertainty \eqref{eq:excitation:varPeak} obtained by exciting the system with a logarithmic signal with ratio $\alpha$ to the analysis of second order systems with a resonance frequency set to
  \begin{equation}
    \wn \alpha^{N+\gamma}\qquad \text{ with } 0 \leq \gamma < 1\text{.}
    \label{eq:excitation:theoreticalBestVariationOmega}
  \end{equation}
  The value $\gamma$ is varied for a fixed value of $N \in \ZN$.
  The value of $N$ will not influence the `shape` (and uncertainty) of the FRF.

  The arguments for a logarithmically spaced grid with an infinite span remain approximately valid for quasi-logarithmically spaced grids with a finite frequency support if the resonance peak of the system under test lies well within the `bulk' of the frequency grid with finite support, i.e. $\RightOpenInterval{\omegaLinLog, +\infty}$, where
  \begin{equation}
    \omegaLinLog \isdef \frac{2\pi \Delta f}{\alpha - 1}
    \label{eq:excitation:linearLogTransitionFrequency}
  \end{equation}
  indicates the lowest frequency where the quasi-logarithmic grid can effectively reproduce the logarithmic spacing.
  In particular for lightly-damped sub-systems, $\wdB \approx \ClosedInterval{(1-\damping)\wn, (1+\damping)\wn}$, such that the resonance peak is in the bulk of the frequency grid when
  \begin{equation} 
    \omegaLinLog =\frac{2 \pi \Delta f}{\alpha -1} \leq \left( 1 - \damping \right) \wn
    \text{.}
    \label{eq:excitation:systemInBulk:generic}
  \end{equation}
  %TODO: turn this into a guideline?
  For more reliable results, however, it is advisable to also have a few logarithmically spaced frequency lines, outside of the resonance peak of the actual system for a more robust result.
  In practice, this means that systems with their resonance peak at the lowest and the very highest edge of the frequency band may not be observed with identical quality as systems at intermediate frequencies.

  The validity of this approximation can be understood intuitively by inspecting the derivatives that constitute the Fisher information matrix (see \figref{fig:excitation:derivatives}): those derivatives are significantly larger near the resonance frequency than at other frequencies.

  \subsection{Exploiting the shape of the derivatives}
   In particular, from \figref{fig:excitation:derivatives}, it can be seen that the derivatives with respect to $(\wn, \damping)$ exhibit a sharper resonance and hence $(\wn, \damping)$ will prove the hardest to identify.
   This means that if one would design a signal to identify just $(\wn, \damping)$, such a signal would also be well-suited to identify the other parameters since their corresponding weight in the Fisher information matrix is very similar (especially in the vicinity of the resonance, where their amplitude is most significant).

   As such, the free parameters $(\dc, \damping, \wn, \wz)$ of the sub-system are reduced to only $(\damping, \wn)$ and the others are fixed during our experiments: $\dc = 1$ and $\wz \to \infty$.

  % When we now take a closer look at the general second order system \eqref{eq:excitation:secondOrderSubsystem}, we still are left with $\dc$ and $\damping$ as free parameters in the prototypes.
  % Note that $\dc$ is a scaling factor for the FRF.
  % Hence the results for different values of $\dc$ only differ by a scaling factor.
  % So we only need to perform the analysis for a fixed value of $\dc$, which was chosen equal to $1$ in our test case.

  The choice of the damping is more important.
  Instead of choosing the damping directly, we choose the peak gain $\left| G\left( \wM \right) \right| \in \set{5 \unit{dB}, 10 \unit{dB}, 20 \unit{dB}, 40 \unit{dB} }$ of the transfer function \eqref{eq:excitation:peakAmplitude} to study the effect of differently damped systems.
  However, do remember that $\abs{G(\wM)} = (2\damping\sqrt{1-\damping^2})^{-1}$ for the simplifications introduced above as in \eqref{eq:excitation:peakAmplitude}.

  \subsection{Selection of the frequency grid spacing $\alpha$}
  Since the damping influences the $3\unit{dB}$ bandwidths, this will become important when one has to determine the grid spacing $\alpha$.
  To deal with this, we introduce the normalized frequency ratio
  \begin{equation}
    \tilde{\alpha} = \frac{\alpha - 1}
                          {\damping}
                   \propto \frac{\omega_{k+1} - \omega_k}
                                {\wdB}
                           \cdot
                           \frac{\wn}{\omega_k}
    \text{.}
    \label{eq:excitation:normAlpha} 
  \end{equation}
  This normalized frequency ratio can be interpreted as a measure of the grid resolution, relative to the $3\unit{dB}$ bandwidth of the second order system.
  The grid resolution in a (quasi-) log grid is $\left( \alpha - 1\right)\omega_k$ while the system bandwidth is approximately $2\damping\wn$.
  Around the resonance, $\omega_k \approx \wn$, such that the last fraction in \eqref{eq:excitation:normAlpha} can safely be neglected.
  
  To determine a suitable value of the frequency spacing ratio $\alpha$ for the logarithmic and (compensated) quasi-logarithmic grid, we construct a logarithmic multisine signal in the frequency domain for a given frequency band (in this example, $\left[ 0.01 \unit{rad/s}, 100 \unit{rad/s}\right]$ was used).
  In the previous section we saw that systems with equal damping and \gls{DC} gain, will behave very similarly when excited by a log signal with spacing $\alpha$.

  Instead of using \eqref{eq:excitation:theoreticalBestVariationOmega} to establish the influence of the resonance frequency, we follow an equivalent approach that is more intuitive to interpret.

  \begin{figure}
    \centering
      \setlength\figurewidth{0.8\columnwidth}
      \setlength\figureheight{0.5\figurewidth}
    \input{\thisDir/figs/misalign.tikz}
    \caption[Depiction of the grid misalignment $\beta$ for a logarithmic generalized multisine.]{Graphical depiction of the misalignment coefficient $\beta$ with respect to a logarithmic frequency grid with spacing $\alpha$ for two systems with different damping.
    The $3\unit{dB}$ bandwidth ($\wdB$) around the peak frequency $\wM$ of the system is highlighted.}
    \label{fig:excitation:misalignment-logms}
  \end{figure}


  We choose the resonance frequency of the systems such that
  \begin{equation}
   \wM\left( \wn \right)  = \omega_k + \beta \left( \omega_{k+1} - \omega_k \right)
   \text{ with } 0 \leq \beta < 1
   \label{eq:excitation:chosenVariationOmega}
  \end{equation}
  where $\omega_k$ and $\omega_{k+1}$ are the excited grid lines closest to the frequency $\wM$ where the FRF has its peak amplitude and $\beta$ is the misalignment coefficient that is varied over the interval $\RightOpenInterval{0,1}$ as depicted graphically in \figref{fig:excitation:misalignment-logms}.
  This means that $\omega_k \leq \wM \leq \omega_{k+1}$ and that $\beta$ has the same role as $\gamma$ factor in \eqref{eq:excitation:theoreticalBestVariationOmega}.
  If we take a look at the special case when $\beta=0$ (or $\beta=1$) in this expression, we see that this situation occurs when the peak value of the \gls{FRF} at $\wM$ coincides with the grid line $\omega_k$ or $\omega_{k+1}$ respectively.
  Intuitively, we study second order systems whose peak frequency $\wM$ is shifted between two neighboring excited frequency lines.
  From the previous section, we know that the exact frequency matters little as long as it remains within the bulk of the frequency grid.
  In this particular example $\wn$ was chosen at approximately $1 \unit{rad/s}$.

  We normalize the \gls{RMS} value of the excitation signal to $1$ and choose $\sigma_E\left( \omega \right) = 0.1$ for all evaluations.
  The noise level is kept constant, which causes the \gls{SNR} per excited line to drop for more finely spaced excitation signals.
  % SNR correct: Pnoise ~ sE^2
  %              Psignal ~F*U^2 = 1 ==> U^2 = 1/F
  %              SNR = Psignal / Pnoise = 1/(sE^2 * F)
  %          ==> SNR decreases as F increases
  
  We then calculate the theoretical variance on the magnitude of the peak in the \gls{FRF} ($\left| G\left( \wM \right) \right|$) as shown in \eqref{eq:excitation:varPeak} for a coarse grid.
  A coarse grid was chosen with $\tilde{\alpha} = 10$; this means that the spacing between the excited lines around the resonance are much larger than the system bandwidth.
  It is obvious that such an excitation signal will not perform well in most cases, as the resonance peak is very likely to be overlooked in such an \gls{FRF}.
  
  For each set of second order systems, we determine for which value of the misalignment coefficient $\beta$ the maximum variance on the FRF is obtained.
  
  For these worst cases, we study how the frequency spacing $\alpha$ influences the uncertainty by decreasing $\alpha$ until no further improvement can be obtained.

  \subsection{Discussion} 
  \label{sec:excitation:bestFrequencyResolution}
  As stated above, we start by evaluating the variance $\sigma_G\left( \wM\right)$ for a coarse frequency spacing $\tilde{\alpha} = 10$ for each of the prototype systems.
  This is shown by the solid lines in \figref{fig:excitation:worstCaseBeta}.
  One immediately notices that the variance depends strongly on the misalignment $\beta$ of the system and the frequency grid.
  
  \begin{figure}
    \centering
      \setlength\figurewidth{0.75\columnwidth}
      \setlength\figureheight{0.68\figurewidth}
    \input{\thisDir/figs/betaWC.tikz}
    \caption[Variance $\sigma_G^2\left( \wM \right)$ as a function of the grid misalignment $\beta$.]{Evaluation of the variance $\sigma_G^2\left( \wM \right) $ for different prototype systems as function of the misalignment $\beta$.
             The full lines depict the variance for a coarsely spaced signal.
             The dashed lines show the variance for a signal with the optimal $\alpha = \alpha^{\star}$ (see \eqref{eq:excitation:optimAlpha}).}
    \label{fig:excitation:worstCaseBeta}
  \end{figure}

  Such signals are clearly undesirable, as a misalignment smaller than the grid spacing of the excitation grid and the system will lead to a large increase in the uncertainty of estimate.
  
  For a large peak amplitude of the \gls{FRF} (low damping), the worst case increase (marked with a cross) is approximately centered around $\beta = 0.5$ (i.e. midway between the nearest excited lines).
  In systems with a higher damping, this symmetry is lost.
  This can be explained intuitively as the $3\unit{dB}$ bandwidth is less symmetric for highly damped systems than in lightly damped systems with a very narrow resonance peak.
  Also note that the value of $\sigma_G\left(\wM \right)$ is practically identical for $\beta=0$ and $\beta=1$.
  This is in accordance with the claim that in the bulk of the grid, a shift over an integer number of lines will only marginally influence the uncertainty.
  
  We explore the systems which have the worst uncertainty (as marked by a cross in \figref{fig:excitation:worstCaseBeta}).
  For the corresponding values of the misalignment $\beta$, we evaluate the uncertainty when the frequency ratio $\alpha$ of the signal is decreased, while the total signal power is kept identical.
  The results are displayed in \figref{fig:excitation:worstCaseAlpha}, where the marked points correspond to the marked points in \figref{fig:excitation:worstCaseBeta}.
  
  \begin{figure}
    \centering
      \setlength\figurewidth{0.75\columnwidth}
      \setlength\figureheight{0.68\figurewidth}
    \input{\thisDir/figs/alphaWC.tikz}
    \caption[Worst-case variance $\sigma_G^2\left( \wM \right)$ as a function of the frequency ratio $\alpha$.]{Worst case variance of the peak in the transfer function for logarithmic
             grid multisines and systems with different peak amplitudes.}
    \label{fig:excitation:worstCaseAlpha}
  \end{figure}

  We notice that for sufficiently low $\tilde{\alpha}$, the variance on $G\left( \omega_M \right)$ is roughly independent of $\alpha$.
  We will choose the optimal grid spacing
  \begin{equation}
    \alpha^{\star} = 1 + \damping
    \label{eq:excitation:optimAlpha}
  \end{equation}
  such that it represents the coarsest frequency spacing for which the variance is approximately equal to that constant value.
  This choice is indicated by the dashed line in \figref{fig:excitation:worstCaseAlpha}.

   \begin{guideline}[Design the logarithmic grid for the damping of the system: $\alpha \leq  1 + \xi_{\min}$]
   To measure a resonant system with damping $\damping > \damping[\min]$ well, a (pink) quasi-logarithmic multisine with frequency ratio $\frac{\omega_k}{\omega_{k-1}} \leq \alpha \leq 1 + \damping[\min]$ is recommended.
   Smaller frequency ratios provide more measurement points at the cost of a reduced \gls{SNR} per frequency bin.
   As such, $\alpha = 1 + \damping \isdef \alpha^{\star}$ provides the required resolution and the best \gls{SNR} per bin.
\end{guideline}

  Choosing a finer grid will not improve the estimated $G\left( \wM \right)$ significantly, but will rather produce an \gls{FRF} where the \gls{SNR} per bin is decreased.
  On the other hand, a coarser grid will significantly decrease the quality of the measurement as it overlooks the resonance peak.
  Hence little information can be extracted from such a measurement (i.e. the variance on the estimate will be large).

  If we determine the effect of the misalignment factor $\beta$ for a grid with frequency ratio $\alpha^{\star}$, we see that misalignment has a negligible effect on the uncertainty as shown by the dashed lines in \figref{fig:excitation:worstCaseBeta} for each of the prototype systems.
  Note that these values do indeed coincide with the limiting values found in \figref{fig:excitation:worstCaseAlpha}.
  
  Now we can interpret \figref{fig:excitation:worstCaseBeta} in more detail.
  We see that for $\beta \in \set{0,1}$ (i.e. perfect alignment), the variance of a course grid (full lines) is lower than it is for a fine grid (dashed lines).
  This is to be expected as a coarse grid places more power at each individual line.
  Thereby it yields a smaller variance at those lines.
  It is better tailored to systems which are closely aligned to the excited frequency grid.
  However, there is a critical consequence: when there is a moderate misalignment $\beta$ of the system and the excitation grid, the variance increases significantly.
  This effect is more important for lightly-damped systems, as their $3\unit{dB}$ bandwidth is very narrow and hence their main response is easier to `miss'.

  To give a more intuitive feeling how the frequency spacing $\alpha$ and the system under test react, both the input spectrum and the \gls{FRF} of the system are visualized in \figref{fig:excitation:signalAndSystem} for the worst value of $\beta$ and the optimal frequency spacing $\alpha^{\star}$.
  The $3\unit{dB}$ bandwidth of the system is highlighted in gray together with the excited frequency lines that lie within that bandwidth.

  \begin{figure}%
    \centering
      \setlength\figurewidth{0.5\columnwidth}
      \setlength\figureheight{0.68\figurewidth}
    \input{\thisDir/figs/SignalAndSystem.tikz}
    \caption[System with highlighted bandwidth and multisine components.]{Depiction of the transfer function $G$ of a second order system, along with a logarithmic multisine excitation $U$ for $\alpha = \alpha^{\star}$. The circled sine components lie within the $3 \unit{dB}$ bandwidth of $G$.}%
    \label{fig:excitation:signalAndSystem}
  \end{figure}

  We see that to attain this, four frequency components are needed within this bandwidth in order to be almost completely independent of misalignment.

  In this section, we have shown that signals with a logarithmic frequency spacing allow to identify systems with equal damping equally well, when the frequency spacing is dense enough.
  Practically, four frequency lines in the $3\unit{dB}$ bandwidth of each second order subsystem suffice to suppress the effect of misalignment of such a subsystem and the frequency grid of the excitation signal.
  
\section{Simulations} \label{sec:excitation:simulation}
In the following set of simulations, we will study how the \gls{FRF} of different second order systems with equal damping (and the corresponding uncertainty) are measured by the multisine excitation signals from \secref{sec:excitation:multisine}.
  \subsection{Setup}
  We consider second-order systems with an equal damping and with resonance frequencies $4$, $8$, $20$, $40$, $80$, $200$, $400$, $800$, $2000$ and $4000 \unit{Hz}$.
  Using the \matlab\ instruction \texttt{cheby1}, discrete-time equivalents of these systems are generated.
  These can be represented by a transfer function of the form:
  \begin{equation}
    G \left( z \right)\Big|_{\damping, \fR} =
       \frac{         b_2  z^{2} + b_1 z + b_0}%
            {\phantom{a_2} z^{2} + a_1 z + a_0}
  \text{.}
  \label{eq:excitation:secondOrderSystemDTidentifiable}
  \end{equation}

  This form can also be obtained by applying Tustin's method or bilinear transform~\citep{Oppenheim1983} to the continuous time system \eqref{eq:excitation:secondOrderSubsystem}.

  The input signal has an equidistant frequency grid with a resolution $\Delta f = 1\unit{Hz}$ between $1 \unit{Hz}$ and $16\,384\unit{Hz}$.
  The sampling frequency $f_s$ is set to $65\,536\unit{Hz}$ to prevent aliasing.
  For the quasi-log grid multisine, the compensated and pink quasi-log multisine, we chose a frequency ratio $\alpha = 1.05$.
  According to \eqref{eq:excitation:optimAlpha}, this frequency ratio is suited for second-order systems with a damping $\damping \geq 0.05$, or a peak amplitude of the \gls{FRF} $\abs{G(\wM)}$ less than approximately $20\unit{dB}$.
  Each of these signals is constructed to have an \gls{RMS} value of $1$.
  The amplitude spectra of these signals are displayed in \figref{fig:excitation:freqGrids} for identical signal power.

  At the output of the systems, white Gaussian noise $e[n]$ is added such that its standard deviation is approximately $10\%$ of the RMS value of the output.

  
  The obtained FRF is then fitted to the transfer function model \eqref{eq:excitation:secondOrderSystemDTidentifiable} using the \texttt{elis} function of the \FDIDENT~\citep{FDIDENT} for \MATLAB.
  This is a maximum likelihood estimator that fits a parametric model to the measured data in the frequency domain.
  %TODO: add formulas with cost function etc.
  To improve convergence and avoid local minima, the initial values for the model are chosen equal to the parameters of the true system $G_0(z)$, so that we can focus completely on the variance of the estimations.

  Such an experiment is repeated $R=1000$ times with a different random phase  spectrum $\phi_k$ and a different noise realization $e[n]$ for each experiment.

  \begin{remark}
  All simulations  have been repeated as well by fitting with the \MATLAB System Identification Toolbox by means of the \mcode{oe} function~\citep{TDIDENT}.
  The observed results were similar and hence are not elaborated further in this text.
  For more information, please refer to~\citep{Larsson2012SYSID}.
  \end{remark}

  \subsection{Systems with a `high' damping}
  To test the signals for systems with a high damping with respect to the frequency ratio in the excitation signal, systems with a damping $\damping = 0.2$ are used.
  White Gaussian noise $e[n]$ with a standard deviation $\sigma_e \approx 0.0678$ added to the output.

  The resulting frequency response function $\hat{G}$ for one such system is shown in \figref{fig:excitation:FRF1} together with the standard deviation $\sigma_{\hat{G}}$ of the estimated parametric model for each of the different excitation signals.
  The bias is at least $20\unit{dB}$ smaller than the standard  deviation.
  Therefore the \gls{MSE} will be determined mainly by the variance of the estimate instead of the bias.

  \begin{figure}[th]
    \centering
      \setlength\figurewidth{0.68\columnwidth}
      \setlength\figureheight{0.68\figurewidth}
    \input{\thisDir/figs/Sys02-4.tikz}
    \caption[Simulated \gls{FRF} of a single system using different excitation signals.]{\gls{FRF}~\legref{leg:excitation:sim:FRF} of a second order system with $\fR=40\unit{Hz}$ and $\damping=0.2$ along with the corresponding variance and bias of the parametric models obtained using three different kinds of multisine excitations.}%
    \label{fig:excitation:FRF1}
  \end{figure}

  \figref{fig:excitation:damping02} displays the estimated transfer functions and their uncertainty in the vicinity of their corresponding resonances for the whole set of considered systems.
  Using this condensed representation and the displayed envelopes of the uncertainty, we can deduce the frequency-dependent behavior of the model variance obtained by the different excitation signals.

  \begin{figure}%
    \centering
      \setlength\figurewidth{0.68\columnwidth}
      \setlength\figureheight{0.68\figurewidth}
    \input{\thisDir/figs/damping02-cmp.tikz}
    \caption[Simulated \glspl{FRF} of systems with damping $0.2$ for different excitation signals.]{%
       \Glspl{FRF} for different second order systems with constant damping $\damping=0.2$ but different resonance frequencies. 
       Comparison of the variance around the respective resonance frequency reveals the performance of each excitation signal.}%
    \label{fig:excitation:damping02}
  \end{figure}

  For the linear grid multisine, the uncertainty is strongly dependent on the frequency, and is proportional to $1/\fR$.
  The uncertainty is generally higher than the uncertainty obtained with logarithmically spaced multisines, as the power has been divided over more excited lines.

  The quasi-log multisine already shows a fairly flat uncertainty of about $-52 \unit{dB}$ at the higher frequency ranges.
  Yet at the lowest frequencies, the $1/\fR$ dependence can be observed again.
  This is caused by the linear distribution of excited frequencies in this range, as shown in \figref{fig:excitation:freqGrids}.

  The compensated quasi-log multisine shows a more constant variance of about $-51 \unit{dB}$ even at these low frequencies.
  As more power is placed at low frequencies, less power can be placed at the higher ones.
  This causes a slightly increased variance compared to the regular quasi-log multisine.
  At the studied system with the lowest resonance frequency, the compensation fails to completely compensate the effects of the reduced number of lines in the $3\unit{dB}$ bandwidth of that system.
  Nevertheless an improvement of about $5\unit{dB}$ is obtained with respect to the regular quasi-log multisine.

  \subsection{Systems with a lower damping}
  Given the optimal frequency ratio \eqref{eq:excitation:optimAlpha}, one expects that quasi-logarithmic grids with a frequency ratio $\alpha = 1.05$ will allow to identify second order systems with damping $\damping = 0.05$ with a constant uncertainty.
  In this set of simulations, we check this by repeating the previous set for such systems.
  In this case, the output noise is chosen $\sigma_e \approx 0.1123$ to keep the \gls{SNR} of $20\unit{dB}$.
  The corresponding results are shown in \figref{fig:excitation:damping005}.
 
  \begin{figure}%
    \centering
      \setlength\figurewidth{0.68\columnwidth}
      \setlength\figureheight{0.68\figurewidth}
    \input{\thisDir/figs/damping005-cmp.tikz}
    \caption[Simulated \glspl{FRF} of systems with damping $0.05$ for different excitation signals.]{\Glspl{FRF} for different second order systems with constant damping $\damping=0.05$ but different resonance frequencies. 
    Comparison of the variance around the respective resonance frequency reveals the performance of each excitation signal.}%
    \label{fig:excitation:damping005}
  \end{figure}

  At the higher frequency bands, an approximately constant quality is still obtained for the (compensated) quasi-logarithmic signals.
  At lower frequencies, however, the logarithmically spaced signals perform only slightly better than the linearly spaced excitation signal.
  This is due to the fact that as the damping $\damping$ decreases, the $3\unit{dB}$ bandwidth of the resonance decreases and therefore less points can be fitted in this bandwidth at the lower frequencies.
  In our case, only $1$ line was excited in the $3 \unit{dB}$ bandwidth (respectively $0.4 \unit{Hz}$ and $0.8\unit{Hz}$ wide) of the systems with the lowest resonance frequencies.
  It is clearly unfeasible to estimate the five parameters in
  \eqref{eq:excitation:secondOrderSystemDTidentifiable} using this little information in the $3\unit{dB}$ bandwidth of the system.
  In other words, the resonances of the systems at low frequencies, can no longer be considered to lie within the bulk of the logarithmic part of the quasi-logarithmic frequency grids.
  This can also be understood from equation \eqref{eq:excitation:systemInBulk:generic}.
  In that equation, $\alpha = \alpha^{\star} = 1 + \damping$ (see \secref{sec:excitation:optimAlpha}) such that the following guideline is the necessary condition for the $3\unit{dB}$ bandwidth of the system to lie within the bulk of the frequency grid.

\begin{guideline}[Use a sufficient grid resolution for each resonance: $2 \pi \Delta f \leq (\damping - \damping^2) \wn$]
\label{guide:excitation:systemInBulk:tuned}
 A quasi-logarithmic multisine with frequency resolution $\Delta f$ and tuned frequency spacing $\alpha=1+\damping$ is only guaranteed to properly excite resonance peaks with damping $\damping$ and resonance frequency $\wn$ when 
  \begin{equation}
    2 \pi \Delta f \leq (\damping - \damping^2) \wn 
    \approx \damping \wn \text{ if $\damping \ll 1$}
    \text{.}
    \label{eq:excitation:systemInBulk:tuned}
  \end{equation}
  \end{guideline}

  \begin{remark}
  \guideref{guide:excitation:systemInBulk:tuned} can be interpreted either as
  \begin{itemize}
     \item  a lower limit on the allowable $\wn$ of the system (when the multisine has been designed ($\Delta f$ and $\damping$ are fixed)), or 
     \item as a requirement on the measurement time $\Tm = (\Delta f)^{-1}$ when lower bounds on $\damping$ and $\wn$ are known.
   \end{itemize}
  \end{remark}

  For this simulation, with $\Delta f=1\unit{Hz}$, $\damping=0.05$, \eqref{eq:excitation:systemInBulk:tuned} evaluates to $\wn \geq 132 \unit{rad/s} = 21 \unit{Hz}$.
  The system with $\wR = 20 \unit{Hz} < 21 \unit{Hz}$ is still well-identified, even though `only' $3$ lines are excited in its bandwidth.
  In that particular case, the frequency grid and the resonance peak are well-aligned ($\beta \approx 0$), which leads to a reduced variance (see also \figref{fig:excitation:worstCaseBeta}).

  We do see, however, that the variance in the vicinity of the resonance of every system is lower than in the case of a linearly spaced excitation.

  \subsection{Simulation example on higher order systems}
To show that the suggested method also works on more general systems, we considered a system that consists of two second order systems \eqref{eq:excitation:secondOrderSystemDTidentifiable} in parallel. This is equivalent to adding their respective transfer functions, therefore, the resulting transfer function is of the form:
\begin{equation}
  G \left( z \right) 
  = \frac{         b_4 z^{4} + b_3 z^{3} + b_2 z^{2} + b_1 z + b_0}%
         {\phantom{a_4}z^{4} + a_3 z^{3} + a_2 z^{2} + a_1 z + a_0}
  \text{.}
\label{eq:excitation:tfO4}
\end{equation}

For example, a system composed of two second order systems with $\damping = 0.2$ and $\fR = 400 \unit{Hz}$ for the first system and $\fR=4 \unit{kHz}$ for the second system was simulated similarly to the previous examples. The standard deviation of the noise $\sigma_e$ was put at $0.0678$. The corresponding results are  shown in \figref{fig:excitation:sumSys}.

\begin{figure}%
  \centering
  \setlength\figurewidth{0.68\columnwidth}
  \setlength\figureheight{0.68\figurewidth}
  \input{\thisDir/figs/Sum02log0-2.tikz}
  \caption[Simulated FRF of sum of two sub-systems.]{FRF for the sum of two second order systems with $\damping=0.2$ and            resonances at $ 400 \unit{Hz}$ and 4 \unit{kHz} as observed by a quasi-logarithmic multisine.}%
  \label{fig:excitation:sumSys}
\end{figure}

We observe that the amplitude of the transfer function is $9.8\unit{dB}$ and $8.1\unit{dB}$ at the resonant frequencies.
The standard deviation on the \glspl{FRF} lies respectively $59.7 \unit{dB}$ and $59.6 \unit{dB}$ lower than the \gls{FRF} at these frequencies. 
This shows that both resonances are captured with a practically identical relative error.

\begin{guideline}[Use quasi-logarithmic multisines as initial excitation]
  Quasi-logarithmic multisines with pink-colored amplitude spectrum facilitate the measurement of multiple sharp resonance peaks over a wide frequency band of interest with a constant quality.
\end{guideline}
  
\section{Measurements} 
\label{sec:excitation:measurement}
  \subsection{Setup}
    \label{sec:excitation:measurement:setup}
    To show that this methodology also works for more complicated systems, we measured the frequency response functions of a \bruelkjaer{} Type 1613 Octave Filter Set using a National Instruments Elvis~II board driven by \labview.
    
    The \BK~1613 contains a set of eleven analog passive band-pass filters with center frequencies $\fc$ that are logarithmically distributed between $31.5\unit{Hz}$ and $31.5\unit{kHz}$ inclusive.
    According to its manual~\citep{datasheet_bk1613}, a $0.5\unit{dB}$ ripple with three resonances is present in the pass-band and the $3\unit{dB}$ break frequencies are located at $\frac{\fc}{\sqrt{2}}$ and $\fc \sqrt{2}$.
    This leads to a sixth-order Chebyshev band-pass filter of Type~I~\citep{Zverev1967} with transfer function
    \begin{equation}
       G_{\mathrm{init}}(s) \approx 
      \frac{0.15922 s_{\mathrm{n}}^3 }
                {              s_{\mathrm{n}}^6 
                + 0.759 s_{\mathrm{n}}^5 
                + 3.564 s_{\mathrm{n}}^4 
                + 1.678 s_{\mathrm{n}}^3 
                + 3.564 s_{\mathrm{n}}^2 
                + 0.759 s_{\mathrm{n}} 
                + 1}
      \label{eq:excitation:bk1613:tf-init}
     \end{equation}
     where $s_{\mathrm{n}} \isdef s \wc^{-1}$ and $\wc=2\pi \fc$ is the center frequency of the respective filter.
     It can be seen from \tabref{tbl:excitation:bk1613:poles} that each pole has $\damping \geq 0.09$.
    Since the different pole pairs that constitute such a filter have a different damping, this is an ideally suited system to evaluate the proposed excitation signals.
    The `shape' of the transfer function of the different filters is nominally identical, which allows for an easy visual comparison between the model uncertainty of the different filters.

    \begin{table}[bt]
      \centering
      \caption{Poles of the initial model of the \bruelkjaer{} 1613 filter with center frequency $\wc$.}
      \label{tbl:excitation:bk1613:poles}
      \begin{tabular}{rrrr}
        \toprule
        \textsc{Pole pair} & \textsc{Damping ($\damping$)} & \textsc{Frequency ($\wn$)} & \textsc{Time constant ($\timeconst$)}\\
        \midrule
        $-(0.067 \pm 0.733j)\,\wc$ & $0.091$ & $0.74\,\wc$ & $15.0\,\wc^{-1}$\\ 
        $-(0.190 \pm 0.982j)\,\wc$ & $0.190$ & $\wc$ & $5.3\, \wc^{-1}$\\ 
        $-(0.123 \pm 1.350j)\,\wc$ & $0.091$ & $1.36\,\wc$ & $8.1 \,\wc^{-1}$\\ 
        \bottomrule
      \end{tabular}
    \end{table}

    As excitation signals, a linear grid multisine, an uncompensated quasi-log multisine and pink quasi-log multisine with excited frequencies between $1 \unit{Hz}$ and $64 \unit{kHz}$ were used.
    These multisines were sampled at $\fs = 256\unit{kHz}$ and had a frequency resolution of $\Delta f = 1 \unit{Hz}$, a frequency spacing $\alpha = 1.05$ and identical \gls{RMS} values of approximately $0.25 \unit{V}$.
    This amplitude was chosen such that the peak amplitude $\max \abs{u(t)}$ was approximately $1 \unit{V}$.
    An attempt was made to use a random-phase multisine with a limited crest factor
    \begin{equation}
      \crest{u} \isdef \frac{\norm[\infty]{u(t)}}{\norm[2]{u(t)}} = \frac{\max{\abs{u(t)}}}{ \sqrt{\Tm^{-1} \int_{0}^{\Tm} u(t)^2 \dd{t}}}
    \end{equation}
    to avoid excessive effects of nonlinear behavior of the filter.
    In particular, for each signal $1000$ realizations of the phase spectrum $\phi_k$ were sampled from a uniform distribution over $\LeftOpenInterval{0,2\pi}$; from these realizations only the multisine with the lowest crest factor was retained and applied to the filters.

    $R=50$ periods of each multisine were measured, from which the first period was discarded to suppress transient effects.
    For each of the remaining periods, the \gls{FRF} of the filter is calculated and its average $G\left(\omega \right)$ over the periods is calculated.
    We also calculate the corresponding standard deviation $\sigma_G\left( \omega \right)$ of the \gls{FRF} and normalize it for a single period of the excitation signal.
    We denote $G\left(\omega \right)$ the non-parametric FRF and $\sigma_G\left( \omega \right)$ its standard deviation, as these measures are determined completely nonparametrically from the measurements.

    For the parametric model, on the other hand, a continuous time \gls{LTI} model of the form
    \begin{equation}
      G(s) = \frac{\Sum_{i=0}^3 b_i s^i}{\Sum_{i=0}^6 a_i s^i}
    \end{equation}
    was estimated using \texttt{elis}~\citep{FDIDENT}.
    To aid convergence to a good optimum, the initial estimate was a sixth-order band-pass type I Chebyshev filter given in \eqref{eq:excitation:bk1613:tf-init}.

    We summarize the behavior of these parametric models by their average $G\left(\theta,\omega \right)$ over the different periods and their standard deviation $\sigma_G\left(\theta,\omega \right)$.

    \subsection{Results}
    In \figref{fig:excitation:measurements} the nonparametric \gls{FRF} and the standard deviation of the corresponding parametric estimates are shown in a similar way as for the simulations in the previous section.

    \begin{figure}%
    \centering
      \setlength\figurewidth{0.8\columnwidth}
      \setlength\figureheight{0.68\figurewidth}
    \input{\thisDir/figs/qlog_a050mall.tikz}
    \caption[Measurements of all BK1613 octave filters using different excitation signals.]{Measured FRFs of the different \bruelkjaer{} 1613 octave filters together with the observed variance for a linear, quasi-log and pink quasi-log grid multisine.}
      \label{fig:excitation:measurements}
    \end{figure}

    The uncertainty $\sigma_G\left(\theta,\omega\right)$ on the parametric model in its pass-band is observed to be nearly constant for most of the filters when excited by the (pink) quasi-log multisine.
    For the linear grid multisine excitation, $\sigma_G\left(\theta,\omega\right)$ in the pass-band is clearly decreasing proportional to $1 / f$ for the different octave filters.
    One can see that the performance of both uncompensated quasi-log and pink quasi-log multisines is practically identical.
    This can be explained by the fact that in this experiment, $f_0$ was chosen low enough to ensure that all the second-order subsystems had their $3\unit{dB}$ bandwidth within the bulk of logarithmic part of the excited grid.
    This observation also shows that in this practical application, the increase in uncertainty between the uncompensated and compensated quasi-log multisines is negligible, as was observed earlier in the simulations.
    However, if the system under test would have a lower resonance frequency, a quasi-log multisine with power compensation would perform better as was seen in the simulations.

\section{Conclusion}
\label{sec:excitation:conclusion}
In this chapter, we recalled that the design of optimal excitation signals may require prior knowledge that is either not sufficiently accurate or even unavailable.
When only the limited prior knowledge that the system resonances lie in a certain (possibly very wide) frequency band, a suitable class of excitation signals can be constructed such that the relative uncertainty of the estimated resonance is independent of the resonance frequency.
We showed that such a signal should have a power spectrum that is distributed in $1/f$.

In this chapter we have illustrated that quasi-logarithmic multisine excitations are a well-suited to measure transfer functions over broad frequency ranges with a constant relative uncertainty of the present resonances.
The suggested amplitude compensation methods allow the user to measure a parametric \gls{LTI} model for systems in an even wider frequency band with a constant uncertainty.
The variance of the estimates at lower frequencies is improved in comparison to the uncompensated case at the cost of a slightly increased (yet constant) variance at higher frequencies for excitation signals with an equal power content.
In practice, this slight increase is likely to be negligible.
The suggested compensation works well as long as the considered systems are sufficiently damped with respect to the density of the excited frequency lines.

We have also shown an effective way to choose the frequency spacing of a (quasi)-logarithmically spaced multisine for an equi-damped set of systems.
The presented method allows to select a maximal value for the frequency ratio $\alpha = 1 + \damping$ for which the performance will not suffer from the unavoidable misalignment of the system with respect to the excited frequencies.
In practice we advise to use an excitation signal with approximately four excited lines within the $3\unit{dB}$ bandwidth of each second-order subsystem.
